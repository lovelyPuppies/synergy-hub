(Optimization, Performance, Algoirhtm)

â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»
ğŸŒŸ Algorithm ; https://en.wikipedia.org/wiki/Algorithm
(Algorithms basics)
  âš“ algorithmic efficiency ; https://en.wikipedia.org/wiki/Algorithmic_efficiency
  âš“ Heuristic ë°œê²¬ë²•, ç™¼è¦‹æ³• ; << https://en.wikipedia.org/wiki/Heuristic

  âš“ Zero-based numbering ; https://en.wikipedia.org/wiki/Zero-based_numbering
    Zero-based indices, One-based indices

  âš“ Random access, direct access ; https://en.wikipedia.org/wiki/Random_access
  âš“ Sequential access ; https://en.wikipedia.org/wiki/Sequential_access

  âš“ Canonicalization, standardization, normalization ; https://en.wikipedia.org/wiki/Canonicalization

================================================
# Optimization: Algorithms, methods, and heuristics
  âš“ Combinatorial optimization ; https://en.wikipedia.org/wiki/Combinatorial_optimization
    âš“ Dynamic programming ; https://en.wikipedia.org/wiki/Dynamic_programming
      # Contents
        ğŸš£ it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. 
      #ï¸âƒ£ğŸš£ sliding window << https://en.wiktionary.org/wiki/sliding_window#English
        (programming) A descriptor for a data analysis technique where overlapping windows of data of fixed size are analyzed.
        ğŸš£ The Sliding Window approach is a technique used in dynamic programming to optimize the memory usage
        , by only keeping track of a subset of the states instead of storing all possible states
    âš“ Greedy algorithm ; https://en.wikipedia.org/wiki/Greedy_algorithm
      ğŸ“ &comment ; == Find a optmized way to reduce Opportunity costs
  âš“ Convex Optimization ; https://en.wikipedia.org/wiki/Convex_optimization
    # Convex minimization
      âš“ Subgradient method ; https://en.wikipedia.org/wiki/Subgradient_method
      âš“ subderivative, subgradient, subdifferential << https://en.wikipedia.org/wiki/Subderivative

  âš“ Iterative method << https://en.wikipedia.org/wiki/Iterative_method
(Optimization, Performance basics)
  âš“ Optimization problem ; https://en.wikipedia.org/wiki/Optimization_problem
    âš“ğŸ“ Exact alogirhtm ; << https://en.wikipedia.org/wiki/Exact_algorithm
    âš“ğŸ“ Heuristic ; << https://en.wikipedia.org/wiki/Heuristic_(computer_science)
      âš“ Fully polynomial-time approximation scheme ; << https://en.wikipedia.org/wiki/Fully_polynomial-time_approximation_scheme
  âš“ğŸ“â­• Memoization | tabling ; << https://en.wikipedia.org/wiki/Memoization
  âš“ String interning ; https://en.wikipedia.org/wiki/String_interning
    intern
  #ï¸âƒ£ square root decomposition ; https://en.wiktionary.org/wiki/square_root_decomposition#English

  âš“ Pure Function ; https://en.wikipedia.org/wiki/Pure_function
    ğŸ“ &comment
      recommendation from
        ğŸ”— https://reactjs.org/tutorial/tutorial.html#why-immutability-is-important
        ğŸ”— https://eslint.org/docs/rules/no-param-reassign

      pure function ì´ ë§ì„ìˆ˜ë¡ ëª¨ë“ˆí™” ìˆ˜ì¤€ì´ ë†’ë‹¤ëŠ” ì˜ë¯¸ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤.
    âš“ referential transparency, referential opacity ; << https://en.wikipedia.org/wiki/Referential_transparency
        ğŸ’¡ An expression is called referentially transparent if it can be replaced with its corresponding value (and vice-versa) without changing the program's behavior.
        This requires that the expression be pure, that is to say the expression value must be the same for the same inputs and its evaluation must have no side effects.
        âšª An expression that is not referentially transparent is called âœ”ï¸referentially opaqueâœ”ï¸.
      #ï¸âƒ£ 2 Examples and counterexamples ; https://en.wikipedia.org/wiki/Referential_transparency#Examples_and_counterexamples
      #ï¸âƒ£ 4 Another example ; https://en.wikipedia.org/wiki/Referential_transparency#Another_example
  
  âš“ Out Of Memory (OOM) ; https://en.wikipedia.org/wiki/Out_of_memory
  âš“ memory footprint ; https://en.wikipedia.org/wiki/Memory_footprint
  âš“ computational resource ; https://en.wikipedia.org/wiki/Computational_resource

  âš“ computer performance ; https://en.wikipedia.org/wiki/Computer_performance
  âš“ programming complexity, software complexity ; https://en.wikipedia.org/wiki/Programming_complexity
    - complicated
    - complex

    2 Types
      - Accidental complexity
      - Essential complexity
  âš“ computational complexity, complexity ; https://en.wikipedia.org/wiki/Computational_complexity
  âš“ mathematical optimization ; https://en.wikipedia.org/wiki/Mathematical_optimization
  

  âš“ğŸ“ lookup table (LUT) ; https://en.wikipedia.org/wiki/Lookup_table
      - Temporal locality
      - Spatial locality
      - Branch locality
      - Equidistant locality
  âš“ stride of an array, increment, pitch, step size ; https://en.wikipedia.org/wiki/Stride_of_an_array


  âš“ Non-Uniform Memory Access, NUMA ; https://en.wikipedia.org/wiki/Non-uniform_memory_access
  âš“ Deep learning processor (DLP), deep learning accelerator ; https://en.wikipedia.org/wiki/Deep_learning_processor
  âš“ AI accelerator, Neural Processing Unit, NPU ; https://en.wikipedia.org/wiki/AI_accelerator
  âš“ Tensor Processing Unit, TPU ; https://en.wikipedia.org/wiki/Tensor_Processing_Unit
    ğŸ“ &comment
      https://voidint.com/2020/10/14/cpu-gpu-tpu-npu/
  
  âš“ GPU virtualization ; https://en.wikipedia.org/wiki/GPU_virtualization
  âš“ paravirtualization, para-virtualization ; https://en.wikipedia.org/wiki/Paravirtualization


  âš“ optimal control ; https://en.wikipedia.org/wiki/Optimal_control

  âš“ convex function ; https://en.wikipedia.org/wiki/Convex_function
    - convex



================================================
(Complexity class)
  âš“ Complexity class ; https://en.wikipedia.org/wiki/Complexity_class
    (Decision problem) << 
      âš“ Decision problem ; https://en.wikipedia.org/wiki/Decision_problem
        ğŸ“ &comment; It is important that all decision problem not only have Exact algoirhtm but may also Heuristic (approximation).
        âš“ Decidable problem ; << https://en.wikipedia.org/wiki/Decidability_(logic)
        âš“ Undecidable problem ; << https://en.wikipedia.org/wiki/Undecidable_problem
      âš“ Complement (complexity) ; https://en.wikipedia.org/wiki/Complement_(complexity)
    (Deterministic) <<
      âš“ğŸ“ Deterministic algorithm ; https://en.wikipedia.org/wiki/Deterministic_algorithm
        Formally, a deterministic algorithm computes a mathematical function; ...
      âš“ğŸ–‡ï¸ Nondeterministic algorithm ; https://en.wikipedia.org/wiki/Nondeterministic_algorithm ğŸ“… 2023-03-22 05:09:48
        ğŸ“ &comment; Note that a decision problem may exist individually according to definition of a optimized problem. refer to TSP for e.g.
        #ï¸âƒ£ğŸ’¡ Use
          ... In computational complexity theory, nondeterministic algorithms are ones that, at every possible step, can allow for multiple continuations
    âš“ğŸ“ Reduction | í™˜ì‚° ; https://en.wikipedia.org/wiki/Reduction_(complexity)
      ... reducible ... When this is true, solving A cannot be harder than solving B. 
      ğŸš£ "Harder" means ... shorthand notation ...
      ğŸ” The mathematical structure generated on a set of problems by the reductions of a particular type generally forms a preorder, ...
    # Considered feasible
      âš“ P | PTIME | DTIME (n^O(1)) ; https://en.wikipedia.org/wiki/P_(complexity)
    # Suspected infeasible
      âš“ Nondeterministic Polynomial time (NP) ; https://en.wikipedia.org/wiki/NP_(complexity)
        ğŸ“ &comment ; understanding from ğŸ”— chatGPT
          NP is the class of decision problems for which there exists a polynomial-time algorithm that can verify a solution in polynomial time.
          namely given Witness (Certificate | Guess), and if Verifier (Certifier) can verify a solution in polynomial time, it is NP.
            - NP is not limited to non-deterministic Turing machines, although they are often used to define the class.
              NP includes the class of decision problems that can be solved by a non-deterministic Turing machine in polynomial time.
              ; A non-deterministic Turing machine is a hypothetical computer that can try all possible paths of computation at once simultaneously.
              It means problems in not only exponentially increasing but also worse than exponential are included in NP.
            - This Certificate can be thought of as a Guess because the machine is not guaranteed to know whether the certificate is valid or not until it is checked.
              note that verifier algorithm (that is type of Decider) is a deterministic Turing machine, not a non-deterministic Turing machine.

          all decision problems are in P or NP (not in P) except for undecidable problems (like Busy Beaver).
          all solvable decision problems in pseudo-polynomial time are NP (not in P) when encoded in binary.

          NP-hard problems are not necessarily in NP, while NP-complete problems are a subset of NP problems that are also NP-hard.
        # Contents
          (witenss | certificate), (verifier, certifier)
        âš“ğŸ“ NP-hardness ; https://en.wikipedia.org/wiki/NP-hardness
          ğŸ“ &comment ; (comparsion) ğŸ†š NP-hardness, NP-completeness from ğŸ”— chatGPT
            To summarize, NP-hardness refers to the lower bound of the difficulty of a problem
            , while NP-completeness refers to the combination of both upper and lower bounds.
            NP-complete problems are the hardest problems in NP
            , while NP-hard problems may be even harder (not in NP) or easier (in NP but not NP-complete).
          #ï¸âƒ£ğŸ“ğŸ’¡ Definition ; https://en.wikipedia.org/wiki/NP-hardness#Definition
        âš“ NP-completeness ; https://en.wikipedia.org/wiki/NP-completeness
          # Contents
            "Complete" refers to the property of being able to simulate everything in the same complexity class.
            - NP-C | NPC
            #ï¸âƒ£ğŸ“ğŸ’¡ Formal definition ; https://en.wikipedia.org/wiki/NP-completeness#Formal_definition
            #ï¸âƒ£ğŸ”ğŸ’¡ NP-complete problems ; https://en.wikipedia.org/wiki/NP-completeness#NP-complete_problems
              ğŸ” Interpretion of Picture ... Some NP-complete problems, indicating the reductions typically used to prove their NP-completeness.
          âš“ğŸ“ Weak NP-completeness ; << https://en.wikipedia.org/wiki/Weak_NP-completeness
            #ğŸ”ğŸ’¡ Strong and weak NP-hardness vs. strong and weak polynomial-time algorithms
          âš“ Strong NP-completeness ; << https://en.wikipedia.org/wiki/Strong_NP-completeness
            A problem is said to be strongly NP-complete (NP-complete in the strong sense)
            , if it remains NP-complete even when all of its numerical parameters are bounded by a polynomial in the length of the input
        
        âš“ P versus NP problem ; << https://en.wikipedia.org/wiki/P_versus_NP_problem
      âš“ co-NP ; << https://en.wikipedia.org/wiki/Co-NP
  âš“ List of complexity classes ; https://en.wikipedia.org/wiki/List_of_complexity_classes

  # Fit approximation <<
    âš“ Computational complexity theory ; << https://en.wikipedia.org/wiki/Computational_complexity_theory
    âš“ Big O notation, Bachmann-Landau notation, asymptotic notation ; https://en.wikipedia.org/wiki/Big_O_notation
      # Contents
        #ï¸âƒ£ Orders of common functions ; https://en.wikipedia.org/wiki/Big_O_notation#Orders_of_common_functions
        #ï¸âƒ£ Related asymptotic notations ; https://en.wikipedia.org/wiki/Big_O_notation#Related_asymptotic_notations
          #ï¸âƒ£ Family of Bachmannâ€“Landau notations ; https://en.wikipedia.org/wiki/Big_O_notation#Family_of_Bachmann%E2%80%93Landau_notations
            - Big Theta, Big Omega
      âš“ Asymptotically optimal algorithm ; << https://en.wikipedia.org/wiki/Asymptotically_optimal_algorithm
      âš“ Upper and lower bounds ; https://en.wikipedia.org/wiki/Upper_and_lower_bounds
    âš“ Time complexity ; << https://en.wikipedia.org/wiki/Time_complexity
      ğŸ“ &comment; understanding
        + ğŸ†š (comparsions) (strongly, weakly, pseudo-) polynomial time   and  Weak NP-completeness
          - (strongly, weakly, pseudo-) polynomial time
            Table: Does always one have polynomial time in proportion to these?
              - size of the input; number of bits required to represent the input.
              - value of the input; magnitude of the largest integer.

            ;                       Size              Value
            strongly polynomial:    O                 O
            weakly polynomial:      O                 not necessarily
              that is, polynomial in the number of integers and the number of bits in the largest integer
            pseudo-polynomial:      not necessarily   O
              that is, polynomial in the number of integers and the magnitude of the largest integer
            
            ğŸ›ï¸ e.g. of weakly polynomial time is The Euclidean algorithm for computing the greatest common divisor of two integers
              Its real running time depends logarithmically on the magnitudes of a and b
            ğŸ›ï¸ e.g. of pseudo-polynomial time is Knapsack problem whose the time complexity is O(n*W).
              n is size of the input and W is value of the input. e.g.
              If doubling the size of n increases the time complexity as 2 times. 
                n = [n1, n2, n3, ... , n10]. from 10 to 20
              However, doubling the size of W increases the time complexity exponentially, just as the shift operation is applied.
                W = 1000 in binary term. from (4bit long; 2^3) to 1000000 (8-bit long; 2^7)
            References
              https://stackoverflow.com/a/27718369
          - Weak NP-completeness
            An NP-complete problem with known pseudo-polynomial time algorithms is called weakly NP-complete. from ğŸ”— Pseudo-polynomial time
            if there is an algorithm for the problem whose running time is polynomial in the dimension of the problem and the magnitudes of the data involved from ğŸ”— Weak NP-completeness

            Note that it is "there is an algorithm for the problem ...", not "there is an solvable algorithm ...".
            namely, it is the case where solvable algorithm includes an algorithm of definition from ğŸ”— Weak NP-completeness.
            
            so all Weak NP-completeness not have pseudo-polynomial time.
            Refer to ğŸ”— Strong and weak NP-hardness vs. strong and weak polynomial-time algorithms
        âœ… (how-to); how to measure time complexity using examples
          merge sort ; https://maramarathon.tistory.com/55
            ë°°ì—´ì˜ ê¸¸ì´ë¥¼ N = 2^k ë¼ í•  ë•Œ, í•©ë³‘ ë‹¨ê³„ëŠ” kë²ˆ ì¼ì–´ë‚œë‹¤. (í•©ë³‘ì€ ê° ë‹¨ê³„ë§ˆë‹¤ N/2*x. xëŠ” í•©ë³‘ë‹¨ê³„)
            í•©ë³‘ì´ ì¼ì–´ë‚  ë•Œë§ˆë‹¤, ì„ì‹œë°°ì—´ì— ì›ì†Œë¥¼ ì •ë ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì›ì†Œ ê°œìˆ˜ë§Œí¼ì˜ ë¹„êµì—°ì‚°ì´ ìˆ˜í–‰ëœë‹¤. ë˜í•œ, ì„ì‹œë°°ì—´ì˜ ì›ì†Œë¥¼ ì›ë˜ì˜ ë°°ì—´ë¡œ ì´ë™ì‹œí‚¤ëŠ” ê³¼ì •ì—ì„œ ì›ì†Œ ê°œìˆ˜ë§Œí¼ì˜ ì´ë™ì—°ì‚°ì´ ìˆ˜í–‰ëœë‹¤.
            ê°ê°ì˜ ë‹¨ê³„ëŠ” ì—¬ëŸ¬ë²ˆì˜ í•©ë³‘ì„ í¬í•¨í•˜ê³  ìˆìœ¼ë‚˜, ê²°êµ­ ëª¨ë“  ì›ì†Œë“¤ì˜ ê°œìˆ˜ í•©ì€ Nê°œ ì´ë¯€ë¡œ ê°ê°ì˜ ë‹¨ê³„ì—ì„œ 2N ë§Œí¼ì˜ ì´ë™, ë¹„êµ ì—°ì‚°ì´ ì¼ì–´ë‚œë‹¤.
            ê²°êµ­, ì‹œê°„ë³µì¡ë„ëŠ” í•©ë³‘ ë‹¨ê³„ kë²ˆ * ê°ê°ì˜ ë‹¨ê³„ì—ì„œ ì—°ì‚° íšŸìˆ˜ 2N = O(kN)ì´ ëœë‹¤.
            N = 2^k, k = logN ì´ë¯€ë¡œ í•©ë³‘ ì •ë ¬ì˜ ì‹œê°„ë³µì¡ë„ëŠ” O(kN) = O(NlogN)ì´ ëœë‹¤. 
      # Contents
        Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform.
        #ï¸âƒ£ Table of common time complexities ; https://en.wikipedia.org/wiki/Time_complexity#Table_of_common_time_complexities
        #ï¸âƒ£ quasilinear time | log-linear time ; https://en.wikipedia.org/wiki/Time_complexity#Quasilinear_time
          linearithmic time
        #ï¸âƒ£ Polynomial time ; https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time
          #ï¸âƒ£ Strongly and weakly polynomial time ; https://en.wikipedia.org/wiki/Time_complexity#Strongly_and_weakly_polynomial_time
            ğŸ“ &comment; https://cs.stackexchange.com/questions/7543/are-there-strongly-polynomial-algorithms-that-take-more-than-polynomial-time
            Strongly polynomial time is defined in the arithmetic model of computation.
            ğŸš£ In this model of computation the basic arithmetic operations (addition, subtraction, multiplication, division, and comparison) take a unit time step to perform, regardless of the sizes of the operands.
          â”” âš“ğŸ“ Pseudo-polynomial time ; https://en.wikipedia.org/wiki/Pseudo-polynomial_time
            ğŸ”— (comparsions) (strongly, weakly, pseudo-) polynomial time   and  Weak NP-completeness
            #ğŸ” Generalizing to non-numeric problems
              ğŸ“ &comment; SAT ë¬¸ì œí•˜ê³ ë‚˜ì„œ ë‹¤ì‹œ ë³´ê¸°. SAT ì™€ 3-SAT, 2-SAT.. ì´ëŸ°ì‹ìœ¼ë¡œ ìˆìŒ., dynamic í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ì¼ë°˜í™”í•œ SAT í’€ ìˆ˜ ìˆëŠ”ë“¯?
                m(n) ì´ size of the input, k(n) ì´ value of the input ì´ë¼ê³  ë³´ë©´ ì–´ëŠì •ë„ í•´ì„ì€ ë¨.
              ğŸ’¡ The distinction between the value of a number and its length is one of encoding: if numeric inputs are always encoded in unary, then pseudo-polynomial would coincide with polynomial.
                for example, refer to SSP wikipedia.
          #ï¸âƒ£ Complexity classes ; https://en.wikipedia.org/wiki/Time_complexity#Complexity_classes
        #ï¸âƒ£ Superpolynomial time ; https://en.wikipedia.org/wiki/Time_complexity#Superpolynomial_time
      âš“ Ackermann function ; << https://en.wikipedia.org/wiki/Ackermann_function
        #ï¸âƒ£ğŸš£ Inverse Ackermann function ; https://en.wikipedia.org/wiki/Ackermann_function#Inverse
          In fact, Î±(n) is less than 5 for any practical input size n, ...
          from ğŸ”— Disjoint-set
            The inverse Ackermann function grows extraordinarily slowly
            , so this factor is 4 or less for any n that can actually be written in the physical universe. 

      âš“ Time Complexity of Python language ; << https://wiki.python.org/moin/TimeComplexity
    âš“ Amortized analysis, ë¶„í•  ìƒí™˜ ë¶„ì„ << https://en.wikipedia.org/wiki/Amortized_analysis
    âš“ Space complexity ; << https://en.wikipedia.org/wiki/Space_complexity
      ğŸ“ğŸš£ &comment: understanding
        - ì¬ê·€í•¨ìˆ˜ì˜ ê²½ìš°, í˜¸ì¶œí•œ ë§Œí¼ í•¨ìˆ˜ê°€ ìŠ¤íƒì— ì¶”ê°€ë˜ë¯€ë¡œ O(n) ì˜ ë³µì¡ë„ë¥¼ ê°€ì§„ë‹¤. 
        - n ë²ˆ í•¨ìˆ˜ë¥¼ í˜¸ì¶œ í–ˆë‹¤ê³  í•´ì„œ ë°˜ë“œì‹œ ê³µê°„ë³µì¡ë„ê°€ O(n) ì€ ì•„ë‹ˆë‹¤. í•¨ìˆ˜ë“¤ì´ ë™ì‹œì— ìŠ¤íƒì— ì¡´ì¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê¸° ë–„ë¬¸.
  âš“ Computational hardness assumption ; https://en.wikipedia.org/wiki/Computational_hardness_assumption
    # Non-cryptographic
      âš“ Exponential time hypothesis ; https://en.wikipedia.org/wiki/Exponential_time_hypothesis

âš“ Computational complexity of mathematical operations ; << https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations
âš“ List of algorithms ; https://en.wikipedia.org/wiki/List_of_algorithms
  ğŸ“ &comment; it integrates some Main subjects:
    - Well-known computer science algorithms
    - Number-theoretic algorithms
  âš“ Algorithmic paradigm ; << https://en.wikipedia.org/wiki/Algorithmic_paradigm
    ğŸ“ &comment; Tip
      - keep on eye to lexical elements: ("a", "the"), ("exactly", "less than", "greater than", "equal than"), <upper case>
      - modularize from low-level to high-level
    âš“ğŸ“ğŸ“ Divide-and-conquer algorithm ; https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm
      As in mathematical induction, it is often necessary to generalize the problem to make it amenable to a recursive solution.
      The correctness of a divide-and-conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.

  #ï¸âƒ£ Combinatorial algorithms ; https://en.wikipedia.org/wiki/List_of_algorithms#Combinatorial_algorithms
    #ï¸âƒ£ General combinatorial algorithms ; https://en.wikipedia.org/wiki/List_of_algorithms#General_combinatorial_algorithms
      âš“ Eight queens puzzle | n queens problem ; https://en.wikipedia.org/wiki/Eight_queens_puzzle
        ğŸ” #P-complete
        #ï¸âƒ£ğŸ“° Related problems ; https://en.wikipedia.org/wiki/Eight_queens_puzzle#Related_problems
    #ï¸âƒ£ Graph algorithms ; https://en.wikipedia.org/wiki/List_of_algorithms#Graph_algorithms
      âš“ Maximum cardinality matching ; https://en.wikipedia.org/wiki/Maximum_cardinality_matching
        âš“ Hopcroftâ€“Karp algorithm ; https://en.wikipedia.org/wiki/Hopcroft%E2%80%93Karp_algorithm
      âš“ Topological sorting | topological ordering ; https://en.wikipedia.org/wiki/Topological_sorting
        âš“ Feedback arc set ; https://en.wikipedia.org/wiki/Feedback_arc_set
          ğŸ“ &comment; It seems that no condition where feedback arc set is connected (namely, it allows forest shape) is.
          np-hard
      #ï¸âƒ£ Graph drawing ; https://en.wikipedia.org/wiki/List_of_algorithms#Graph_drawing
      #ï¸âƒ£ Network theory ; https://en.wikipedia.org/wiki/List_of_algorithms#Network_theory
      #ï¸âƒ£ Routing for graphs ; https://en.wikipedia.org/wiki/List_of_algorithms#Routing_for_graphs
        âš“ğŸ“ minimum spanning tree (MST), minimum weight spanning tree  ; https://en.wikipedia.org/wiki/Minimum_spanning_tree
          # Contents
            #ï¸âƒ£ Properties ; https://en.wikipedia.org/wiki/Minimum_spanning_tree#Properties
              #ï¸âƒ£ Cut Property ; https://en.wikipedia.org/wiki/Minimum_spanning_tree#Cut_property
              #ï¸âƒ£ Minimum cost edge ; https://en.wikipedia.org/wiki/Minimum_spanning_tree#Minimum-cost_edge
            #ï¸âƒ£ Algorithms ; https://en.wikipedia.org/wiki/Minimum_spanning_tree#Algorithms
              #ï¸âƒ£ Classic algorithms ; https://en.wikipedia.org/wiki/Minimum_spanning_tree#Classic_algorithms
                ğŸš£ ... related decision problems such as determining whether a particular edge is in the MST or determining if the minimum total weight exceeds a certain value are in P.
          âš“ğŸ“ Kruskal's algorithm ; https://en.wikipedia.org/wiki/Kruskal%27s_algorithm
        âš“ğŸ“ Shortest path problem ; https://en.wikipedia.org/wiki/Shortest_path_problem
          âš“ Dijkstra's algorithm ; https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm

        âš“ Hamiltonian path problem | Hamiltonian cycle problem ; << https://en.wikipedia.org/wiki/Hamiltonian_path_problem
          âš“ Hamiltonian path | traceable path ; << https://en.wikipedia.org/wiki/Hamiltonian_path
            (Hamiltonian cycle | Hamiltonian circuit)
          âš“ğŸ“ Travelling salesman problem (TSP) ; https://en.wikipedia.org/wiki/Travelling_salesman_problem
            # Contents
              (| travelling salesperson problem)
              ğŸ” The TSP has several applications even in its purest formulation, ...
              #ï¸âƒ£ğŸ“ Description ; https://en.wikipedia.org/wiki/Travelling_salesman_problem#Description
                # As a graph problem
                # Asymmetric and symmetric
                #ğŸ” Related problems
              #ï¸âƒ£ Computing a solution ; https://en.wikipedia.org/wiki/Travelling_salesman_problem#Computing_a_solution
                #ï¸âƒ£ Exact algorithms ; https://en.wikipedia.org/wiki/Travelling_salesman_problem#Exact_algorithms
                  âš“ğŸ“ğŸ” Heldâ€“Karp algorithm ; https://en.wikipedia.org/wiki/Held%E2%80%93Karp_algorithm
                    #ï¸âƒ£ğŸ“ Algorithmic complexity ; https://en.wikipedia.org/wiki/Held%E2%80%93Karp_algorithm#Algorithmic_complexity
                      ğŸ“ &comment; derivation process of Time complexity
                        $k(n-k-1)\binom{n-1}{k} = k(n-k-1)\frac{(n-1)!}{k!(n-k-1)!}$ ...
                        binomial coefficient ë¶€í„° ê³µë¶€í•´ì•¼í• ë“¯.
                #ï¸âƒ£ Heuristic and approximation algorithms ; https://en.wikipedia.org/wiki/Travelling_salesman_problem#Heuristic_and_approximation_algorithms
                  # Constructive heuristics
            
            âš“ Bottleneck traveling salesman problem (bottleneck TSP) ; << https://en.wikipedia.org/wiki/Bottleneck_traveling_salesman_problem
            âš“ Traveling purchaser problem (TPP) ; https://en.wikipedia.org/wiki/Traveling_purchaser_problem
            âš“ Set TSP problem ; https://en.wikipedia.org/wiki/Set_TSP_problem
          âš“ Nearest neighbour algorithm ; https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm

        âš“ Eulerian path | Eulerian trail ; https://en.wikipedia.org/wiki/Eulerian_path
          (Eulerian circuit | Eulerian cycle)


      #ï¸âƒ£ Graph search, graph traversal ; https://en.wikipedia.org/wiki/List_of_algorithms#Graph_search
        âš“ State space search ; https://en.wikipedia.org/wiki/State_space_search
       & https://en.wikipedia.org/wiki/Graph_traversal
        âš“ tree traversal, tree search, walking the tree; << https://en.wikipedia.org/wiki/Tree_traversal
        #ï¸âƒ£ Types ; https://en.wikipedia.org/wiki/Tree_traversal#Types
          #ï¸âƒ£ Depth-first search ; https://en.wikipedia.org/wiki/Tree_traversal#Depth-first_search
            N: Visit the current node.
            L: Recursively traverse the current node's left subtree.
            R: Recursively traverse the current node's right subtree.
            #ï¸âƒ£ Pre-order (NLR) ; https://en.wikipedia.org/wiki/Tree_traversal#Pre-order,_NLR
            #ï¸âƒ£ Post-order (LRN) ; https://en.wikipedia.org/wiki/Tree_traversal#Post-order,_LRN
            #ï¸âƒ£ In-order (LNR) ; https://en.wikipedia.org/wiki/Tree_traversal#In-order,_LNR
            #ï¸âƒ£ Reverse pre-order (NRL) ; https://en.wikipedia.org/wiki/Tree_traversal#Reverse_pre-order,_NRL
            #ï¸âƒ£ Reverse post-order (RLN) ; https://en.wikipedia.org/wiki/Tree_traversal#Reverse_post-order,_RLN
            #ï¸âƒ£ Reverse in-order (RNL) ; https://en.wikipedia.org/wiki/Tree_traversal#Reverse_in-order,_RNL
           & âš“ğŸ“ Depth-first search (DFS) ; https://en.wikipedia.org/wiki/Depth-first_search
            The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) 
            ğŸš£ Extra memory, usually a stack, is needed to keep track of the nodes discovered so far along a specified branch which helps in backtracking of the graph.
            
            #ï¸âƒ£ğŸ“ 2 Example ; https://en.wikipedia.org/wiki/Depth-first_search#Example
            #ï¸âƒ£ğŸ“ğŸ’¡ 4 Pseudocode ; https://en.wikipedia.org/wiki/Depth-first_search#Pseudocode
              ğŸš£ The non-recursive implementation is similar to breadth-first search but differs from it in two ways: ğŸ†š ...

            âš“ğŸ“ğŸ” iterative deepening search, iterative deepening depth-first search (IDS), (IDDFS); << https://en.wikipedia.org/wiki/Iterative_deepening_depth-first_search
              ğŸ” ... it visits the nodes in the search tree in the same order as depth-first search, but the cumulative order in which nodes are first visited is effectively breadth-first.
          #ï¸âƒ£ Breadth-first search; https://en.wikipedia.org/wiki/Tree_traversal#Breadth-first_search
           & âš“ğŸ“ğŸ” Breadth-first search (BFS) ; https://en.wikipedia.org/wiki/Breadth-first_search
            ğŸš£ Extra memory, usually a queue, is needed to keep track of the child nodes that were encountered but not yet explored.
            For example, in a chess endgame a chess engine may build the game tree from the current position ...
            ğŸš£ when the start node (sometimes referred to as a 'search key') is explicitly given ...
            ğŸš£ ... Note that the word node is usually interchangeable with the word vertex.
            - ğŸ” solution node DFS and BFS ğŸ†š, (start node | search key)
            - ğŸ” Breadth-first search can be generalized to graphs, when ... 

            #ï¸âƒ£ğŸ“ Pseudocode ; https://en.wikipedia.org/wiki/Breadth-first_search#Pseudocode
            #ï¸âƒ£ Analysis ; https://en.wikipedia.org/wiki/Breadth-first_search#Analysis
              #ï¸âƒ£ğŸ“ğŸ”ğŸ’¡ Time and space complexity ; https://en.wikipedia.org/wiki/Breadth-first_search#Time_and_space_complexity

      #ï¸âƒ£ Subgraphs ; https://en.wikipedia.org/wiki/List_of_algorithms#Subgraphs

    #ï¸âƒ£ Sequence algorithms ; https://en.wikipedia.org/wiki/List_of_algorithms#Sequence_algorithms
      #ï¸âƒ£ Approximate sequence matching ; https://en.wikipedia.org/wiki/List_of_algorithms#Approximate_sequence_matching
        âš“ String metrics ; << https://en.wikipedia.org/wiki/String_metric
          âš“ Hamming distance ; https://en.wikipedia.org/wiki/Hamming_distance
      #ï¸âƒ£ Selection algorithms ; https://en.wikipedia.org/wiki/List_of_algorithms#Selection_algorithms
        & âš“ Selection algorithm ; https://en.wikipedia.org/wiki/Selection_algorithm
        #ï¸âƒ£ Partition-based selection ; https://en.wikipedia.org/wiki/Selection_algorithm#Partition-based_selection
          âš“ Quickselect ; << https://en.wikipedia.org/wiki/Quickselect
          #ï¸âƒ£ Median selection as pivot strategy ; https://en.wikipedia.org/wiki/Selection_algorithm#Median_selection_as_pivot_strategy
      #ï¸âƒ£ Sequence search ; https://en.wikipedia.org/wiki/List_of_algorithms#Sequence_search
        âš“ğŸ“ğŸ” Binary search | half-interval search | logarithmic search | binary chop, ; https://en.wikipedia.org/wiki/Binary_search_algorithm
          # Contents
            ğŸš£ There are specialized data structures designed for fast searching, such as hash tables, that can be searched more efficiently than binary search.
            ğŸ” There are numerous variations of binary search ...
            #ï¸âƒ£ Algorithm ; https://en.wikipedia.org/wiki/Binary_search_algorithm#Algorithm
              #ï¸âƒ£ğŸ–‡ï¸ Preocedure ; https://en.wikipedia.org/wiki/Binary_search_algorithm#Procedure ğŸ“… 2023-03-10 05:37:50
                #ï¸âƒ£â­• Alternative procedure ; https://en.wikipedia.org/wiki/Binary_search_algorithm#Alternative_procedure
      #ï¸âƒ£ Sequence merging ; https://en.wikipedia.org/wiki/List_of_algorithms#Sequence_merging
        & âš“ğŸ“ Merge algorithm ; << https://en.wikipedia.org/wiki/Merge_algorithm
          ğŸš£ Merge algorithms are a family of algorithms that take multiple sorted lists as input and produce a single list as output, containing all the elements of the inputs lists in sorted order. 
      #ï¸âƒ£ Sequence sorting ; https://en.wikipedia.org/wiki/List_of_algorithms#Sequence_sorting
        & âš“ğŸ“ Sorting algorithm ; https://en.wikipedia.org/wiki/Sorting_algorithm
          # Contents
            The most frequently used orders are numerical order and lexicographical order, and either ascending or descending.
            ğŸš£ Sorting is also often useful for canonicalizing data and for producing human-readable output.
            ğŸš£ Formally, the output of any sorting algorithm must satisfy two conditions:
              - The output is in monotonic order (each element is no smaller/larger than the previous element, according to the required order).
              - The output is a permutation (a reordering, yet retaining all of the original elements) of the input.
            ğŸ’¡ For optimum efficiency, the input data should be stored in a data structure which allows random access rather than one that allows only sequential access.
            #ï¸âƒ£ Classification ; https://en.wikipedia.org/wiki/Sorting_algorithm#Classification
              #ï¸âƒ£ğŸš£ Stability ; https://en.wikipedia.org/wiki/Sorting_algorithm#Stability
            #ï¸âƒ£ğŸ’¡ Comparison of algorithms ; https://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms
              #ï¸âƒ£ Comparison sorts ; https://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_sorts
              #ï¸âƒ£ Non-comparison sorts ; https://en.wikipedia.org/wiki/Sorting_algorithm#Non-comparison_sorts

            #ï¸âƒ£ 4 Popular sorting algorithms ; https://en.wikipedia.org/wiki/Sorting_algorithm#Popular_sorting_algorithms
            #ï¸âƒ£ğŸ“ğŸš£ Simple sorts ; https://en.wikipedia.org/wiki/Sorting_algorithm#Simple_sorts
              ... ğŸ†š
              #ï¸âƒ£ğŸ“ Insertion sort ; https://en.wikipedia.org/wiki/Sorting_algorithm#Insertion_sort
                ğŸš£ In arrays, the new list and the remaining elements can share the array's space, but insertion is expensive, requiring shifting all following elements over by one.
                Shellsort is a variant of insertion sort that is more efficient for larger lists.
              #ï¸âƒ£ğŸ“ Selection sort ; https://en.wikipedia.org/wiki/Sorting_algorithm#Selection_sort
                selection sort is an ğŸš£ in-place comparison sorting algorithm.
                ğŸš£ It does no more than n swaps, and thus is useful where swapping is very expensive.
            #ï¸âƒ£ğŸ“ Efficient sorts ; https://en.wikipedia.org/wiki/Sorting_algorithm#Efficient_sorts
              ğŸ’¡ various modifications are used, First ...
              #ï¸âƒ£ğŸ“ Merge sort ; https://en.wikipedia.org/wiki/Sorting_algorithm#Merge_sort
                ğŸš£ It is also easily applied to lists, not only arrays, as it only requires sequential access, not random access.
              #ï¸âƒ£ğŸ“ Heapsort ; https://en.wikipedia.org/wiki/Sorting_algorithm#Heapsort
                ğŸš£ When it is removed and placed at the end of the list, the heap is rearranged so the largest element remaining moves to the root.
                Using the heap, finding the next largest element takes O(log n) time, instead of O(n) for a linear scan as in simple selection sort.
                #ï¸âƒ£ğŸ“ğŸ“ Overview ; https://en.wikipedia.org/wiki/Heapsort#Overview
                  - iParent, iLeftChild, iRightChild
                  Heapsort can be performed in place. The array can be split into two parts, the sorted array and the heap.
                  The heap's invariant is preserved after each extraction, so the only cost is that of extraction.
                #ï¸âƒ£ğŸ“ Algorithm ; https://en.wikipedia.org/wiki/Heapsort#Algorithm
                  ğŸ’¡ The buildMaxHeap() operation is run once, and is O(n) in performance. The siftDown() function is O(log n), and is called n times.
                  Therefore, the performance of this algorithm is O(n + n log n) = O(n log n).
                  #ï¸âƒ£ Pseudocode ; https://en.wikipedia.org/wiki/Heapsort#Pseudocode
                    ğŸ’¡ The sorting routine uses two subroutines, heapify and siftDown. The former is the common in-place heap construction routine, while the latter is a common subroutine for implementing heapify.



              #ï¸âƒ£ğŸ“ğŸ” Quicksort ; https://en.wikipedia.org/wiki/Sorting_algorithm#Quicksort

            #ï¸âƒ£ Bubble sort and variants ; https://en.wikipedia.org/wiki/Sorting_algorithm#Bubble_sort_and_variants
              #ï¸âƒ£ğŸ“ Bubble sort ; https://en.wikipedia.org/wiki/Sorting_algorithm#Bubble_sort
                Bubble sort can also be used efficiently on a list of any length that is nearly sorted
          # Theory
            âš“ In-place algorithm ; https://en.wikipedia.org/wiki/In-place_algorithm
            âš“ Online algorithm ; << https://en.wikipedia.org/wiki/Online_algorithm
            âš“ Adaptive sort ; https://en.wikipedia.org/wiki/Adaptive_sort
            âš“ Loop invariant ; << https://en.wikipedia.org/wiki/Loop_invariant
          # Exchange sorts
            âš“ğŸ“ Bubble sort, sinking sort ; https://en.wikipedia.org/wiki/Bubble_sort
              ... is named for the way the larger elements "bubble" up to the top of the list.
              #ï¸âƒ£ Implementation ; https://en.wikipedia.org/wiki/Bubble_sort#Implementation
                #ï¸âƒ£ğŸš£ Optimizing bubble sort ; https://en.wikipedia.org/wiki/Bubble_sort#Optimizing_bubble_sort
            âš“ Quicksort ; https://en.wikipedia.org/wiki/Quicksort
              #ï¸âƒ£ Algorithm ; https://en.wikipedia.org/wiki/Quicksort#Algorithm
                #ï¸âƒ£ Implementation issues ; https://en.wikipedia.org/wiki/Quicksort#Implementation_issues
                  #ï¸âƒ£ Choice of pivot ; https://en.wikipedia.org/wiki/Quicksort#Choice_of_pivot
              #ï¸âƒ£ Formal analysis ; https://en.wikipedia.org/wiki/Quicksort#Formal_analysis
                #ï¸âƒ£ Worst-case analysis ; https://en.wikipedia.org/wiki/Quicksort#Worst-case_analysis
                #ï¸âƒ£ Best-case analysis ; https://en.wikipedia.org/wiki/Quicksort#Best-case_analysis
                #ï¸âƒ£ Average-case analysis ; https://en.wikipedia.org/wiki/Quicksort#Average-case_analysis
          # Selection sorts
            âš“ğŸ“ Selection sorts ; https://en.wikipedia.org/wiki/Selection_sort
            âš“ğŸ“ Heapsort ; https://en.wikipedia.org/wiki/Heapsort
              ğŸš£ Unlike selection sort, heapsort does not waste time with a linear-time scan of the unsorted region; rather, heap sort maintains the unsorted region in a heap data structure to more quickly find the largest element in each step.[1]

          # Insertion sorts
            âš“ğŸ“ Insertion sort ; https://en.wikipedia.org/wiki/Insertion_sort
              #ï¸âƒ£ Algorithm; https://en.wikipedia.org/wiki/Insertion_sort#Algorithm
                ğŸš£ a slightly faster version ...
            âš“ Patience sorting ; https://en.wikipedia.org/wiki/Patience_sorting
          # Merge sorts
            âš“ğŸ“ Merge sort ; https://en.wikipedia.org/wiki/Merge_sort
              #ï¸âƒ£ Ping-pong merge sort ; https://en.wikipedia.org/wiki/Merge_sort#Ping-pong_merge_sort
              #ï¸âƒ£ In-place merge sort ; https://en.wikipedia.org/wiki/Merge_sort#In-place_merge_sort
          # Hybrid sorts
            âš“ Timsort ; https://en.wikipedia.org/wiki/Timsort
            âš“ Introsort ; https://en.wikipedia.org/wiki/Introsort
          # Distribution sorts
            âš“ Counting sort ; https://en.wikipedia.org/wiki/Counting_sort
        # Other
          âš“ Bitonic mergesort ; https://en.wikipedia.org/wiki/Bitonic_sorter
            bitonic sequence

      #ï¸âƒ£ Subsequences ; https://en.wikipedia.org/wiki/List_of_algorithms#Subsequences
        âš“ longest common subsequence (LCS) ; https://en.wikipedia.org/wiki/Longest_common_subsequence
        âš“ğŸ“ Longest increasing subsequence ; https://en.wikipedia.org/wiki/Longest_increasing_subsequence
          This subsequence is not necessarily contiguous, or unique.
          #ï¸âƒ£ğŸ“ Example ; https://en.wikipedia.org/wiki/Longest_increasing_subsequence#Example
          #ï¸âƒ£ğŸ“ Efficient algorithms ; https://en.wikipedia.org/wiki/Longest_increasing_subsequence#Efficient_algorithms
      #ï¸âƒ£ Substrings ; https://en.wikipedia.org/wiki/List_of_algorithms#Substrings
        âš“ğŸ“ String-searching algorithm | string-matching algorithms ; https://en.wikipedia.org/wiki/String-searching_algorithm
          # Contents
            haystack, (needles, patterns)
            ğŸš£ In practice, the method of feasible string-search algorithm may be affected by the string encoding
      

      âš“ğŸ“ running total | rolling total | partial sum << https://en.wikipedia.org/wiki/Running_total
        The purposes of a running total are twofold ...

    âš“ Linear programming (LP) | linear optimization | ì„ í˜• ê³„íšë²•, ç·šå‹è¨ˆåŠƒæ³• ; https://en.wikipedia.org/wiki/Linear_programming
      #ï¸âƒ£ Covering/packing dualities ; https://en.wikipedia.org/wiki/Linear_programming#Covering/packing_dualities
        - Maximum independent set
        âš“ğŸ“ Set cover problem ; https://en.wikipedia.org/wiki/Set_cover_problem
        âš“ Packing problems ; https://en.wikipedia.org/wiki/Packing_problems
          âš“ Bin packing problem ; https://en.wikipedia.org/wiki/Bin_packing_problem
            When the number of bins is restricted to 1 and each item is characterised by both a volume and a value
            , the problem of maximizing the value of items that can fit in the bin is known as the knapsack problem.
  #ï¸âƒ£ Computational mathematics ; https://en.wikipedia.org/wiki/List_of_algorithms#Computational_mathematics
    #ï¸âƒ£ Geometry ; https://en.wikipedia.org/wiki/List_of_algorithms#Geometry
      #ï¸âƒ£ Line segment intersection: finding whether lines intersect, usually with a sweep line algorithm
        âš“ğŸ“ Sweep line algorithm | plane sweep algorithm ; << https://en.wikipedia.org/wiki/Sweep_line_algorithm
          (conceptual sweep line, sweep surface)
          âš“ Voronoi diagram ; https://en.wikipedia.org/wiki/Voronoi_diagram
          â”” âš“ Fortune's algorithm ; https://en.wikipedia.org/wiki/Fortune%27s_algorithm

      âš“ Closest pair of points problem ; https://en.wikipedia.org/wiki/Closest_pair_of_points_problem
      âš“ Area of a triangle ; << https://en.wikipedia.org/wiki/Area_of_a_triangle
        #ï¸âƒ£ğŸ” Using coordinates ; https://en.wikipedia.org/wiki/Area_of_a_triangle#Using_coordinates
      âš“ Tessellation | tiling ; << https://en.wikipedia.org/wiki/Tessellation
    #ï¸âƒ£ Number theoretic algorithms ; https://en.wikipedia.org/wiki/List_of_algorithms#Number_theoretic_algorithms
      âš“ Generation of primes ; https://en.wikipedia.org/wiki/Generation_of_primes
        âš“ğŸ“ sieve of Eratosthenes ; https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes
          #ï¸âƒ£ Algorithm and variants ; https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes#Algorithm_and_variants
          #ï¸âƒ£ Algorithmic complexity ; https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes#Algorithmic_complexity
      âš“ Integer factorization ; https://en.wikipedia.org/wiki/Integer_factorization
        âš“ğŸ“ trial division ; https://en.wikipedia.org/wiki/Trial_division
      âš“ Multiplication algorithm ; https://en.wikipedia.org/wiki/Multiplication_algorithm
        âš“ Karatsuba algorithm ; https://en.wikipedia.org/wiki/Karatsuba_algorithm
      âš“ Primality test ; https://en.wikipedia.org/wiki/Primality_test
        ğŸ“ &comment; It is in NP. it different with relation between PRIEMS and COMPOSITES;witness CO-NP.
    #ï¸âƒ£ Numerical algorithms ; https://en.wikipedia.org/wiki/List_of_algorithms#Numerical_algorithms
      #ï¸âƒ£ Root finding ; https://en.wikipedia.org/wiki/List_of_algorithms#Root_finding
       & https://en.wikipedia.org/wiki/Root-finding_algorithms#Bracketing_methods
        #ï¸âƒ£ Bracketing (no derivative) ; https://en.wikipedia.org/wiki/Root-finding_algorithms#Bracketing_methods
          âš“ğŸ“ğŸ“ Bisection method | ì´ë¶„ë²•, äºŒåˆ†æ³• ; https://en.wikipedia.org/wiki/Bisection_method
            (bisection method | dichotomy method | binary search method | interval halving)
            ğŸ” It is a very simple and robust method, but it is also relatively slow.
              ğŸš£ Because of this, it is often used to obtain a rough approximation to a solution which is then used as a starting point for more rapidly converging methods.
            For polynomials, more elaborate methods exist for testing ...
            #ï¸âƒ£ The method ; https://en.wikipedia.org/wiki/Bisection_method#The_method
              #ï¸âƒ£ Algorithm ; https://en.wikipedia.org/wiki/Bisection_method#Algorithm

    #ï¸âƒ£ Optimization algorithms ; https://en.wikipedia.org/wiki/List_of_algorithms#Optimization_algorithms
      âš“ Subset sum problem (SSP) ; https://en.wikipedia.org/wiki/Subset_sum_problem
        ğŸ“ &comment; SSP's Time complexity: pseudo-polynomial time.
        # Contents
          ğŸ” SSP is a special case of the knapsack problem and of the multiple subset sum problem.
          ... The sub-problem for two elements sum is known as two-sum ...
          #ï¸âƒ£ Exponential time algorithms ; https://en.wikipedia.org/wiki/Subset_sum_problem#Exponential_time_algorithms
            #ï¸âƒ£ğŸ“ Horowitz and Sahni ; https://en.wikipedia.org/wiki/Subset_sum_problem#Horowitz_and_Sahni
          #ï¸âƒ£ Pseudo-polynomial time dynamic programming solutions ; https://en.wikipedia.org/wiki/Subset_sum_problem#Pseudo-polynomial_time_dynamic_programming_solutions
        âš“ 3SUM ; << https://en.wikipedia.org/wiki/3SUM
          ğŸ“ &comment; It is not NP-completeness because it's quadratic algorithm uses "hashing" instead of brute-force search.
            the 3SUM problem can be 'reduced' to the subset sum problem.
          #ï¸âƒ£ Quadratic algorithm ; https://en.wikipedia.org/wiki/3SUM#Quadratic_algorithm
        âš“ Multiple subset sum ; << https://en.wikipedia.org/wiki/Multiple_subset_sum
      
      âš“ğŸ“ Interval scheduling ; << https://en.wikipedia.org/wiki/Interval_scheduling
        subset of intervals
      âš“ Minimax ; https://en.wikipedia.org/wiki/Minimax
      âš“ Vertex k-center problem << https://en.wikipedia.org/wiki/Vertex_k-center_problem
        ğŸ“ &comment; "center" means ğŸ”— Graph center
  #ï¸âƒ£ Computer science ; https://en.wikipedia.org/wiki/List_of_algorithms#Computer_science
    #ï¸âƒ£ Cryptography ; https://en.wikipedia.org/wiki/List_of_algorithms#Cryptography
    #ï¸âƒ£ Machine learning and statistical classification ; https://en.wikipedia.org/wiki/List_of_algorithms#Machine_learning_and_statistical_classification
    #ï¸âƒ£ Programming language theory ; https://en.wikipedia.org/wiki/List_of_algorithms#Programming_language_theory
      #ï¸âƒ£ Parsing ; https://en.wikipedia.org/wiki/List_of_algorithms#Parsing
        & âš“ Parsing algorithms ; << https://en.wikipedia.org/wiki/Parsing
        âš“ Top-down parsing ; https://en.wikipedia.org/wiki/Top-down_parsing
        âš“ Bottom-up parsing ; https://en.wikipedia.org/wiki/Bottom-up_parsing
  #ï¸âƒ£ Operating systems algorithms ; https://en.wikipedia.org/wiki/List_of_algorithms#Operating_systems_algorithms
    #ï¸âƒ£ Scheduling ; https://en.wikipedia.org/wiki/List_of_algorithms#Scheduling
    & âš“ Optimal job scheduling problems ; https://en.wikipedia.org/wiki/Optimal_job_scheduling
      # Other requirements
        âš“ Interval scheduling ; https://en.wikipedia.org/wiki/Interval_scheduling




  âš“ Streaming algorithm ; << https://en.wikipedia.org/wiki/Streaming_algorithm
  âš“ Searching algorithm ; << https://en.wikipedia.org/wiki/Search_algorithm
    ğŸ“ &comment; It can not be bound in one paragraph, so it is in placed here. describe details to other paragrpahs.
  âš“ Hybrid algorithm ; << https://en.wikipedia.org/wiki/Hybrid_algorithm
    ğŸ“ &comment; It can not be bound in one paragraph, so it is in placed here. describe details to other paragrpahs.
  
  âš“ğŸ“ Parametric search ; << https://en.wikipedia.org/wiki/Parametric_search
    #ï¸âƒ£ğŸ“ Technique ; https://en.wikipedia.org/wiki/Parametric_search#Technique
      ğŸš£ (test algorithm, decision algorithm). discontinuously
      Advanced versions of the parametric search technique use a parallel algorithm as the test algorithm ...
    #ï¸âƒ£ğŸ“ Comparison with binary search ; https://en.wikipedia.org/wiki/Parametric_search#Comparison_with_binary_search
  âš“ Couting problem ; << https://en.wikipedia.org/wiki/Counting_problem_(complexity)
(List of algorithms basics)
  âš“ğŸ“ Pointer jumping | path doubling ; https://en.wikipedia.org/wiki/Pointer_jumping
  
  âš“ Parallel algorithm ; https://en.wikipedia.org/wiki/Parallel_algorithm
  âš“ sequential algorithm | serial algorithm ; https://en.wikipedia.org/wiki/Sequential_algorithm
# Number-theoretic algorithms
  (other Number-theoretic) <<
    âš“ combination ; https://en.wikipedia.org/wiki/Combination
      ğŸ“ &comment; Time Complexity: O(r * (n choose r)) ğŸ”— https://stackoverflow.com/questions/53419536/what-is-the-computational-complexity-of-itertools-combinations-in-python
        , where n is the length of the iterable and r is the size of the combinations being generated.
      - k-combination
    âš“ permutation ; https://en.wikipedia.org/wiki/Permutation
      Technically, a permutation of a set S is defined as a bijection from S to itself.
âš“ List of random number generators ; << https://en.wikipedia.org/wiki/List_of_random_number_generators
  âš“ Permuted congruential generator (PCG) ; https://en.wikipedia.org/wiki/Permuted_congruential_generator
âš“ List of computability and complexity topics ; https://en.wikipedia.org/wiki/List_of_computability_and_complexity_topics
  #ï¸âƒ£ Named problems ; https://en.wikipedia.org/wiki/List_of_computability_and_complexity_topics#Named_problems
    âš“ Knapsack problem ; https://en.wikipedia.org/wiki/Knapsack_problem
    âš“ List of knapsack problems ; << https://en.wikipedia.org/wiki/List_of_knapsack_problems

    âš“ Satisfiability ; https://en.wikipedia.org/wiki/Satisfiability
      âš“ Boolean satisfiability problem | ì¶©ì¡± ê°€ëŠ¥ì„± ë¬¸ì œ << https://en.wikipedia.org/wiki/Boolean_satisfiability_problem
        (| propositional satisfiability problem | SATISFIABILITY | SAT | B-SAT)
        #ï¸âƒ£ Complexity ; https://en.wikipedia.org/wiki/Boolean_satisfiability_problem#Complexity
          # 3-satisfiability | 3-SAT | 3CNFSAT | 3-satisfiability
            The 3-SAT instance ... reduced to a clique problem.
  
  âš“ List of NP-complete problems ; << https://en.wikipedia.org/wiki/List_of_NP-complete_problems
  âš“ List of unsolved problems in computer science ; << https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_computer_science
  âš“ List of unsolved problems in mathematics ; https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_mathematics
    #ï¸âƒ£ Unsolved problems ; https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_mathematics#Unsolved_problems
      #ï¸âƒ£ Dynamical systems ; https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_mathematics#Dynamical_systems
        âš“ Collatz conjecture ; https://en.wikipedia.org/wiki/Collatz_conjecture
  âš“ List of undecidable problems ; << https://en.wikipedia.org/wiki/List_of_undecidable_problems


âš“ Category:Computer graphics algorithms ; https://en.wikipedia.org/wiki/Category:Computer_graphics_algorithms
  âš“ Line drawing algorithm ; https://en.wikipedia.org/wiki/Line_drawing_algorithm
    âš“ğŸ“ Bresenham's line algorithm ; https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm
      # Method
        the top-left is (0,0) such that pixel coordinates increase in the right and down directions (e.g. that the pixel at (7,4) is directly above the pixel at (7,5)), and the pixel centers have integer coordinates.
          ğŸ‡¹ğŸ‡· ì¢Œí‘œ (x, y)ëŠ” í”½ì…€ì˜ ì¤‘ì‹¬ì„ ë‚˜íƒ€ë‚´ë©°, ì´ ì¢Œí‘œëŠ” ì‹¤ì œë¡œ í™”ë©´ìƒì˜ ì ì´ ì•„ë‹ˆë¼ í”½ì…€ ê·¸ ìì²´ë¥¼ ì°¸ì¡°í•œë‹¤.
          ğŸ›ï¸ e.g. (0,0) is at the top left corner of the grid, (1,1) is at the top left end of the line and (11, 5) is at the bottom right end of the line

        The algorithm will be initially presented only for the octant in which the segment goes down and to the right ( x 0 â‰¤ x 1 and y 0 â‰¤ y 1), and its horizontal projection x 1 âˆ’ x 0 is longer than the vertical projection y 1 âˆ’ y 0  (the line has a positive slope less than 1).
          ğŸ‡¹ğŸ‡·: 2ì°¨ì› í‰ë©´ì—ì„œ Octant ë¡œ í‘œì‹œí•  ê²ƒì´ë‹¤.
            Octant 1: ê¸°ìš¸ê¸° 0 ì´ìƒ, 45ë„ ì´í•˜ (â†’ ë°©í–¥)
            Octant 2: ê¸°ìš¸ê¸° 45ë„ ì´ìƒ, 90ë„ ì´í•˜ (â†— ë°©í–¥)
            Octant 3: ê¸°ìš¸ê¸° 90ë„ ì´ìƒ, 135ë„ ì´í•˜ (â†– ë°©í–¥)
            Octant 4: ê¸°ìš¸ê¸° 135ë„ ì´ìƒ, 180ë„ ì´í•˜ (â† ë°©í–¥)
            Octant 5: ê¸°ìš¸ê¸° 180ë„ ì´ìƒ, 225ë„ ì´í•˜ (â†™ ë°©í–¥)
            Octant 6: ê¸°ìš¸ê¸° 225ë„ ì´ìƒ, 270ë„ ì´í•˜ (â†˜ ë°©í–¥)
            Octant 7: ê¸°ìš¸ê¸° 270ë„ ì´ìƒ, 315ë„ ì´í•˜ (â†™ ë°©í–¥)
            Octant 8: ê¸°ìš¸ê¸° 315ë„ ì´ìƒ, 360ë„ ì´í•˜ (â†˜ ë°©í–¥)

        In this octant, for each column x between x 0 and x 1, there is exactly one row y (computed by the algorithm) containing a pixel of the line
        , while each row between y 0 and y 1 may contain multiple rasterized pixels.
          ğŸ‡¹ğŸ‡· Octant í‰ë©´ì´ê¸° ë–„ë¬¸ì—, ê¸°ìš¸ê¸°ê°€ 1ë³´ë‹¤ ë‚®ì•„ì„œ ë‹¤ìŒê³¼ xì¢Œí‘œ 1ê°œë‹¹ column ì€ 1ê°œë§Œ ê°€ì§ˆ ìˆ˜ ìˆì§€ë§Œ, y ì¢Œí‘œ 1ê°œë‹¹ 1ê°œ ì´ìƒì˜ row ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.

        ... ğŸš£ it keeps an error bound at each stage, which represents the negative of the distance from (a) the point where the line exits the pixel to (b) the top edge of the pix
          


    âš“ Xiaolin Wu's line algorithm ; https://en.wikipedia.org/wiki/Xiaolin_Wu's_line_algorithm


================================================
etc Allgoirhtms
  âš“ 2048 (video game) ; https://en.wikipedia.org/wiki/2048_(video_game)
    developer Gabriele Cirulli
    https://play2048.co/
  âš“ Endgame tablebase ; https://en.wikipedia.org/wiki/Endgame_tablebase



----------------------------------------------
coding competitions
  âš“ Google ğŸ”ª Code jam ; https://codingcompetitions.withgoogle.com/codejam/archive
  âš“ Google ğŸ”ª Hash code ; https://codingcompetitions.withgoogle.com/hashcode/archive
