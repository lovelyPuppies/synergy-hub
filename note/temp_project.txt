https://pmnxis.github.io/posts/my_first_commerical_rust_embedded_product_1/

XP-Pen Driver ... Gimp 에서 인식시키기
  * Deco LW:
    https://www.xp-pen.com/download/deco-lw.html
    ... download .deb file
    %shell> sudo dpkg -i XPPENLinux3.4.9-240607.deb


개인정보보호법.. 어플리케이션.. 구글 드라이브 참고..
  you will need to provide:
    An official link to your app's Privacy Policy
    A YouTube video showing how you plan to use the Google user data you get from scopes
    A written explanation telling Google why you need access to sensitive and/or restricted user data
    All your domains verified in Google Search Console 

DSP Digital signal Processing? 이미지와 관련..?

제가 언급한 ~/.local/share/applications/kakaotalk.desktop 파일은 애플리케이션 메뉴에 KakaoTalk을 추가하거나, 바탕화면에 아이콘을 생성하는 데 사용되는 파일입니다. 이 파일을 생성하면 다음과 같은 기능을 제공합니다:
  애플리케이션 메뉴:
      Ubuntu의 애플리케이션 메뉴에서 KakaoTalk을 검색하고 실행할 수 있게 됩니다. 예를 들어, Ubuntu의 좌측 상단에 있는 "활동" 메뉴에서 KakaoTalk을 검색하면 이 아이콘이 나타납니다.
  바탕화면 아이콘:
      이 파일을 바탕화면에 복사하면 바탕화면에서 바로 KakaoTalk을 실행할 수 있는 아이콘이 생성됩니다.

  좌측 바(Launcher)에 고정된 아이콘
    좌측 바(Launcher)에 고정된 아이콘은 애플리케이션을 실행하거나 고정할 때 자동으로 생성됩니다.
    앱을 실행한 후, 좌측 바에 아이콘 고정:
        KakaoTalk을 한 번 실행하면, 해당 아이콘이 좌측 바에 나타납니다. 아이콘을 우클릭한 후 "즐겨찾기에 추가" 또는 "바에 고정"을 선택하면 아이콘이 좌측 바에 고정됩니다.
    .desktop 파일을 통한 고정:
        .desktop 파일을 사용하여 애플리케이션을 고정할 수도 있습니다. 이 파일을 생성하고 앱을 실행하면, 해당 아이콘이 좌측 바에 자동으로 나타납니다.
  따라서, 제가 설명한 .desktop 파일은 애플리케이션 메뉴에 아이콘을 추가하거나 바탕화면에 아이콘을 생성하는 방법이며, 이를 통해 KakaoTalk을 실행할 수 있습니다. 좌측 바에 아이콘을 고정하려면 KakaoTalk을 한 번 실행한 후 고정하면 됩니다.

    
에이다 부스트 학습; 가능한 모든 커널을 가지고 이미지 전체를 스캔하여 하르 특징을 계산한다?
https://docs.ultralytics.com/models/yolo-nas/

0 0?? 
  0: Dump backup. 0 is not dump
  0: fcsk (File System check). 0s is not check

  /etc/fstab 문제로 부팅이 실패할 경우:
  /etc/fstab에서 오류가 발생해 부팅이 실패하거나 중단된 경우, 다음과 같은 방법으로 문제를 해결할 수 있습니다:

  복구 모드에서 수정: 부팅 시 Grub 메뉴에서 복구 모드로 부팅한 후, /etc/fstab 파일을 수정할 수 있습니다.

  라이브 USB로 부팅 후 수정: 시스템이 정상적으로 부팅되지 않는 경우, 라이브 USB를 사용해 시스템을 부팅한 후, /etc/fstab 파일을 마운트하고 수정할 수 있습니다.
❔ Hard link, Soft link
  https://en.wikipedia.org/wiki/Hard_link
  https://en.wikipedia.org/wiki/Symbolic_link



MediaPipe
  https://github.com/google-ai-edge/mediapipe
    # Contents
      🔏 License: Apahce License 2.0

      These libraries and resources provide the core functionality for each MediaPipe Solution:
        🪱 MediaPipe Tasks: Cross-platform APIs and libraries for deploying solutions. Learn more.
        🪱 MediaPipe models: Pre-trained, ready-to-run models for use with each solution.
      These tools let you customize and evaluate solutions:
        🪱 MediaPipe Model Maker: Customize models for solutions with your data. Learn more.
        🪱 MediaPipe Studio: Visualize, evaluate, and benchmark solutions in your browser. Learn more.

    Fash Mesh ; https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_mesh.md
  

  https://ai.google.dev/edge/mediapipe/solutions/guide
    https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker
      # Models
        Face detection model, Face mesh model, Blendshape prediction model

  https://mediapipe-studio.webapps.google.com/demo/face_landmarker
 🌟 탈모약>>>>>>>>>>>>>>>>>>>> 프로페시아, 아보다트, 미녹시딜?
  
https://en.wikipedia.org/wiki/Facial_Action_Coding_System
  ... blendshape scores (coefficients representing facial expression) from https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker
  https://mediapipe-studio.webapps.google.com/home

MediaPipe API
  # Python
    # mp
      ⚓ Overview ; https://ai.google.dev/edge/api/mediapipe/python/mp

https://stackoverflow.com/questions/59051631/what-is-the-use-of-stub-files-pyi-in-python

❓ 파이썬에서 주로 C++로 구현된 부분을 호출하기 때문일 수 있습...
  pyi stub pile
https://pypi.org/project/type-enforced/

❓ 휴대용 책 지지대

Software Design Specification
  Software Design Design Method: -
  Hardware Constraints: -
  UI: - : pyqt6 로 토글형 스티커 IO 설계
알고리즘
  ..함수 간의 동작?


👍📰 GPT APP: Writing Assitant
  "어색하게 이어지는 부분을 수정하고 배려있는 표현과 격식체를 사용해서, 전문적으로 구체적인 근거와 함께, 전체적으로 잘 다듬어줘."
  // 수요 증가에 대한 주장을 피하자.
  // 고령자의 일상과 기억을 기록하는 문제가 사회적 과제로 떠오르고 있다 라는 구체적인 사례나 뉴스가 부족.

🌪️ TODO: MonkeyType
  타입 힌트 자동 생성 도구:
    MonkeyType:
        MonkeyType은 코드 실행 시 동적 추적을 통해 실제로 사용하는 타입을 기반으로 타입 힌트를 추론하고, 이를 바탕으로 타입 힌트를 생성해 줍니다.
    ... protobuf 에서 추적 안되는 것도 생성해주는지 체크.
  https://pypi.org/project/types-protobuf/

사용자들은 생성된 이미지와 텍스트 기록을 웹 또는 모바일 앱을 통해 쉽게 확인할 수 있습니다. 또한, 이 기록을 디지털 앨범으로 관리하거나 SNS에 공유할 수 있는 기능을 제공합니다​.

**** Usecase, Class Diagram, Activity diagram
  유스케이스 다이어그램: 시스템과 사용자 간 상호작용을 설명하는 고수준 개요.
  활동 다이어그램: 유스케이스나 프로세스의 세부 흐름.
  상태 다이어그램: 객체의 상태 전이.
  시퀀스 다이어그램: 객체 간의 메시지 흐름.
  클래스 다이어그램: 시스템의 구조와 관계.

  추천 순서:
    유스케이스 다이어그램: 전체 시스템의 개요를 먼저 설명.
    활동 다이어그램: 주요 프로세스의 세부 흐름을 보여줌.
    시퀀스 다이어그램: 상호작용 흐름을 시각화.
    클래스 다이어그램: 시스템의 구조를 설명. 정적 구조.
    상태 다이어그램: 객체의 동적 상태 변화를 설명.

기술문서는 서술형 X. 수학적으로.


애플 센터스테이지 기능 구현해보기?? 
  카메라가 사용자를 따라다니면서 포착하는듯
  smooth 하게 천천히 이동해야 함.

BlazeFace (short-range)
  스마트폰 카메라나 웹캠에서 셀카 같은 이미지에서 하나 또는 여러 개의 얼굴을 감지하기 위한 가벼운 모델입니다. 이 모델은 단거리에서 전면 카메라 이미지에 최적화되어 있습니다. 이 모델 아키텍처는 사용자 지정 인코더가 있는 Single Shot Detector(SSD) 합성곱 네트워크 기술을 사용합니다. 자세한 내용은 Single Shot MultiBox Detector에 대한 연구 논문을 참조하세요.

PPT...
  이런 경우, 프로젝트의 기획 배경, 기술적인 설명, 개인적인 동기 순서로 나열하면 더 자연스럽게 흐름을 정리할 수 있습니다. 수정된 버전은 다음과 같습니다:

GaussianBlur, Sobel filter, 이진화, ConvexHull, Edge Area, Mask, HSV 색 처리.. 엣지 검출.. sobel maskvs canny edge, 캘리브레이션
  ...


  
📰 카카오톡 API.. authorization code..
📰 zoom-in, zoom-out 기능. 자동 추적 기능 만들기.. smooth 하게 움직이도록?
https://en.wikipedia.org/wiki/Docent
청년 안심주택 임대보증금지원 ; https://soco.seoul.go.kr/youth/main/contents.do?menuNo=400022
복지앱
상조보험




📰📰📰📰📰 >>>>>>>>>>
  >>>>>>>>> 음성으로부터 감정 분석하는 기술 필요...: 예: Cogito, Beyond Verbval, Affectiva, Nexmo (Vonage API)

  니다 -> X. 명사형으로 끝내는 것이 나은 듯?
  개인화된>> 제거
  자연어 처리(NLP) ..  해야 할듯. 생성형 AI 가 다 해 주는거아닌가? 문장으로부터 키워드를 추출해서 이미지를 생성하는 것이 아니라 동적인 이미지를 생성해야 해서.
  >> 가독성있게 추려달라...
  프라이빗 클라우드..

  디지털 앨범은 전통적인 사진첩을 디지털화한 것으로, 사용자들이 저장한 사진, 영상, 텍스트 기록 등을 한 곳에 모아 쉽게 관리할 수 있는 시스템입니다. 이 앨범은 사용자들이 자신만의 소중한 순간을 정리하고 언제든지 다시 확인할 수 있는 공간을 제공합니다. 특히 이 서비스에서는 음성 기록과 이미지, 위치 정보 등이 통합되어 기록되기 때문에, 이러한 데이터를 시각적으로 정리해주는 개인 맞춤형 기록 보관소 역할을 합니다.


====================================
AL/DL Momentu
  인텔 가우디 사피온??

자동차가.. 매우 보수적인 집단이었다..
  중국- BYD 체리, 그레이트볼?.. 자동ㅈ차 나오는주기가 2년ㄷㄷ?
전기차는.. 바퀴를 돌리는 동력이 화석기관에서 오냐 동력기관에서 오냐..
  오히려 하이브리드가 더 나을  수 있다?>
  iCQA? GQ 센서?..로우레벨 드라이버 경험해보는 것이 좋다.
  커널쪽을 좀 파봐라..
인텔이 서버쪽은강했는데 엣지가 약해서.. 엣지 분야를 개발중이다?
  Hands-on 을 같이하면서 써보는 것이 목표.



평서형 종결어미
해라체 (격식 반말, 평서형 다체)

Imagent Large scale Visual Recognition challenge. ILSVRC
  사람기준 5%.. 2015년 ResNet, GoogleNet-v4, SENet.. 오차율 계쏙 줄어듬..



딥러닝과 연역적인 방법이 다르다.
  딥러닝은.. 데이터셋 학습이 필요.
openvino.. pytorch 에서도 extension 쓸 수 있닫고 한다. 
  대신 성능 90/100%

AI Growth.
AI Products
  항만.. 불법ㅇ으로 들어오는 외노자 관리?
  LG energy solution, Hyundai, Hanwha, 롯데, 현대중공업, 네이버, View the Invisible, Know the Unknown (VUNO?)
  🚣 SAMSUNG Medison, DELL, Lottel, EMARt, Beart Lobotics, 신한은행, LG Energy Solution
    SAMSUNG Medison 가 가장 많이 일했다고 한다.
    NVIDIA 가 2년 보증이라서... intel 이랑 많이 일햇나?..
    애기..에게 뭔가 판단할 떄 사용?? 병원에서 쓰는거라 Watt 를 많이 못쓴다고 한다. 
    병원에서 기기들이 전력을 많이쓰면 문제가 생긴다. 저전력에서 구동가능한 것이 필요해서.. 인텔에서 굉장히 성공적인 케이스로 분류한다고 함.
    OpenVino - Intel
  완전 고성능은 NVIDIA, 중간 성능 기능으로 OpenVINO 가 적하다고 한다.
  >>>>> 면접볼때마다 여기에서는 다른 것을 할 수 있다 어필할 정도로. 공부해야 할듯.
AI 는 자연으로부터 영감을 많이 받았다고 한다.
삐삐삐-차량 소리.. 박쥐의 초음파..로..부터 영감을 받았다.
사이드?..어쩌구 저쩌구는 사막뱀의 열추적? 으로부터 영감을 바았다.
❓ 세임펜? 사진?을 집으면 어떤 동물인지 말해주는 제품?
  펫나우"와 같은 AI 기반 동물 인식 기술을 가리키는 것
주차관리시스템 ...
🆚 텐서플로우, pytorch 언제사용하나 물어볼가

❓ 면접.. 그거 지금 있는데 --> 지금 그럼 회사는 어떤 시스템을 사용하고 어떤 네트웤을 쓰고 계시나요?
  그 모델 같은 경우는 이런 모델은 낮춘다면 .. time-critical 하지 않아서 FPS 는 조금 떨어지겟지만, .. 어쩌구.. 

Image segementation
  설명: positioning 판단하죠? 사람 영역 이렇게 판단하죠?
OCR


인텔 코리아 곽우영
  CPG :Communication Product Group? 
  인텔 모바일, 타블릿.. 하는 조직

반도체: conductor, insulator 의 중간 물질..
  모든 가전제품에 들어가는..
  원자재: 갈륨, 실리콘.. ..

Indstur

World=changing Technology. 
  -- 
  Diriving Product Leadership


>>>>>> [Intel] Lunar Lake - AI PC
  On-Device.. NPU 자체가 없었다.
올해부터 나오는 PC는 On-Device 라고 명칭한다고 한다. NPU 탑재.
  그 중 Lunar Lake
  CPU, GPU, NPU 가 함께 내장된다는듯.
    HP GPU? LNC?

  45 TOPS 가 넘어야 On-Device 라고 한다고 한다.

  🪱 Intel - 포베로스는 칩렛 구조의 3D 반도체 적층 기술이다

  Lunar Lake - 🪱 MOP package (Memory on Package)
    Foveros
  
  * Ultimate Thin& Light Systems
    - Detachables
    - Premium Fanless Latops
    - Premium Modern T&L Latops
  ?? LNL Archiecture
  >>>> ❔ Lunar Lake Architecture

⚓ Board Support Package (BSP) https://en.wikipedia.org/wiki/Board_support_package
BSP? LPDRR5x x64, LPDRR6x x64
SoC 도식화된 그림?
AEP
  주로 인텔의 Optane DC Persistent Memory 기술과 관련된 용어로, Apache Pass라는 코드명을 가지고 있는 Intel's Optane DC Persistent Memory (AEP: Apache Pass)
System on a chip (SoC) ; System on a chip

* Motherboard. 앞/뒷면
  Heat sync.. IO Module.., 
  노트북에서 주로 열나는 분, Soc, 메모리, 파워
* Die = Tile?
🆚 Chips vs Dies, Chiplets vs Dielets
  The terms Chiplets and Dielets, are frequently used interchangeably but strictly speaking the suffix “let” indicates a diminutive size typically below 100mm2. Chips refer either to the design IP or to the packaged dies. Dies typically mean bare singulated instantiations of the chip design that are not packaged

  https://www.reddit.com/r/chipdesign/comments/1e9izse/chips_vs_dies_vs_chiplets_vs_dielets/?rdt=41376
  메모리간 CPU 간의 설계를 잘해야한다. 안그러면 메모리 성능이 깎인다. 둘이 모두 가까우면 좋겠지만..
    항상 이 부분이 문제라고 한다.
  삼성도 패키징을 잘한다고 한다. 근데 CPU 를 잘못만듦. intel, Tsmc 는 잘 만듦.
  Tsmc: 5nano, 10nano 를 잘한다고한다. 7anano,  >> tsmc 3 nano.. ? 반도체 수율 (Yield) 이 안좋아서 고민하고 있다고 한다. 정상적으로 작동하는 칩의 비율.
  intel: 14nano (5년 전).. 을 잘한다고 한다.

AI 연산을 하게 되면
  Relo, Convolution --> framework 에서불러주는 API 일 뿐이고.
    실제로 컴퓨터 내부에서는 3*3 -> ...
    CPU 는.. 곱하고 더하고.. 총 4번? 이 필요하다고 한다.
    GPU 같은 경우는: ...
      GPU 안에는 실제로는 GPGPU 를 쓴다고 한다. SIMD single instruction, muli data 를 지원?
    NLP: 
  GPGPU (General-Purpose computing on Graphics Processing Units)
  사피온이 리벨리온한테 통합이 됬고 퓨리오는 아직 좋은 성능을 못내고 있다고 함.
  ❓ 면접: CPU GPGPU, NPU 연산.. 내부 장치, TPU
  ㄱ기존의 pytorch 로 하면 one-stinrctuon - one-instruction 이엇지만
    ㅏㅎ나 하나더ㅏ고 곱하고 곱하고.. 
    몇가지 연산을 하나의 instruction..
    Compute Tile 2D
    OpenVino 를 쓰면 이거를 한번의 명령에 여러 개의 연산을 적용할 수 있다고 한다.
    oficcialy, OpenVino 는 인텔에서만 돌아간다고 한다.
  
    Lunar Lake는 **마이크로컨트롤러(Microcontroller)**
      https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units
    
GPG key 
  ⚓ https://en.wikipedia.org/wiki/GNU_Privacy_Guard
WARNING:absl:You are saving your model as an HDF5 file via model.save() or keras.saving.save_model(model). This file format is considered legacy. We recommend using instead the native Keras format, e.g. model.save('my_model.keras') or keras.saving.save_model(model, 'my_model.keras')
  absl은 Abseil이라는 라이브러리의 약자입니다. 이는 Google에서 제공하는 Python 로깅 및 유틸리티 라이브러리로, Keras에서 모델 저장 시 경고 메시지를 로깅하는 데 사용됩니다. Abseil은 로그 처리와 같이 표준 로깅을 보다 확장된 기능으로 제공하기 위해 종종 사용됩니다.
  
  
❓ 면접: 네트워크 ip 주소, 서브넷 마스크, 게이트웨이
https://rufus.ie/ko/

precetron? CNN, RNN ~ LLM ~ OpenVINO


SEO / 패싯?

Install List
  from class room..
      sudo apt install -y vim git-all htop net-tools tree mplayer mesa-utils  

  OpenVIno
    # https://docs.openvino.ai/2024/get-started/install-openvino.html
    🪠%shell> 
      wget https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB
      sudo cp GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB /etc/apt/trusted.gpg.d/intel-sw-products.gpg
      echo "deb https://apt.repos.intel.com/openvino/2023 ubuntu22 main" | sudo tee /etc/apt/sources.list.d/intel-openvino-2023.list
      sudo apt update -y
      apt-cache search openvino
      sudo apt install -y openvino


  Whale
    🪠%shell> 
      wget https://installer-whale.pstatic.net/downloads/installers/naver-whale-stable_amd64.deb -O /tmp/naver-whale-stable_amd64.deb && \
      sudo dpkg -i /tmp/naver-whale-stable_amd64.deb
    🚨 (issue): bug; 📅 
    Unintsall; %shell> sudo apt remove -y naver-whale-stable

  Telegram
    🪠%shell> sudo snap install telegram-desktop
  zoom
    🪠%shell> sudo snap install zoom-client
    https://snapcraft.io/zoom-client
wget -O - https://raw.githubusercontent.com/p1ratrulezzz/telegram-linux-installer/master/telegram-installer.sh | bash

\\INTEL-TEACHER\shared

\\10.110.16.180
\\10.10.16.180

sudo apt update
sudo apt install ibus ibus-hangul
ibus-setup
ibus restart
설정 메뉴에서 Settings를 엽니다.
Region & Language 메뉴로 이동합니다.
Input Sources에서 + 버튼을 클릭하고, **Korean (Hangul)*
/// 이력서  /성적증명서


Commands
  ls -al         # == ll    ❓ 

📰👍 Snap은 리눅스에서 소프트웨어를 패키징하고 배포하는 새로운 방법입니다. Canonical (우분투를 개발하는 회사)에서 개발한 이 기술은 여러 리눅스 배포판에서 동일한 애플리케이션을 일관되게 설치하고 실행할 수 있도록 돕습니다. Snap의 주요 특징과 장점은 다음과 같습니다:
snap install telegram-desktop   # https://en.wikipedia.org/wiki/Snap_(software)
  https://github.com/canonical/snapcraft
  https://snapcraft.io/store
  e.g.
    snap find editor
    snap install telegram-desktop



https://telegram.org/dl/desktop/linux
pip install jupyter

(how); firefox, open the new tab with same url
  Alt + D 또는 F6을 눌러 주소 표시줄로 커서를 이동합니다.
  Alt + Enter를 누르면 현재 탭의 링크가 새 탭에서 열립니다.

  이거는 모든 브라우저 공통이야?
  Popup View for Google™ Translate ; https://addons.mozilla.org/ko/firefox/addon/dictionary-anywhere/


========= 왕성식 교수님
=========-- 백재우 교수님   // 수치해석/수학
=========-- 김성근 교수님   // 컴퓨터 비전

AI
  Perceron ; https://en.wikipedia.org/wiki/Perceptron
  CNN ; https://en.wikipedia.org/wiki/Perceptron
    르네부터?
  RNN ; https://en.wikipedia.org/wiki/Recurrent_neural_network
  LSTM ; https://en.wikipedia.org/wiki/Long_short-term_memory
  LLM ; https://en.wikipedia.org/wiki/Large_language_model
    Transofmrer, GPT, Attension..?

  YOLO(You Only Look Once) ; https://arxiv.org/abs/1506.02640
  
  OpenVINO notebook
    iGPU
  Openmodel zoo

  Smart Factory
    Arduino/ Mult-threading_OTX/Smart Factory
    Simulation/Handson
  Team Project
    CVAT&OTX
    MSA 설계
    Computer visionAI 기술 동향
    BSPㄵ 두햐ㅜㄷㄷㄱ wjsakd
    LLM design guide
    개발 procses
    취업 멘토링
    발표 잘하는 방법
    최종 점검
    careeer 면담.
Ubuntu GNONE terminal (Default)
Ubuntu File explorer Nautilus (default)
  Ubuntu Shortcuts
    폴더를 북마크로 등록. Ctrl + D  (파일 익스폴러러 안에서만 가능)
Ubuntu Shortcuts (등록 필요)
  Keyboard Shortcuts - View and Customize Shortcuts
  Keyboard Shortcuts - Launchers
    Home folders - Super+E
Ubuntu Settings - Printer install. type;     .ppd; PostScript Printer Description
  프린터 기능 설명, 프린터 옵션 제공, 프린터와의 호환성

spike neural network?
뉴로모픽 
VCS
  %shell>
    cat ~/.gitconfig
  Software Development Lifecycle
    Requirement;  Speicifcation
    Design; Architecture
    Development
    Test; Quality
    Deployent; Release
    Maintenance;   Hotfix
  Centralized VCS vs. Distributed
  What is Git 📰.. 모두 설명..
    git add... stage, un-stage, working-dir
    git status     vs git log
      git log --oneline
    git reset hard, soft, mixd and revert
      💡 관점이: HEAD Poitner, commit history, staging area, working directrory .. 4개로 나뉨.
    git log --graph --decorate --abbrev-commit --all --pretty=oneline

  📰 Non-bare repository vs. Bare repository
    client-side vs server side?.. ㅇㅇ.. bare repo 는 클론도 가능.
  💡 GitHub CLI ; https://cli.github.com/
    Install ; https://github.com/cli/cli/blob/trunk/docs/install_linux.md
    gh auth login
    https://cli.github.com/manual/gh_repo_create
    gh repo create <repository-name> --public --source=. --remote=origin
    gh repo view <repository-name>
      gh repo view intel-05
    github milestone.. issue.. // Milestone은 특정 기한 또는 목표를 설정해 여러 Issue를 묶는 기능입니다.
      // 언제 사용하는지?
    git pull --rebase     vs .. git upll == get fetch + git merge
      A --- B --- C   (main)          # 로컬 브랜치 상태
             \
              D --- E   (origin/main)  # 원격 브랜치 상태

      A --- B --- D --- E --- C'   (main, origin/main)

  https://www.gnu.org/software/bash/manual/bash.html#index-HISTSIZE

  >>> https://tally.so/r/w7D05A
    git stash       # == git stash push
    git stash push -m ""    # stash 이름 지정

    git stash list
    git stash apply
    git stash pop
    git stash drop

    가장 최근 stash는 stash@{0}
    git stash clear


vim
  i       삽입 모드
  :       명령어 모드
    :wq   저장하고 종료
    :q!   저장하지 않고 강제 종료
  ESC     모드 종료
gnone terminal
redirction ; https://www.gnu.org/software/bash/manual/html_node/Redirections.html
  >   write       // Redirecting Output
  >>  append      // Appending Redirected Output
마일스톤
PR:
  Development
  Successfully merging this pull request may close these issues.

🪱 OAuth 동작 방식:
  사용자 권한 요청: rclone 같은 애플리케이션이 사용자의 Google Drive 파일에 접근하려고 할 때, Google은 사용자에게 해당 애플리케이션에 권한을 부여할지 묻습니다.
  사용자 동의: 사용자가 권한을 부여하면, Google은 해당 애플리케이션에 Access Token을 발급합니다. 이 토큰은 애플리케이션이 사용자의 데이터를 안전하게 접근할 수 있도록 해줍니다.
  Access Token 사용: 발급받은 토큰을 통해 애플리케이션은 Google API에 접근해 데이터를 요청하고 필요한 작업을 수행할 수 있습니다.
  리프레시 토큰: Access Token은 만료되기 때문에, 애플리케이션은 리프레시 토큰을 사용하여 새로운 Access Token을 얻어 지속적인 접근을 할 수 있습니다.

  * Oauth Client
    Role of the OAuth Client: Why is it called "Client" and not "Server"?
    OAuth Client typically refers to an application that requests data from a resource server (e.g., Google Drive). The term "client" is used because the application plays the role of the client in the OAuth process.

    Concept Explanation:
      OAuth Client: The entity requesting data (the application, e.g., rclone).
      OAuth Server (Authorization Server): The server responsible for authentication and authorization (e.g., Google OAuth server).
      Resource Server: The server that provides the actual data the client is trying to access (e.g., Google Drive).

    The OAuth Client accesses the resource server on behalf of the user and obtains an Access Token from the OAuth Server, which is then passed to the resource server. Therefore, the OAuth Client is the one requesting authentication and authorization, while the OAuth Server grants the authorization.
      The client is the user's application (e.g., rclone, Gimp) that requests permission via OAuth to access data on behalf of the user.
      The server is a service like Google or GitHub that grants this permission.

    Thus, the OAuth Client requests authorization from the server and retrieves data, making it appropriate to call it a "client."

📖 Book: 파이썬으로 배우는 머신러닝의 교과서 // 이토 마코토 지음. 박광수(아크몬드) 옮김. 한빛미디어, SE SHOEISHA
  
  http://www.hanbit.co.kr/src/10124


Bash Shell에서 Gimp를 실행하고, Shell을 종료해도 Gimp가 종료되지 않도록 하는 방법
  gimp  &
    dis own # 백그라운드 프로세스를 현재 쉘 세션에서 분리하여, Shell을 종료해도 Gimp가 종료되지 않도록 만듦

  nohup  gimp &    # 프로그램을 백그라운드로 실행하면서 그 프로세스를 no hang-up (쉘 종료) 로부터 보호.
  setsi d gimp &   # setsid 명령은 프로그램을 새로운 세션에서 실행하기 때문에, 부모 Shell과 관계없이 독립적으로 실행됩니다.

Math
  🪱 Coprime integers; https://en.wikipedia.org/wiki/Coprime_integers
    In number theory, two integers a and b are coprime, relatively prime or mutually prime
  초월함수 ; https://en.wikipedia.org/wiki/Transcendental_function
    **다항식(Polynomial)**은 하나 이상의 변수와 그 변수에 대한 **계수(coefficient)**로 구성된 식으로, 변수의 지수가 음수나 분수가 아닌 자연수로 제한됩니다..
  
  다항식이 아닌 함수들
    지수함수 exex: 변수가 지수에 있는 함수는 다항식이 아닙니다.
      지수함수는 밑이 일정한 상수이고, **지수(제곱하는 값)**가 변수로 주어지는 함수입니다
    로그함수 ln⁡(x)ln(x): 변수가 로그 안에 있으면 다항식이 아닙니다.
    삼각함수 sin⁡(x),cos⁡(x)sin(x),cos(x): 주기적인 함수는 다항식이 아닙니다.
    분수 함수 1xx1​: 지수가 음수인 경우 다항식이 아닙니다.
  오일러 상수 e
  🪱 Euler's totient function
    ϕ 기호: ϕ(파이 또는 피)는 그리스 문
    오일러의 피 함수(Euler's Totient Function, ϕ(n))에 대한 설명입니다. 이 함수는 특정 정수 n보다 작거나 같은 양의 정수 중에서 n과 서로 소(즉, 최대공약수가 1인)인 수의 개수를 나타냅니다.
    만약 n=9n=9인 경우, 9와 서로 소인 수는 1, 2, 4, 5, 7, 8이므로 ϕ(9)=6ϕ(9)=6입니다.
      >>> 암호학에서 중요한 역할을 하며, RSA 알고리즘과 같은 암호 시스템에서 사용됩니다.
  totient ; https://en.wiktionary.org/wiki/totient
    (mathematics) The number of positive integers not greater than a specified integer that are relatively prime to it.
  🪱 relatively prime ; https://en.wiktionary.org/wiki/relatively_prime#English
    (mathematics, of a number) having no factors (except the number 1) in common with a specified other number or numbers.
      e.g. 24 is relatively prime to 35.


Gaussian function ; https://en.wikipedia.org/wiki/Gaussian_function
스칼라, 벡터
  https://en.wikipedia.org/wiki/Dot_product
    In mathematics, the dot product or scalar product[note 1] is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors), and returns a single number. 


  scalar: 양
    주로 기울이지 않은 일반 텍스트나 이탤릭체로 표기됨.
  vector: 크기와 방향
    주로 굵은 텍스트나 화살표 표시로 나타냄
    단위 벡터? 벡터를 그 크기로 나눈 값.
    
    inner product (내적)
      내적의 결과값은 스칼라.
    outer product
      외적의 결과값은 벡터
    
    투영?
    벡터의 연산
  
  Derivatives
  편미분? 차원을 늘린다? multivariable of partial derivatives... partial? round?

  dell =.. round x Gradient : Derviative of vector.. 
  gradient f.. n 차원..
  투명점???


  https://marketplace.visualstudio.com/items?itemName=shd101wyy.markdown-preview-enhanced
  
  퀴버?.. 이 graidant 의 방향?
  https://shd101wyy.github.io/markdown-preview-enhanced/#/



4. 행렬곱 (Matrix Multiplication)
  행렬곱은 여러 변수를 동시에 처리하기 위한 연산입니다. 그림에서, 작은 2x2 행렬과 벡터의 곱셈을 보여줍니다. 이 연산은 공간 변환이나 시스템의 복잡한 계산을 처리할 때 사용됩니다.
  예시:
  컴퓨터 그래픽스에서 3D 모델을 회전하거나 크기를 조정할 때 행렬곱을 사용합니다. 예를 들어, 3D 모델의 좌표를 저장한 행렬과 회전 변환을 나타내는 행렬을 곱하면, 회전된 객체의 새로운 위치를 얻을 수 있습니다.

Sigmoid function = ?? 
  1/(1 + e^(-z))



https://en.wikipedia.org/wiki/Gaussian_function

"양자 컴퓨팅에 대해 서론, 정의, 수학적 의미, 응용, 해석, 결론의 형식으로 구성해줘."


Testing.. Pylance Vscode
  "markdown-preview-enhanced.previewTheme": "github-dark.css",
  "python.analysis.inlayHints.callArgumentNames": "all",

❓>>>>>>>>>..........? 비전과제한거 resource/ 폴더 사라짐;;;...




https://tramamte.github.io/2018/07/ /rsa/
중국인의 나머지 정리

👍 https://marketplace.visualstudio.com/items?itemName=James-Yu.latex-workshop
  1. install Late xWorkshop
  2. The only requirement is a compatible LaTeX distribution in the system PATH. For example, TeX Live. We strongly recommend TeX Live. Other possible choices are:
    https://www.tug.org/texlive/
    sudo apt install -y texlive-full fonts-noto-color-emoji

  3. restart VSCode
    Notice that you have to restart VS Code and the operating system after changing the variable.
https://www.overleaf.com/learn/latex/Questions/Inserting_emojis_in_LaTeX_documents_on_Overleaf
  Why It’s Hard to Render Emojis Without \emoji{}:
    LaTeX's Text Rendering: Unlike modern word processors, LaTeX is very strict about font handling. Each character must be assigned a specific font that contains the glyph (in this case, the emoji). Without specific commands, LaTeX won’t switch between fonts automatically.
    Font Overriding: Even though fonts like Noto Sans CJK KR support a wide range of characters, they don't include emoji glyphs, which is why LaTeX needs explicit instructions to switch to Noto Color Emoji or a similar font when rendering emoji characters.



Cannot view file PDF file. File not found: file:///home/wbfw109v2/repo/intel-edge-academy-6/note/sorted_note/test.pdf
🚨 error: Recipe terminated with fatal error: spawn latexmk ENOENT.
  latexmk 도 설치 ㅣㄹ요한듯



🚨 error
  VSCode + LaTeX Workshop의 기본 LaTeX 엔진은 **pdflatex**입니다. 이 엔진은 대부분의 경우에서 작동하지만, Unicode 지원이 필요하거나 fontspec 같은 패키지를 사용할 때는 XeLaTeX 또는 LuaLaTeX로 변경해야 합니다.

  XeLaTeX 또는 LuaLaTeX
  
  Compiling
    Building the document
      LaTeX recipes
        https://github.com/James-Yu/LaTeX-Workshop/wiki/Compile#latex-recipes
  한글, 영어 지원하는 무료 폰트.. 구글 
    2. NotoNoto Sans CJK KR
    // how to usage
    \usepackage{fontspec} % XeLaTeX 또는 LuaLaTeX에서 폰트 사용을 위해 필요
    % Noto Sans CJK KR 폰트 설정
    \setmainfont{Noto Sans CJK KR}

    \usepackage{fontspec} % 폰트 설정을 위해 필요
    \setmainfont{Noto Sans CJK KR} % Noto Sans CJK KR 폰트 사용
    \usepackage{xcolor}   % 색상 설정을 위해 필요
    % 배경색과 글자색 설정
    \pagecolor{black}  % 페이지 배경을 검정색으로 설정
    \color{white}      % 글자색을 흰색으로 설정

아이다스 시스템.. 레이더 시스템..


퍼플렉서 티?

Gemini vs Omni..

AL/DL Momentum : chatGPT app for iOS
Zeta ; https://zeta-ai.io/ko    AI chatting bot service
  HER AI
AI 유명한 논문들이 컨퍼런스 statista. aiindex.org AI; (Source: Scopus.com, Crunchbase, VentureSource, Sand Hill Econometrics, Monster.com)

ImagNet large scale visual recognition
  2010 - NEC-UIUC Lin et al.
  2011 - XRCE Florent Perronnin, Jorge Sanchez
  2012 - AlexNet
  2013 - ZFNet
  2014 - GoogLeNet
  VGGNet (Second Winner)
  2015 - ResNet     // Alphago??
  2016 - GoogLeNet-v4
  2017 - SENet
AI is changing every market
  EMERGENCY RESPONSE
    Real-time emergency and crime response
  ENERGY
    Maximize production and uptime
  EDUCATION
    Transform the learning experience
  SMART CITIES
    Enhance safety, research, and more
  FINANCE
    Turn data into valuable intelligence
  HEALTH
    Revolutionize patient outcomes
  INDUSTRIAL
    Empower truly intelligent Industry 4.0
  MEDIA
    Create thrilling experiences
  RETAIL
    Transform stores and inventory
  SMART HOMES
    Enable homes that see, hear, and respond
  TELECOM
    Drive network and operational efficiency
  TRANSPORT
    Efficient and robust traffic systems

.. 프롬프트? 페르소나를 부여? AI??
릴리스에이아이(Lilys AI) ; 유튜브 요약 AI
??? 뉴로모픽 칩, 로이히, 라바 ..
LG 에너지 솔루션 - 불량품 검사에서도 AI 가 사용한다고 한다.

Sidewinder... 미사일.. 자동추적? , Ultrasound.. 자율주행 센서.


Deep Learning Workflow (생각하게 만들기)
  1. Problem Definition
    - DL Type
      ✔ Classification
      ✔ Detection
      ✔ Segmentation
      ✔ ...

  2. Data Preparation
    - Datasets
      ✔ Obtain data (ex, images, video, audio)
      ✔ Annotation

  3. Training
    - In-house model
    - Open model
    - Model training

  4. Optimization (Training)
    - Training optimization
      ✔ Accuracy

  5. Deployment (Inference)
    - Inference optimization
      ✔ Deploy model on edge

  단계 구분:
  - Use Case
  - Date-prep
  - Training
  - Optimization
  - Deployment

Neural style transfer ??? Colorization



Training 에서 Foreward, Backward
YOLO: Real-Time Object Detection
  피쳐맵?..을 만든다?

언제 optimization 을 해야하는가?
  - 도메인이 바뀌는 경우
  - FP16 vs FP32
Transfer learning
  Low-level Feature --> Mid-level featuer --> High-level Feature..
    



genetic algoirthm..


1950: Alan Turning - Turing test
1958: Frank Rosenblatt - perception
1969: Marvin Minsky - a book of 'Perceptrons'
  MLP 를 할 수 있는건 알았지만 너무 계산량이 많아서.. Perceptron 모델에 대한 한계
    ANd, NANd, OR, XOr.. perceptron이 라인하나로 
1986: david rumelhart -> Multi Layer Perceptron (MLP)
  계산량을 Back propagation 으로 줄임.. 근데 아무리 쌓아도 성능의 한계점이 보임..
1998: Yann Lecun - LeNet-5
  Hidden layer 가 깊어지면 back propagation 이 제대로 안됨.
  다 때려넣지 말고, 이미지의 특징을 추출해서 넣어보자. ㅇㅇ CNN 을 제안함
2012: Alex Krizhevsky - AlexNet
  GPU 도 처음 씀..

1986 (RNN.. recurrent 한 구조가 포함됨..)
  -> LSTM -> Seq2Seq -> ATtention -> Transformer -> GPT-1 -> BERT -> GPT-3
  transformer .. 원래는 병렬 처리가 안됬다. 위치에 따른 처리..


Neuron in our rains..
  전달되는 메커니즘이 뉴런..
  Unistab function, Sigmoid, Relu,
  Spiking neurl network 에서는 이를 그대로 묘사한다? integrate fire?
...
분포재조정.
normalize.. 에서 입실론 값?..
감마, 베타.. scale, and shift..

class activation map CAM. Segmentation, object detection.. 다음주..


activation functino 특징은: non-linear?해야한다
input -> Hypothesis
  relu 도 "1" 은 의미가 있다.
  측면 억제 효과?
  입출력단을

최대 gradiant 가 0.25라서 sigmoid 는 chaine rule 을 적용하기 어렵다. ; gradiant vanishing
Hyperbolic Tangent (Tanh) 도 마찬가지.    // RNN 계열은 이를 쓰고, ?? 다른 것은 RELU 를 많이 쓴다?고한다?
  --> Batch normalization ?? ..
    >>>?? nobatch+ 
      nobatch+sig --> 
      batch 를 쓰면 파라미터 수가 늘어서 안 쓰는 것이 낫나?
    backpropagation 에서 weighting 을 할 때 문제가생기기 대문에 해 줌.
  ReLu 는 graidant vanishing 문제가 거의 나오지 않지만 0 이하에서 너무 반응이 없어서 leakyReLU 를 만들엇다.
  레이어가 깊을 수록 업데이트가 사라져감.
graidant vanishing... ??????
??? 미분의 벡터형이 gradiant? 
Epoch& Iteration.. If batch size big (BGD?)
If Batch size is small(ㄶㅇ?)
batch 사이즈를 크게 하는것이 좋지만, 메모리 한계상...

알고리즘의 방향이 graidant 가 줄어드는 방향으로 학습을 택했다. 라서 gradiant decesnt 라고 이름으 정함.

    
underfitting, overfitting.. noise..
  eary_stopping
  🎖️ Dataset
    Train, Validation, Test
    4차원 데이터.. Batch * Height * Wdight * Channel
    ?? CNN은 4차원이고 ANN 은 3차원??..
    그림 해석.. "Epoch & Iteration & Batch"
dropout..


전장 시스템 ..?? 자동체. 현대자동차.
  CAN signal
  크러스터 시스템?
    My-come..? 마이컴? 
    여러가지 하드웨어적인 컨트롤도 하고, 여러가지 can signal 도 정의하고..
  자동차에서 제일 중요한 이슈.. battery drain..
    전장 시스템에 있는 mycom 도 다 꺼지는 것이 문제..

⌨️ Navigate Brackets
  Ctrl + Shift + \ 

  remove Brackets 
  Ctrl + Alt + BackSpace

전자쪽은 (C언어) 통일되어있지만
PLC 는 다 다르다고 함.. (base 가 기계쪽이라 그럼..)
  Hypervisor ..

중간 파일 만들고 pdf 파일만 다른 곳에 만들게 하는 방법
우뤼도 adam.. 정도 


⚓ Society of Automotive Engineers (SAE) International ; https://en.wikipedia.org/wiki/SAE_International
⚓ Self-driving car ; https://en.wikipedia.org/wiki/Self-driving_car
  #️⃣ Definitions ; https://en.wikipedia.org/wiki/Self-driving_car#Definitions
    #️⃣ SAE classification ; https://en.wikipedia.org/wiki/Self-driving_car#SAE_classification
Latex
  ⚓ Wikibooks - Latex ; https://en.wikibooks.org/wiki/LaTeX/
    ⚓💡 Mathematics ; https://en.wikibooks.org/wiki/LaTeX/Mathematics
      mathbf
  ⚓ Official Latex ; https://www.latex-project.org/help/documentation/
    # Typesetting complex mathematics
      # Specifically targeting the typesetting of mathematics is:
        ⚓ User’s Guide for the amsmath Package (Version 2.1) ; https://www.latex-project.org/help/documentation/amsldoc.pdf
  
  ⚓ Emoji Docs ; https://ctan.math.washington.edu/tex-archive/macros/luatex/latex/emoji/emoji-doc.pdf

  VSCOde extension
  @ suggestions
    Inserting Greek letters



Sigmoid
  https://en.wikipedia.org/wiki/Sigmoid_function












Here's the revised LaTeX code with improved structure, avoiding redundancy and keeping a smooth flow between the points:

⚓ Parallelogram ; https://en.wikipedia.org/wiki/Parallelogram
  Area formula
📰 단일 진실 공급원으로 만들고싶은데 .. 명ㅇ..


⚓ find ; https://www.gnu.org/software/findutils/manual/html_mono/find.html
  2.1 find Expressions ; https://www.gnu.org/software/findutils/manual/html_mono/find.html#find-Expressions
    The expression that find uses to select files consists of one or more 🪱 primaries, each of which is a separate command line argument to find.
    An expression can contain any of the following types of primaries: 
      ... 🪱 actions
        have side effects and return a true or false value; and  ...
  3.3.2 Multiple Files
    Action: -execdir command {} +; https://www.gnu.org/software/findutils/manual/html_mono/find.html#index-_002dexec-1
      This insecure variant of the ‘-execdir’ action is specified by POSIX.
      The main difference is that the command is executed in the directory from which find was invoked, meaning that ‘{}’ is expanded to a relative path starting with the name of one of the starting directories
      , rather than just the basename of the matched file. The result is always true. 
⚓ Function ; https://en.wikipedia.org/wiki/Function_(mathematics)
  unspecified function ...

Latex Engine
  https://en.wikipedia.org/wiki/LuaTeX
    https://www.luatex.org/
Latex auomation build tool; latexmk ; https://github.com/debian-tex/latexmk
  LATEX MaKe


📰📰📰📰 TODO: Documentation
  추가내용을 통합하기: 오류가 발생하지 않도록 기존 구조를 유지하고, 새로운 섹션만 추가
  다음 latex 텍스트에 추가내용을 통합해줘.  절대 오류나지 않게 포맷은 최대한 건드리지말고
  괄호수를 잘 세고 잘 닫을 것.
각각..의 matrix 안의 세부..단어, veector- 가 컴퓨터비전/머신러닝에서 어떻게 쓰이는지 실제 조사하기.
📰📰📰📰 TODO
  about_matrix.tex
    기타:
      고유값 (Eigenvalue), 고유벡터 (Eigenvector): w, v = np.linalg.eig(x)
			고유공간,.. 역행렬과의 관계..행렬의 대각화와의 관계.. (기존에 썻던것과 합치기?분리하기?)
				물체가 가지는 고유한 성질을 찾기 위해 사용함. ㅇㅇ 공명
      np.linalg....
      https://rfriend.tistory.com/163

  Perceptron & ANN
    AL/DL areas and applications: past, current, upcoming
    Multi-layer perceptron --> 쌓아서 만든 것이 ANN?
    뉴런을 수학적으로 모델링한 것..
    알파고 프로그램.. 구글의 딥마인드?.. 2001년부터 한 프로그램
  GRadiant
    Gradient(∇) : Derivatives of vector..
  뇌 속 신경망.. 미세하게 떨어져있다.. 
    inputs.. x_n --> weights.. wij -> transfer function (Sigma)... -> net input net_j ---> activation function(threshold seta_j) -----> activation (o_j(?))
    Activation Function..
      "비선현성을 부여한다는 것" 이 무슨 뜻
  back propagation.. 에서 Chain rule 쓴다는데.. 뭔소리..
    weighting? 이 곱해진다?...
    input -> .. caclulate the sigma?W from sigmaF ???
  딥러닝.. matrix 에서 왜 차원을 높여야 하는가?.. ..
    ?? 숫자 OCR 학습에 대한 예시로...
    ?? 차원을 높일수록 좋다? cube-root?
    Local Minima.. ?
      - loss contour of a VGG-56 DNN's loss function
    cost function.. 우리가 만들려고 한 모델과 데이터 사이의 오차를 나타낸 것?
    saddle points?.. maxima in antoher direcion, a minia in .. direction
    직교성?
    지수 기저함수?
    다차원 벡터의 내적
      벡터 간의 연산이므로 차원이 같아야 함.
    matrix.. 로 확장하면 여러 벡터의 내적을 한번에 진행 가능?
      ?? N 개 batch
  서로소; relatively prime ..
    커널행렬을 인공신경망 형태로 표현하면, 피쳐맵을 각 가중치가 곱해진 채널의 합으로 표현할 수 있음

  신경망: Tensor? 전방패스? 내적?
  파라미터 라는 정의가..?
    이미지로 치면 각각 i, j 의 인덱스의 합??인가? 즉 개수?
  Loss Function
    종류
      평균 제곱 오차(Mean Squared Error): 회귀 문제에서 자주 사용되며, 예측값과 실제값의 차이의 제곱을 평균한 값
      교차 엔트로피(Cross-Entropy): 분류 문제에서 자주 사용되며, 예측한 확률 분포와 실제 분포의 차이를 계산
    최적화(Optimization)
      최적화는 손실 함수의 값을 최소화하기 위해 모델의 **가중치(Weight)**를 조정하는 과정입니다. 이 과정을 통해 모델이 주어진 데이터를 잘 학습하고, 예측 성능을 개선할 수 있습니다.
      "가중치".. 가 정학히 뭐지? COEFFICIENTs ? 벡터?
      예: 경사 하강법(Gradient Descent): 가중치를 업데이트하기 위해 손실 함수의 기울기를 계산하여 손실을 최소화하는 방법입니다.
  Liniear regression
    DOI (Data of interest)  ... e.g. squared error
    Value of interest (VOI) ... e.g. least ?

    find DOI data with VOI

    ?? What is cost function?
  Gradient Descent..
    cost function is convex function
    Goal
    A_k = Learning Rate / Step Size
    Delta?입실론? x(k): Search direction
    Adaptive Learning rate scheme
    입실론..

    Momentum descent algoirhtm
    Adagrad: 2011
      Division and square root applied element-wise .. means
    RMSprop: 2012
      ...deacy.. 아주 오래전 궤적의 영향을 줄이고, 가까운 시간의 궤적의 영향을 높이기 위해.. deacy 추가
    Adam 2014
      Hyper-parameter 설정에서 가장 roust 하다고 알려짐.

  Min Sqaure error 를 Mean aboulste error 를 쓰는 이유..

Validation set

RNN 이나 시계열 쪽은 batch 전체를 한 번에 집어넣을 수 없다? 대형 언어 모델? serial
  배치 정규화가 안된다는건가? layer normalization

  position encoading. 위치가 어디있는지 표시하는
  sigmoid 는 최대 gradiant 가 0.25라서 배치정규화를 안했을 때 sigmoid 를 쓰기 vs ReLu
  
  A2 A1 Ax .. activation functino 이 없으면 layer 를 아무리 써도 하나를 쓴거로 치환할 수 있는 상황으로 나온다..
  선형적인 것을 비선형적으로 바꿔주는..
  CNN Sequential model (keras) 왜 conv2d 이후 batchnomralization 을 계속 같이 해주는 이유. 
    이거 할 때마다 output shape와 param size 는 어떤 관계가 있는것?
  미니 배치? 배치, 배치 정규화, iteratio, epoch, learning rate, step size 의 차이는 또 뭐야
    batch normzliation 에서 변수는 4개: 평균, 표준편차, 감마, 베타?
      mini-batch vs batch
    이미지와 딥러닝 관점에서 Serialize?
      공간적인 특징을 추출해서 넣자. 해서 ANN -> CNN 이 추가됨? Missing Spatial Relationship
      + ANN 은 이미지를 한 번에 다 넣어서(?맞나?) 메모리가 너무 많은 양을 사용해야한다.
      "Feature Engineering" 이 차이가있음 ANN
        ANN 은 plattern 하고 잘라서 벡터로 이어붙인다??? 그래서 특징이 쪼개진다.
        CNN 은 컨볼루션..필터를 사용해서 공간적으로 주변 픽셀과 연관도가 있도록 특징 학습이 된다.
  CNN: Spatially-local correlation . 인접 픽셀 간 높은 상관관계
    Invariant feature? ..사진 좌우 반전시에도 특징 똑같음.. 예: 귀?
    >> 합성곱 연산 제안됨. Convolution
      ..? 메모리를 더 적게 사용한다?
    multi-channel.. ??
  Convolution e.g.: original, sharpen, emboss, horizontal/vertical line,
  weights, bias?

  loss = binary_crossentrpoy vs .. sparse?..

  회귀/펄셉트론 모델에서 W 는 벡터라는데, 벡
  Adaptive ...? - Moementum (velocity ?), Adagrid, RMSProp, Adam, 
    각각 이전 모델에 대해 무어엇을 개선했는지
  ㅁㅇ므; 하이퍼 파라미터 설정에 가장 robust 하다고 알려짐.
  
  https://en.wikipedia.org/wiki/Hadamard_product_(matrices)
    - Hadamard product (also known as the element-wise product, entrywise product[1]: ch. 5  or Schur product
     원소별 곱(element-wise product) or 성분별 곱
  
  Activation functinon
    Softmax 의 역할: normalize 를 해서 ... ~?
    input 값을 exponential 함수로 매핑해놓아서, 큰 값은 더 크게 하고,작은 값은 더 크게하는효과?.. 증폭효과.

  Error back propagation

  CNN.. Fully connected ?
  컬러 이미지는 3차원의 tensor 로 표현됨.. tensor?
  weights, shared weight?
  Feature Engineering
    Feature map.... Feature map 을 RGB 를 따로 구성하나? CNN 을 여러번 할텐데.. 그러면 이미지 크기가 계속 줄지 않아? 풀링?
    filter 는 학습으로 특징이 생기지만, filter layer 의 수는 사용자가 결정
  CNN
    stride.. 이슈: 한 번에 한 칸씩 이동하면 너무 오래걸리지 않을까?
    해결책: filter 가 한번에 여러 칸 이동하도록 허용하자. >> Stride // 영역을 대표하는 값을 가져오자.
    근데 데이터 손실이 일어나지 않나? 비는 영역만 없게 하고, 겹치는 테두리?패드 부분만 갖게 하면 괜찮은건가
    >> Convolution + RELU // POOLINg 을 반복 ... --> Flatten -> fully connected -> ..soft max?
      // 특징 추출                                //               ????
    - flatten: matrix/tensor 구조를 1차원의 vector 로 변환하는 과정.

  Stride 를 크게 만드는 것이 풀링?
  padding. stride=1 이 기본값? ㅇㅇ.. 
  !! Outut size... <<< ..
    Mxa pooling vs average pooling.. 차이.. 
    문제점: 고차원의 tensor 를 보다 compact 하게 축약해야 하지 않을가?
    Pooling: 일정 영역의 정보를 축약하는 역할.
    ?? 알렉스?인가에서 맥스 폴링 했더니 더 성능이 잘 나와서 average 보다 max pooling 을 쓴다고 함
  ?? pooling 을 할수록 색이 연해진다? 실제 고화질 꽃에대한 폴링에 따른 결과물 차이..?
  >>>>>>>>>>> Feature map 이 학습되는 것이 아니라 커널이 학습되는 것이다!!!
  CNN 의 하이퍼 파라미터: Convolution filter 의 크기, 개수
  Dense? 가 Full connected layer 에 해당되는 부분?
  Dense 에 활성함수 sigmoid 이후 softmax 한 이유? conv layer 에서만 relu 씀ㄴ
  👍 https://netron.app/
VGGNet
  .. 3*3 으로 하더라도
    - 필터 크기 작게하면 메모리 크기를 줄일 수 있고,
    - layer 깊이만 틀리면 많은 특징을 추출할 수 있어서.? 3*3 을 사용한다는듯?
Transfer learning? 이 가능한 이유?. low-level feature, mid-level feature, high-level feature
  ..
  // input size 맞춰줘서 학습 시켜야 한다? 가장 중요...resize 등.
  ?? Batch & layer normalization?
    batch 의 수에 따라 성능에 영향을 받지 않도록 하는 것? layer normalization ??
  ?? 원리.. source task labels
  --> taget task labels.. 가 다를 수잇는데 내부적으로 구현이 어떻게 되어있나?
  Training images..Sliding Patches.. C1 ~ C5 ..
Machine learning vs deep learning..
  input -> feature extraction -> classification -> output (mask)
  input -> feature extraction + clasification -> output (mask)
>>>>>>>>.설명 가능한 AI.. 작업? 수치를 의미있게 뽑아낼 수 있다?
mono depth vs super resolution
  ?? 신기.. 멀리 있는거랑 가까이 있는거 해상도? 색이 다르게 나온다.
CNN use case.. cont.
  아마존 재고 파악..
    https://www.slideshare.net/awskorea/amazon-deeplearningcasesandmlonaws
ResNet.. GoLeNet-v4, SENet .. 에서는 layer 가 깊어져서 gradiant vanishing 문제를 어떻게 해결했나?
   5%.. IMagenet.. 오류율 미만..
AlexNet: Relu + local response normalization
  overlapping pooling/drouput/data augmentation
    stride 를 2*2로 햇다고 함.
  multiple GPU  .. 어떻게?
  normalizatino 을 처음 사용함.

Convolution architecture..
GoogLeNet (Inception), 2014
  3*3.. ..? Inception? Concatenation?.. 이게ㅁ 무슨..
  보조분류기를 추가한건가? Auxsilary classifier.. 왜? vanishing 현상을 막기 위해? 연산량을 줄이기 위해? 파라미터 개수가 VGGNet 보다 훨씬 적은데 성능이 좋다고 함.ㄴ
  global average pooling
  channel concat
  ? 1*1 convolution 을 하면 피쳐맵 개수가 줄어든다?
Detection / Segmentation Annotation..

The \include command doesn't allow relative paths starting with ../. It is safer to use \input if you're working with subdirectories, as \input is more flexible in handling relative paths.


https://help.ubuntu.com/stable/ubuntu-help/shell-workspaces-switch.html.en
🌟 //////////// 수식 다 한 다음에 컴퓨터 그래픽스에서 어떻게사용되는지 직접 코드 짜보기

https://en.wikipedia.org/wiki/Mathematical_Alphanumeric_Symbols

다음과 같은 분류로 다음 주제에 대해 latex 로 작성해줘.
* 지시사항
  - 매우 구체적이고 정교하게 작성해줘. 간단히가 아니라. (그렇다고 가독성이 안좋아지면 안 됌)
  - (없는 경우) 각각 연산식이 정의된 사항에 대해 실제 값 수식을 넣어서 예시도 들것. 또한 예시를 들기 전에 "🛍️ e.g." 로서 한 줄 추가한다.
    - 단, 실제로 식에 값을 대입하여 예시를 든 것에만 \emoji{shopping} e.g. 라고 표시하기.
  - (없는 경우) 문맥에서의 Lexicon/Glossary/Terminology 들은 영어로도 표시할것
    🛍️ e.g.
      코사인 유사도 (Cosine Similarity)
      선형 독립성 (Linear Independence)
  - 그 외 이미 만들어진 다른 주제의 latex 의 구성을 따를 것.
    예를 들어 emoji 사용 방법.
  - 실제 정의에 대한 대수 수식이 삭제되지 않도록 다시 한 번 쓴 것을 확인해봐, 이후 내용을 많이 보충해줘.
  - 절대적으로 예시 latex 를 우선적으로 참조하여 이와 같은 최상위분류를 갖도록 해.
    // 여러개의 하위 주제를 갖는 경우 (예: 미분 -> 편미분), 이러한 미분, 편미분을 최상위 분류로 두고, 하위 주제들을 다음과 같이 두어 구성할것. ("하위 주제"에 대해서만임. 프로젝션의 개념에서 외적의 개념이 나오면 개별적으로 구성하지 말고 간단히만 "외적"에대해 설명하고 넘어갈 것)
    1. 배경과 역사
      // 주제의 기원과 역사적 배경을 파악하는 질문을 통해 개념이 처음 어떻게 등장했고, 어떤 문제를 해결하려고 했는지를 알 수 있습니다.
      "이 개념(또는 이론)은 언제 처음 도입되었고, 어떤 문제를 해결하기 위해 등장했나요?"
      "이 주제의 역사적 배경은 무엇인가요?"
      // 예: "벡터 내적은 언제 처음 정의되었고, 그 기원은 무엇인가요?"
    2. 정의와 목적
      //주제가 무엇인지 정의하고, 그것이 어떤 목적을 위해 존재하는지를 명확하게 묻습니다. 이를 통해 개념의 기본적인 성질을 이해할 수 있습니다.
      "이 개념은 무엇을 정의하나요?"
      "이 연산 또는 개념이 실질적으로 어떤 문제를 해결하기 위해 존재하나요?"
      // 예: "커널 convolution 연산은 정확히 무엇을 의미하나요, 그리고 왜 그런 방식으로 정의되었나요?"
    3. 연산 | 메커니즘의 이유
      // 특정 방식으로 정의된 이유를 묻습니다. 주제의 수학적, 과학적 또는 기하학적 이유에 대해 탐구하는 질문입니다.
      "왜 이 연산은 이런 식으로 정의되었나요?"
      "이 메커니즘이 이렇게 동작하는 이유는 무엇인가요?"
      // 수학적 정의가 왜 그렇게 되었는지 성분 유도. (이미 알려진 원리를 이용하는 경우, 간단히 해당 공식만 이용했다고 언급만. e.g. 피타고라스 정리를 이용함. 이라고 표시.)" 
      // 예: "왜 벡터 내적은 두 벡터의 크기와 방향의 곱으로 정의되었나요?"
    4. 비교 | 대조 | 성질
      // 등등. 하위 4.1, 4.2 .. 이런식으로 추가 구성
      // 비슷한 개념이나 연산과 비교를 통해 그 차이점과 유사점을 묻습니다. 이를 통해 주제를 더 깊이 이해할 수 있습니다.
      "이 개념은 다른 개념과 어떻게 다르며, 유사한 점은 무엇인가요?"
      "이 연산은 cross-correlation과 어떻게 다른가요?"
      // 예: "벡터 내적과 벡터 외적은 어떻게 다르고, 어떤 상황에서 각각을 사용하는가요?"
    5. 응용 | 실제 사례
      // 이 개념이 실제로 어떻게 적용되는지를 물어보면, 이론적 개념의 실질적 활용 방법을 알 수 있습니다.
      "이 개념은 실제로 어디에서 사용되나요?"
      "이 연산은 실생활에서 어떤 문제를 해결하기 위해 사용되나요?"
      // 예: "커널 convolution은 컴퓨터 비전에서 어떻게 사용되나요?"
      //  , 단위벡터의 물리적 및 수학적 의미
    6. 관련 논문 | 참고 자료
      // 주제에 대해 더 깊은 탐구를 원할 때는 관련된 논문, 서적, 연구 자료 등을 물어보는 것이 유용합니다.
      "이 주제와 관련된 중요한 논문이나 연구는 무엇인가요?"
      "이 개념에 대한 더 깊은 이해를 위해 참고할 만한 문헌은 무엇인가요?"
      // 예: "벡터 내적의 정의와 기원을 다룬 중요한 논문은 무엇인가요?"
    7. 관련 용어
      // 예: 행렬이라면, "판별식" 에 대해, "가우스-조던 소거법" 등에 대해 기록한다. 하위분류가 될 수 있는 역행렬, 단위행렬 등은 개별적으로 문서를 작성할 것이므로 작성하지 않는다.
      // 관련용어에 수학적 정의가 았다면 2차 분류와 마찬가지로 배경과 역사, 정의와 목적, 연산 | 메커니즘의 이유, 응용 | 실제 사례를 함께를 쓴다. (하위 분류로서!!!)
	- 문단 간에 적절한 띄어쓰기를 추가하고, 다음 문단을 새 줄에서 시작해. (latex 에서 렌더링 되는 기준으로)
	- 수학식이 아닌 일반 텍스트를 굵게 하는 것은 ** ** 표시가 아니라 \textbf{}를 사용해.





차원이 높으면 왜 좋은가?
	Local Minima
		loss contour of a VGG-56 DNN’s loss function
			Gradient of local and global minima is zero
			Improper initialization point may cause convergence to a local minima
				– you’re doomed!
Multi dimension
	Saddle Points, Edge Length, Truncation Parameter
		• A minima in one direction, a maxima in another direction
		• Occurs where two maxima meet
	다차원 벡터의 내적
		• 두 벡터 𝑥1, 𝑥2 의 내적은 두 벡터 간의 projection 에너지의 적분으로 정의됨
		• 두 벡터가 신호이고, 같은 신호일 경우 신호의 에너지가 됨
		벡터 간의 연산이므로 차원이 같아야 함
			• 내적의 결과는 스칼라!! ?
	방향성분 추출
		• 𝑥𝑛 에 관한 기저함수 ∅𝑛 의 가중치 혹은 계수는 𝑥𝑛 와 ∅𝑛 의 내적으로 추출됨
		• 𝑥𝑛 와 ∅𝑛 의 내적은 𝑥𝑛 의 ∅𝑛 방향의 정사형 성분으로 ∅𝑛 방향의 가중치로 해석됨
	직교성과 기본 벡터(basis function)
		• 직교성(orthogonal & orthonormal)
			• 신호 𝑥(𝑡)에 관한 식의 기저함수를 𝜙𝒏 라고 할 때, 다른 기저함수 간의 내적이 0일 경우 orthogonal이라고 함
			• 동일 기저함수 간의 내적이 1일 경우, orthonormal이라고 함
	sin/cos함수 기본 벡터
		• 지수 기저함수(파동의 주파수 기본으로 모델링)
	matrix 확장
		• matrix로 확장하면 여러 벡터의 내적을 한꺼번에 진행 가능
		• 커널행렬을 인공신경망 형태로 표현하면, 피쳐맵을 각 가중치가 곱해진 채널의 합으로 표현할 수 있음.

		


법선벡터? 표면법선? (normal vector) 에 대해서도
  ===========
  벡터의 내적..
  3.6 컴퓨터 그래픽스에서의 응용
  컴퓨터 그래픽스에서 내적은 물체의 표면과 빛의 각도를 계산하는 데 사용됩니다. 예를 들어, 물체 표면에 빛이 닿을 때, 빛과 표면 법선 벡터 간의 내적을 계산하여 표면의 밝기나 그림자를 결정할 수 있습니다.

  벡터의 외적..
  3.6 컴퓨터 그래픽스에서의 응용
  컴퓨터 그래픽스에서는 물체의 회전과 변환을 처리하는 데 외적을 사용하며, 특히 물체의 표면에 수직인 법선 벡터 (normal vector)를 계산하는 데 유용합니다.
  
  3.3 🖼️ 컴퓨터 그래픽스에서의 단위벡터 응용 (Applications of Unit Vector in Computer Graphics)
  컴퓨터 그래픽스(Computer graphics)에서는 카메라의 방향(Direction), 물체의 표면 법선(Normal vector) 등을 나타내기 위해 단위벡터가 사용됩니다. 이러한 단위벡터는 물체의 조명(Lighting), 그림자(Shadow) 처리 등에 중요한 역할을 합니다.

  https://numpy.org/doc/stable/reference/generated/numpy.sum.html

https://marketplace.visualstudio.com/items?itemName=DavidAnson.vscode-markdownlint
https://marketplace.visualstudio.com/items?itemName=shd101wyy.markdown-preview-enhanced

>> 다음 예시 구성과 같이 단위벡터에 대해 설명해줘  
    Norm
      L1 Norm
        벡터와 원소의 절대값의 합
      L2 Norm
        유클리디안 거리로 계산된 벡터의 길이
      무한 놈
        벡터의 원소 중 절대값이 가장 큰 값.
      Entropy
      평균과 분산..
      BN.. Batch Normalization ..? // layer 를 추가한 것과 같다?..
        mini-batch mean
        mini-batch variance
        normalize
        scale and shift
    // 각각에 대한 수학 기호도 표시.

  Entropy~
    소스 심볼에 대한 정보량 I 심볼의 발생확률(의외성) P?
      심볼의 평균 정보량
        엔트로피(entropy, 무질서도로 표현)
        Shannon 이 수학적으로 정의

        Entropy: 수식..
        Corss-Entropy: ... 수식..
        KL-divergence: .. 수식..
      Cost function
        MSE (Mean Squared Error)
        Binary cross-entropy
        Optimizer adam?
        Categorical across-entropy
          ..vs spare categorical crossentropy.. 다중 클래스 분류?
      조건부 확률
        베이지안 이론..
          P(A) 사전확률 (prior probability)
          P(B|A) 우도확률(likelihood probability)
          P(A|B) 사후 확률(posterior probability)
  Convolution..
  교환법칙을 성립하게 하기 위해서 convolution 시 상하좌우 반전을 시켜야 한다.
    이미지 복구? 를 위해서 ?도?.. 전치를 한다고 한다.....
    cv2, wiki 에서도 이미지 프로세싱에서는 컨볼루션 시 뒤집어줬는데 왜 keras 는 안뒤집는건지?

    https://angeloyeo.github.io/2019/08/06/determinant.html

  Euler's number; 네이피어 상수, 오일러 수, 자연상수
    시그모이드 함수
  e?.. 시그모이드 함수??..
    convlution 할 때 왜 뒤집나? flip 위상을...

  ❓ ANN 은 Flatten 을 먼저 하나?


편미분 기호.. 읽는 방법, 기호 ∂
음성인식과 생성형 AI를 활용한 의류 가상 피팅 키오스크 -  감다공
  피팅 횟수 제한, 니트, 흰색, 셔츠 
  ..
  카메라에 ㅇㅇ.. 
  패션시장 규모 ㅇㅇ.
  트렌드 리더
  d;stpds
  > 기대효과
  발려동물로 대상 넓히기, 모델이 직접 광고를 이ㅜ해.. 

  얼굴 인식 -> 성별 및 연령인식 -> 의상 추천
  >> Vector DB 에서 의상추출 ??
    음성 -> 텍스트 데이터로 변환
    
  Stable diffusion  >> txt2img

    간트 차트

  3월?
    스켈레톤을 통한 신체 사이즈 측정
  UI 통합..
마이크 성공..
  문제
  >>>>>>>>>>>>>. 외부 환경 
    고급 소음 제거 알고리즘 잡음 제거.
    사용자-카메라 간의 거리 변화에 따른 픽셀 왜곡으로 인해 스켈레톤의 신체 데이터의 정확한 측정이 어려움

    손모양에 따른 정확한 의미 인식이 어려움. Yolo-tiny.. 특정 동작에 따라 모션 인식으로 
      >>>>>>>> 
    사용자와 주변 환경의 구분이 어려움..

  Google trnaslate?? 를 사용? speech regonition ?

  openvino 가 서양..사람으로 인식


  pytorch 모델인 text encoder 를 openvino 의 IR 형식으로 변환하여 사용
  U-Net... 
  파이썬으로 크롤링하여 SQL DB 화

  acgpn ? 딥러닝모델... 여성 옷에 최적화.. 
  Stable diffusion

Opencv
  opencv 를 사용하여 얼굴 감지 후 성별과 연령 인식. ; OPEN-source Computer Vision
  Main modules
    core.
    imgproc.
    imgcodecs.
    videoio.
    highgui.
    video.
    calib3d.
      광각적 왜곡을 줄이는 용도
    features2d
    objdetect
    dnn.
    ml.
    flann.
    photo.
    stiching
    gapi.
  Extra modules
    alphamat.
    aruco.
    bgsegm.
    bioinspired.
    ccalib.
    cudararithm.
    cudabgsegm.
    cudacodec.
    cudafeaturse2d.
  >> Codec 이란?
  빛의 3원색 vs 색의 3원색

  jpeg 는 압축률이 높다. png 는 무손실에 가까운? 압축방식??
  ⚓ Flags for video I/O ; https://docs.opencv.org/3.4/d4/d15/group__videoio__flags__base.html
    idiom... cv::VideoCaptureProperties
      # Set desired frame width and height
      cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
      cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    CAP_DSHOW 
      Python: cv.CAP_DSHOW
      DirectShow (via videoInput) 
      https://en.wikipedia.org/wiki/DirectShow
  SAVINg video ; https://docs.opencv.org/3.4/dd/d43/tutorial_py_video_display.html
  카메라를 통한성별, 연령 인식 -> 음성 인식 및 음성된 음성 확인 및 키워드 추출 - >추출된 키워드 기반의 옷 추천 -> 사진 합성을 위한 사용자 촬영

  어깨보다 손이 밖에 있으면 ? 자동으로 카운트? 되도록 해서 사용자와 의상 이미지 합성하여 출력.배경도 똑같이.
    

지향성 마이크 >>
  앱코 마이크
  로컬에 스테이블 디퓨전 ..을 받아와서 사용하는 혀익

LoRA(Low-Rank Adaptation, 저 랭크 적응?)은 Stable Diffusion 모델을 세부 조정하기 위한 학습 기법입니다. Stable
  컨트롤넷으로 사진의 뼈대를 가져와서, 옷 입혀야 함.. 

  ㅒㅔ두퍄ㅜㅐ .. ㄻㅊㄷ ㅇㄷㅅㄷㅊ샤ㅐㅜ. ㅁㅎㄷ, ㅎ둗ㅅㄷㄱ
  ㅆ리ㅑㅅㄷㄴ디랻-ㄴㄸ흐둣ㅁ샤ㅐㅜ
  https://github.com/CMU-Perceptual-Computing-Lab/openpose


차량        ---
stm32 mcu // RTOS:Free.. ROS      <-->     Embedded Linux jetson nano + 수신호 처리
guvcview


>>>>>>>>>> https://netron.app/ 모델 학습 어떻게 됬는지 확인
⭕ Functional API !1
  https://www.tensorflow.org/guide/keras/functional_api


  class Layer(BackendLayer, Operation, KerasSaveable):
    @traceback_utils.filter_traceback
    def __call__(self, *args, **kwargs):
      ...
      # 실제로 레이어 연산을 수행하기 위해 상위 클래스의 __call__ 호출
      outputs = super().__call__(*args, **kwargs)
      ...
      return outputs

    ?? Keras.. method chaining pattern.. 


  super()는 부모 클래스의 메서드를 호출할 때 사용되지만,
  
  다중 상속 상황에서는 MRO에 따라 메서드 탐색이 . 즉, super().__call__(*args, **kwargs)는 MRO에서 가장 먼저 나오는 부모 클래스의 __call__() 메서드를 호출합니다.
  BackendLayer 에는 없고, Operation 에는 있다.
  
  - call() 은 예약어가 아님.. __call__() 은 예약어이고. call() 은 텐서플로우 내부적으로 사용하는 함수.
    __call__() 은 다음과 같이 인스턴스를 함수처럼 사용할 때 호출됨.
      model = Flatten()(model) # 이전 ANN dataset 학습 시 사용한 Functional API 호출 방식.

  - MultiHeadAttention layer 는 Layer 클래스를 상속받음.
  
  - class Layer 는 def __call__() 에서 super().__call__() 를 사용한다.
    이는 부모 클래스 (Layer 는 다중 상속됨) 중 MRO (Method Resolution Order) 에 따라 __call__() 이 정의되어있는 클래스를 찾아서 해당 함수를 호출합니다. 여기에서는 keras.src.ops.operation.Operation.

    @keras_export(["keras.Layer", "keras.layers.Layer"])
    class Layer(BackendLayer, Operation, KerasSaveable):
        """
        This is the class from which all layers inherit.
        A layer is a callable object that takes as input one or more tensors and that outputs one or more tensors. It involves computation, defined in the call() method, and a state (weight variables).
        ...
        """
        @traceback_utils.filter_traceback
        def __call__(self, *args, **kwargs):
          ...
            ####################
            # 7. Call the layer.
            ...
                    if new_scope is not None:
                        with new_scope:
                            outputs = super().__call__(*args, **kwargs)
                    else:
                        outputs = super().__call__(*args, **kwargs)
  - 결국 이 곳에서 call() 함수를 호출합니다.
    // keras.src.ops.operation.Operation
    @traceback_utils.filter_traceback
    def __call__(self, *args, **kwargs):
        # Plain flow.
        if any_symbolic_tensors(args, kwargs):
            return self.symbolic_call(*args, **kwargs)
        if getattr(self, "quantization_mode", None) is not None:
            return self.quantized_call(*args, **kwargs)
        else:
            return self.call(*args, **kwargs)

    코드에서 traceback_utils.is_traceback_filtering_enabled()를 확인하는 이유는, 예외가 발생할 때 더 유용한 디버깅 정보를 제공하기 위해서입니다. 이 부분은 call 메서드를 감싸서, 예외가 발생하면 추가적인 정보를 제공하도록 설계되었습니다.
    레이어가 심볼릭 텐서를 사용하는 경우에는 self.symbolic_call를 호출.
    만약 양자화(quantization) 모드가 활성화된 경우에는 self.quantized_call를 호출
    그 외에는 기본적으로 self.call 메서드를 호출됩니다.
  📰 TODO
    // 고차함수 구현해보기

@keras_export("keras.Operation")
class Operation:

>>>>>>>>>>>>> 크롤링 한 이미지: svg, png .. waifu ncnn py 로 wsl 에서 확대 시 색상이 이상해지는 문제?

📰 Real-ESRGAN >> 사용해보자. WSL, Ubuntu 에서 모두 작동한다고 한다.  📅2024-09-17 01:46:50
  https://github.com/xinntao/Real-ESRGAN
  Real-ESRGAN은 최근에 발표된 최신 이미지 업스케일링 모델로, 기존의 ESRGAN(Enhanced Super-Resolution GAN)을 개선한 버전입니다. 특히 일반 사진 및 애니메이션 업스케일링에서 뛰어난 성능을 보여줍니다.
    장점
    고해상도 업스케일링에 적합하며, 특히 노이즈가 있는 이미지나 디테일이 많은 이미지에서 우수한 성능을 발휘합니다.
      Waifu2x처럼 애니메이션과 일반 이미지 둘 다에 잘 동작합니다.
      이미지의 전반적인 품질을 크게 향상시키며, 세부 사항이 잘 유지됩니다.

    단점:
      SRGAN보다 구현이 복잡하고, 더 많은 연산 자원이 필요할 수 있습니다.
      특히 저성능 GPU에서는 학습 및 추론 시간이 길 수 있습니다.


"editor.tabSize": 2
  + auto detect..

🪱 req ; https://en.wiktionary.org/wiki/req
  Abbreviation of request.
Runet GoogleNet VGGNet, RNN/LSTM .. Segment detection 이후 
  Attention, Sequneced Sequnece.. Tansformer 기반 GPT?

  GoogleNet; 3가지 특징
    inception, 보조 분류기 (gradient vanisihin 문제 해결), global average poolin (연산량 줄임)
      ?? 왜 어떻게 문제해결한거지 어떻게
ResNet
  가장 깊고 성능이 좋은 네트워크. Revolution of depth.
    better recognition than human. 깊이의 혁명
  input 을 주기적으로 넣어주는 skip connection 구조.
  output size가 줄 때마다, 채널을 2배씩 늘어남.
  1*1.. ? 으로 연산량을 왜..
  bottleneck layer: 1*1. 3*3. 1*1.. 이거 왜 사용/ 이게무슨구조?
    improve efficiency. simliar to googleNet
  Residual running.
  weight layer... 차분 구조..? weighting 에 반영이 된다..
Image Localization: CNN Class Activiation Map;
  왜 그렇게 판단했는지 판단하는ㄱ ㅓㅅ.
  CNN Class Activation Map
  Fully Conneced layer 대신 Global average polling (GAP) 이라는 개념을 도입
  !!! 공간적인 특징을 가지고 있다. 이를 yolo 에서도 사용한다.

  Grad-CAM..
  ReLU
  Grad-CAM 으로 .. Heatp map 구성?
  🚣 >>> "Imagenet 1000 class list" search.
  FCN models..
Segmentation
  픽셀 단위의 예측을 위한 최초의  둥-새-둥  방식.
    skip cmbine downsampling upsampling
    ???
  Fully Convolutionalization
    FC 계층을 모두 Conv 계층으로 대체헤 히트맵을 획득.

  input image title --> output segmentation map
  약간 사이즈 차이가 있을 수 있음. concatenate 해서 붙인다고 함.
💡 Confusion Matrix
  + 정확도와 재현율의 차이?
    예를 들어 암세포... 에 대해서 있는 것들을 다 찾아내는 것이 중요해서.. 
    Precision, Recall
    🪱 PR Cruve... 서로 반비례하는 특성. 왜?
    🪱 iOU
Object Detectoin Yolu
  멀티 레이블 이미지 분류와 바운딩 박스 회귀를 위한 기본적인 영역을 정하기 어려운 문제가 있음.

  models
    2-Stage model
      Region proposal -> classification (Plain cnns) -> multi-class classification, bounding box regression
      e.g. RCNN, fastRCNN, fasterRCNN
    1-Stage model
      Conv layers (Faeature extraction) -> Features Map -> For each grid/spatial location
      e.g. Yolo, SSD
      두 가지 네트워크를 하나로 합해 효율성을 극대화시킬 수 있는 1-stage
      2-stage network 는 정확하고 느린데, 1-stage 는 빠른데 정확도는 다소 떨어짐.
      각 그리드 셀은 하나의 클래스만을 예측하기 때문에 객체가 겹쳐 있으면 제대로 예측하기 어려움.


  🆚 RCNN, fastRCNN, fasterRCNN
    fasterRCNN 은 ROI pulling? pollinG? 이라는 것을 한다고 한다. RPN
      정사각형 영역을 만들기 위해 resize를 하는 것이 아니라 pooling 을 사용한다?
      >> resize 를 함으로서 왜곡문제를 해결한다고 한다?
  🔪 1-Stage object detection
    -------------------- CNN Layers --------------------
    spatial details <---- Our approach ---> Semantics
    ..? Semantics (?? Last layer of CNN e.g., fc7 in VGGNet)
  x, y, w, h,confidnence, probability of class 를 같이 가지고 있음.. grid cell? 이 뭐지?
  bounding box descripter
    ??tx1
  iou .. Intersection over Union.. 

  🚣 Yolo V3 (Prediction Across Scales)
    ?? 13*13, 26*26, 52*52  
  Feature Pyramid.. ?map?
  Dark architecture again?
2023 년 LLM. 덴마크 공대. 미래 생성하는 모델 개발.
multi-modal.. 언어와 영상을 통합하는?..
  언어모델?
  RNN-> LSTM -> Seq2Seq -> Attension -> Transformer -> GPT-1 -> BERT -> GPT-3 (2020) -> GPT-3.5 (2022)
    Long Short-Term Memory
    🪱 Hyperbolic tangentH..
    이전의 셀 상태

  RNN: 언어의 순서 처리를 위해 recursive한 path 생성. gradiant vanishing 문제 생성.
  Attension: 데이터가 순서대로 들어가야해서 batch 를 쓰기가너무 어려움. 동시에 넣을 수 있도록 Transformer 에서 순서를 표시할 수 있도록 개선..
Open source vs Private Models, 5-Shot MMLU Performance
  https://www.aitimes.com/news/articleView.html?idxno=155196
Non-sequential Data, Sequential Data
  순서가 있는 인공신경망 구조? 벡터 표현?

시간 개념을 반영할 수있는 인공 신경망..
  one ton one
  one to many
    image captioning?
  many to one
  many to many
    .. 2 종류..
    🪱 Abnormal Score 
  encoading block? decoading block?
  forward path.. h_t = f
단어 임베딩.:컴퓨터가 단어의 의미를 이해하고 처리할 수 있도록 단어를 벡터로 변환.
  tensor 의 embedding?
  단어의 긍정/부정적인 것을 분류?
  https://word2vec.kr/search/
Word2Vec -> CBOW, Skip gram.
  Continuous Bag of Words.
  Skip gram 모델
FastText // Facebook
ELMo- Embeddings from Language Models
Seq2Seq
  Neural Machine Translation...
  encoder--->
    LSTM (I) -> LSTM (am) -> LSTM (a) -> LSTM (student)..
  --> contenxt
  decoder
    LSTM(...) -> ...
  hideen layer 랑 합쳐져셔?..
Attention
  단어 간의 내적을 계산한다. 상관관계; attension score?
  ??? Attention mechanism
  start of sentence?
  coefficient?
  >>>>> 여기서 key, query, value 가 히든 벡터인데 transformer 에서는 hidden vector 가 아니라고 한다? 
  컨텍스트 벡터
  문제점
    입력 시퀸스를 순차적으로 처리해서 병렬 처리  
positional encoading?
  sign(주파수 + ...)
    주파수가 다른 애들끼리는 내적이 0이 된다?위치마다 서로 다른 frequency 를 ..
Attention Is All You Need
  https://arxiv.org/abs/1706.03762
  .. "Add & Norm" 이 batch 가 아니라 layer normalization 라는데 무슨 뜻이야?
  GPT 에서는 ?? 뭐가 빠진다고 함..

  https://codingopera.tistory.com/43
  cross-attention
  maksed self-attention
  encoder-decoder cross-attention ?? 이게 뭐야...
🪱 turn-by-turn navigation ; https://en.wiktionary.org/wiki/turn-by-turn_navigation#English
🪱 emergent abilities .. in GPT?
  in-context learning ? meta-learning? zero short inference ?
  GPT 3~ Hallucination 제거.
  AI Open Community
RNN .. SimpleRNN 에서(Return sequences=True) ???

🌟 AI 인공지능 경진대회 ; https://dacon.io/
  >>> keras vision 코드 snippet ; https://keras.io/examples/vision/
    https://keras.io/examples/vision/video_classification/
      RNN; Sequentiald data.. 동작 판단.
      CNN-RNN 같이 사용 이유?
    https://keras.io/examples/vision/video_transformers/

    ... https://keras.io/api/layers/attention_layers/multi_head_attention/

>> 오픈비노 -> 라즈베리 파이에 올려서 테스트 가능.



RNN: Abnormal Score: Min square error.... 로 사용?? 왜?
  forward path.

ELMO ? deep contexturalized word Embeddings from Language Models
  BERT 와 차이? BERT 는 encoder만 있어서 순서대로 나오지 않는다? 자연어 처리?
  gpt 5 sora 5

⚓ Convolutional Neural Network ; https://en.wikipedia.org/wiki/Convolutional_neural_network
  # Fully connected layers
    🪱 logit; 모델의 예측 값에 대한 선형 결합 ??
      이 logit은 정답 확률로 변환되어야 하는데, 그때 사용하는 것이 활성 함수입니다.
# Differentiable computing 
  # Concepts
    ⚓ Activation function ; https://en.wikipedia.org/wiki/Activation_function
      # Contents
        The activation function of a node in an artificial neural network is a function that calculates the output of the node based on its individual inputs and their weights.
        #️⃣💡 Mathematical details ; https://en.wikipedia.org/wiki/Activation_function#Mathematical_details
          # Ridge Activation Functions:
            - ⚓ ReLU (Rectified Linear Unit) | rectifier ; https://en.wikipedia.org/wiki/Rectifier_(neural_networks)
              # Variant ; https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Variants
                - ELU (Exponential Linear Unit)
            - Leaky ReLU

          # Radial Activation Functions:
            - Radial Basis Function (RBF)
            - Gaussian Activation Function

          # Folding Activation Functions:
            - ⚓ Sigmoid function ; https://en.wikipedia.org/wiki/Sigmoid_function
              🪱 sigmoid curve
            - Tanh (Hyperbolic Tangent)
            - ⚓ Softmax function | Normalized exponential function | softargmax ; https://en.wikipedia.org/wiki/Softmax_function
              .. 여기서 z_i​는 **출력 노드 i**의 logit, **N**은 클래스의 총 개수입니다.

🌟 Mathematical notation ; https://en.wikipedia.org/wiki/Mathematical_notation
  ⚓ Hat notation ; https://en.wikipedia.org/wiki/Hat_notation
    In statistics, a circumflex (ˆ), called a "hat", is used to denote an estimator or an estimated value.
  << Mathematical operators and symbols in Unicode ; https://en.wikipedia.org/wiki/Mathematical_operators_and_symbols_in_Unicode
  << ⚓ Glossary of mathematical symbols ; https://en.wikipedia.org/wiki/Glossary_of_mathematical_symbols
    # Calculus
      << ⚓ cursive d ; https://en.wikipedia.org/wiki/%E2%88%82
        ... usually to denote a partial derivative such as ∂ z / ∂ x {\displaystyle {\partial z}/{\partial x}} (read as "the partial derivative of z with respect to x").
        어떤 변수를 기준으로 그 함수의 변화를 측정한다고 표현하므로 "with respect to" (; ~에 관하여) 를 사용.
    # ℝ; Real number ; https://en.wikipedia.org/wiki/Real_number
    # ℝⁿ; Real coordinate space ; https://en.wikipedia.org/wiki/Real_coordinate_space
      ℝ¹, ℝ², ℝ³ ...
      Eⁿ (Euclidean -)
      E¹ (Euclidean line)
      E² (Euclidean plane)
      E³ (Euclidean three-dimensional space)
Hidden이라는 용어는 입력층과 출력층 사이에서 발생하는 중간 계산을 설명하기 위해 사용됩니다. RNN에서는 은닉 상태가 이전 타임스텝의 정보를 저장하고, 이를 바탕으로 다음 타임스텝에 전달됩니다. 이때의 중간 정보들이 외부에서는 보이지 않지만 중요한 정보이므로 **"Hidden"**이라고 불립니다.
1.
"""
Rectification (정류) 개념에서 온 것입니다. 원래 rectifier는 교류(AC)를 직류(DC)로 변환하는 장치를 의미합니다. ReLU는 음수 입력을 0으로 변환하는데, 이는 전기 공학에서 교류 신호의 음수 값을 없애는 과정과 유사합니다.
ReLU의 수식:
ReLU(x)=max⁡(0,x)
ReLU(x)=max(0,x)

    입력 값이 0보다 크면 그대로 출력되고, 0보다 작으면 **0으로 "정류"**됩니다.
"""
를 영어로. 


2. 선형성과 비선형성을 영어로. wikipedia 에 문서가 있나?

Gradient Vanishing

Feed Forward 가 뭐야? Dense layer 라고 하는건가?


TODO: crawling
  python docs
  "sidebar" like in https://en.wikipedia.org/wiki/Softmax_function
  "navbox" like in https://en.wikipedia.org/wiki/Softmax_function

⚓ Functional Programming ; https://en.wikipedia.org/wiki/Functional_programming
  ⚓ fold | reduce | accumulate | aggregate | compress | inject ; https://en.wikipedia.org/wiki/Fold_(higher-order_function)
    💡 ... refers to a family of higher-order functions that analyze a recursive data structure and through use of a given combining operation, recombine the results of recursively processing its constituent parts, building up a return value. 
CoCo format. 

👍 Sites
  Youtube 요약 - Lily
  Power Point AI 조수 - Gamma https://gamma.app/
  Ai Video 생성기 - https://www.heygen.com/

Ctrl + Shift + U. >> Unicode input
>>>>>>>>>>> VSCOde extension ; Unicode Latex https://marketplace.visualstudio.com/items?itemName=oijaz.unicode-latex
  X.. latex 문법과 조금씩 다름.
  or Latex - (Ctrl+Alt+X): Side bar tab - Snippet view  e.g. Letter syltes - \mathbb{R} 
>>>>>>>>>>> VSCOde extension ; Insret Unicode ; https://marketplace.visualstudio.com/items?itemName=brunnerh.insert-unicode
  ℝ Double-STRUCK CAPTIAL R
  Advantages: human-readable.¹₁
  e.g. superscript, subscript
  + math preview
  \(X^{sup}_{sub}\)

  \int_{a}^{b} f(x) \, dx
\( ax + by + cz + d = 0 \)
\[\frac{\partial}{\partial x} (ax + by + cz + d) = a, \quad \frac{\partial}{\partial y} (ax + by + cz + d) = b, \quad \frac{\partial}{\partial z} (ax + by + cz + d) = c\]
https://marketplace.visualstudio.com/items?itemName=Hyeon.c-math-viewer



TODO: https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions
  https://en.wikipedia.org/wiki/Ridge_function
Gaussian-error linear unit (GELU)

in https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ 📅 2024-09-23 09:14:56
  lspci | grep -i nvidia
  uname -m && cat /etc/*release
  gcc --version
  # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=24.04&target_type=deb_network
  
  # CUDA Toolkit Installer
  wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb && \
  sudo dpkg -i cuda-keyring_1.1-1_all.deb && \
  sudo apt update -y && \
  sudo apt -y install cuda-toolkit-12-6

  # Install the open-source NVIDIA driver
  # `nvidia-open` is based on NVIDIA's open-source kernel module, offering better compatibility with the Linux kernel and allowing the community to contribute and access the code.
  sudo apt install -y nvidia-open
  
  # checks your system for hardware components (like GPUs) and shows the available drivers for them
  sudo ubuntu-drivers devices

  # Install the proprietary NVIDIA driver
  # `nvidia-driver-560` is a closed-source driver provided by NVIDIA, optimized for performance in tasks like gaming, machine learning, and other GPU-accelerated workloads.
  sudo apt install -y nvidia-driver-560

  # ...  13. Post-installation Actions ; https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#post-installation-actions
  echo 'export PATH=/usr/local/cuda-12.6/bin${PATH:+:${PATH}}' >> ~/.bashrc
  echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' >> ~/.bashrc
  
  ## ➡️ Reboat your computer
  # 13.2. Recommended Actions
  # 13.2.1. Install Persistence Daemon
  #  CUDA Samples are now located in https://github.com/nvidia/cuda-samples, which includes instructions for obtaining, building, and running the samples.
  #   NVIDIA is providing a user-space daemon on Linux to support persistence of driver state across CUDA job runs. The daemon approach provides a more elegant and robust solution to this problem than persistence mode. For more details on the NVIDIA Persistence Daemon, see the documentation here.
  #   The NVIDIA Persistence Daemon can be started as the root user by running:
  sudo /usr/bin/nvidia-persistenced --verbose
  # it may be already run. check by either of 
  #   sudo cat /var/log/syslog | grep nvidia
  #   sudo journalctl -xe | grep nvidia
  ps aux | grep nvidia-persistenced
  sudo nvidia-smi -pm 1
  # you can check persistence mode by $shell> nvidia-smi
  
  # 13.2.3. IDE Integration
  #  ⚓ Nsight Visual Studio Code Edition ; https://marketplace.visualstudio.com/items?itemName=NVIDIA.nsight-vscode-edition

  # 13.2.2. Install Writable Samples ; https://github.com/nvidia/cuda-samples

https://docs.ultralytics.com/#yolo-a-brief-history
YOLOv8 is the latest version of YOLO by Ultralytics. 

  https://docs.ultralytics.com/datasets/detect/coco/

yolo 8
  ⚓🚣 Yolo clsses name ;
    https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml
  # https://github.com/ultralytics/ultralytics
  git clone https://github.com/ultralytics/ultralytics.git
  👍 IDE integration; https://docs.ultralytics.com/integrations/vscode/
    https://marketplace.visualstudio.com/items?itemName=Ultralytics.ultralytics-snippets

wbfw109v2@wbfw109v2-500TFA-500SFA ~/r/i/p/python-study (main) [1]> poetry add torch=^2.4.1
  Updating dependencies
  Resolving dependencies... (0.4s)

  Because no versions of torch match >2.4.1,<3.0.0
  and torch (2.4.1) depends on nvidia-cudnn-cu12 (9.1.0.70), torch (>=2.4.1,<3.0.0) requires nvidia-cudnn-cu12 (9.1.0.70).
  And because tensorflow[and-cuda] (2.17.0) depends on nvidia-cudnn-cu12 (8.9.7.29)
  and no versions of tensorflow match >2.17.0,<3.0.0, torch (>=2.4.1,<3.0.0) is incompatible with tensorflow[and-cuda] (>=2.17.0,<3.0.0).
  So, because python-study depends on both tensorflow[and-cuda] (^2.17.0) and torch (^2.4.1), version solving failed.

  CUDA와 cuDNN을 직접 설치한 후에는 tensorflow를 tensorflow[and-cuda] 없이도 GPU에서 사용할 수 있습니다. tensorflow 기본 패키지 자체는 CPU와 GPU 모두에서 동작할 수 있도록 설계되어 있으며, 사용자가 CUDA 및 cuDNN을 수동으로 설치할 수 있습니다.
  ➡️ tensorflow 에서 지침대로 설치했더니 문제가 되는듯.. tensorflow[and-cuda] 를 지우고 cuda 를 개별적으로 설치해보자. tensorflow 만 설치되도록 해보자.

>>>>>>>> bash script
  Shell Parameter Expansion ; https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html
    ${parameter:+word}
    🛍️e.g. idiom
      export PATH=/usr/local/cuda-12.6/bin${PATH:+:${PATH}}


Ubuntu ESM (Expanded Security Maintenance)
# if remote os is linux, ... otherwise it may requires Xming, XQuartz
sudo apt install -y xauth
xauth란?
  xauth는 "X Authority"의 약자이며, X11 서버가 클라이언트에게 접근 권한을 부여하기 위해 사용하는 인증 정보를 관리하는 도구입니다. X11은 네트워크를 통해 그래픽 인터페이스를 전송할 수 있는데, 이때 클라이언트가 X 서버에 접근할 때 권한을 확인해야 합니다. xauth는 이러한 접근을 위한 인증 파일(.Xauthority 파일)을 관리합니다.
  X11 포워딩을 사용할 때, 원격 서버가 로컬 X 서버에 접근할 수 있도록 인증 정보를 공유하는 역할을 하며, 원활한 그래픽 전송에 필요합니다.
choco install vcxsrv
  https://community.chocolatey.org/packages/vcxsrv
  %shell: Powershell> XLuanch
    Select display settings; Choose how Xming displays programs
      ✔️ Multiple Windows
    Select how to start Xming; Choose session type and whether a client is tsarted immediately.
      ✔️ Start no client
        This will just start Xming. You will be able to start local clients layer.
    Speicify paraemter settings; Enter clipboard, remote font server, and all other parameters.
      ✔️ Clipboard
    ... 📰📝 Can not find Native OpenGL option
      
nameserver의 의미와 윈도우 IP 주소: cat /etc/resolv.conf | grep nameserver 명령은 WSL2의 네트워크 설정에서 DNS 서버 주소를 보여줍니다. 이 nameserver는 일반적으로 윈도우 시스템의 내부 네트워크 IP 주소를 나타냅니다. 이 IP 주소는 WSL2가 윈도우와 통신하는 네트워크 인터페이스에서 할당된 것으로, 로컬 윈도우의 X 서버와 통신하기 위해 사용됩니다.
  

https://docs.openvino.ai/2024/notebooks/async-api-with-output.html
OpenVINO Training Extensions ; https://openvinotoolkit.github.io/training_extensions/stable/guide/get_started/introduction.html
  https://openvinotoolkit.github.io/training_extensions/stable/guide/get_started/installation.html#install-openvino-training-extensions-for-users-cuda-cpu
  https://openvinotoolkit.github.io/training_extensions/stable/guide/get_started/installation.html#run-tests
mermaid 그래프에 추가.
  tensort / openvino .. 
  .
⚓ https://docs.docker.com/engine/install/ubuntu/
  #️⃣ Uninstall old versions ; https://docs.docker.com/engine/install/ubuntu/#uninstall-old-versions
  #️⃣ Instasll using the apt repository
# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc
# Add the repository to Apt sources:
echo "deb [arch=(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu (source /etc/os-release; echo $VERSION_CODENAME) stable" \
    | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
# Update the package list:
sudo apt-get update
https://docs.docker.com/desktop/install/linux/
https://docs.docker.com/desktop/install/linux/ubuntu/
sudo apt-get update
sudo apt-get install ./docker-desktop-<arch>.deb
# install docker
sudo apt install -y docker.io
GPG stands for GNU Privacy Guard, a cryptographic system used to verify the integrity and authenticity of downloaded files or software. In the context of Docker installation, the GPG key is used to ensure that the Docker packages you're installing are indeed from the official Docker repository and haven’t been tampered with.
sudo apt-get install ca-certificates curl gnupg lsb-release
🆚📰 Why is apt-key being deprecated? keyrings....
🛺 automation script: install docker 🔗 https://docs.docker.com/engine/install/ubuntu/
  # Add Docker's official GPG key:
  sudo apt update -y
  sudo apt install -y ca-certificates curl
  sudo install -m 0755 -d /etc/apt/keyrings
  sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
  sudo chmod a+r /etc/apt/keyrings/docker.asc
  # Add the repository to Apt sources:
  bash -c 'echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null'
  # 
  sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
  sudo chown -R $USER:$USER ~/.docker
  #
  sudo systemctl status docker
  sudo systemctl start docker
  sudo systemctl enable docker
  
  # 
  sudo docker run hello-world
🚣 OTX as docker
  git clone https://github.com/openvinotoolkit/training_extensions.git
  cd training_extensions/docker
  bash ./build.sh
  docker image ls | grep otx
  export OTX_VERSION=2.1.0
  docker run -it --rm -v ~/data:/workspace/data otx:${OTX_VERSION}-cuda /bin/bash
  docker run -it --rm otx:${OTX_VERSION}-cuda otx train --task classification --model mobilenet_v2 --train-data /tmp/train --val-data /tmp/val --output /tmp/output
sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 1
ModuleNotFoundError: No module named 'otx'
  poetry shell로 가상환경을 활성화한 상태에서 sudo를 사용하면, root 권한으로 실행되기 때문에 가상환경이 제대로 적용되지 않는 문제가 발생합니다. sudo는 기본적으로 새로운 세션에서 실행되기 때문에, 현재 활성화된 가상환경 정보가 root 사용자에게 전달되지 않습니다. 이 때문에 ModuleNotFoundError가 발생한 것입니다.
  cd ~/repo/intel-edge-academy-6/prototypes/python-study && poetry shell
  docker build 명령어가 기본적으로 root 권한을 필요로 하기 때문일 가능성이 있습니다. 하지만, 기본적으로 Docker는 sudo 없이 실행될 수 있도록 설정할 수 있습니다. 즉, Docker를 sudo 없이 실행할 수 있도록 권한 설정을 해두지 않았을 경우 sudo가 필요하게 됩니다.
    # -a: append. G: group
    sudo usermod -aG docker $USER
    newgrp docker # new group
CDMA; LTE, 5G
https://github.com/tailscale/tailscale/issues/5160





nvidia-persistenced
  Roles
    - GPU 드라이버 상태 유지: nvidia-persistenced 데몬은 GPU의 드라이버 상태를 여러 작업 사이에 유지합니다. 이는 GPU가 작업을 종료할 때마다 드라이버가 해제되고 다시 로드되는 것을 방지하기 위해 사용됩니다.
    - 성능 향상: 드라이버 상태를 유지함으로써, 작업 간 드라이버 로드 및 초기화 시간을 줄이고 성능을 최적화할 수 있습니다.
  WSL 에서 flask 로 model 을 추론하고 돌려보내주는 서버를 만들 떄 로딩시간이 매우 길어질 수 있다. (이전 경험...)
    nvidia-persistenced가 도움이 될 수 있는 상황이 바로 그런 경우입니다. Flask 서버에서 모델 추론을 수행할 때, 특히 WSL 환경에서 GPU를 사용하는 경우, 드라이버가 초기화되는 데 시간이 걸릴 수 있습니다. 매번 새로운 추론 요청이 들어올 때 GPU가 다시 초기화되는 문제가 발생할 수 있고, 이로 인해 긴 로딩 시간이 생길 수 있습니다.


Current pwd: /home/wbfw109v2/repo/intel-edge-academy-6 
  e.g. %shell>
    yolo train data=/home/wbfw109v2/datasets/kct-yh2hv-cat_dog_dataset-2023-01-30-2015/data.yaml model=yolov8n.pt epochs=10 batch=4 && \
    yolo val data=/home/wbfw109v2/datasets/kct-yh2hv-cat_dog_dataset-2023-01-30-2015/data.yaml model=yolov8n.pt split=test && \
    # check result files in /home/wbfw109v2/datasets/kct-yh2hv-cat_dog_dataset-2023-01-30-2015/runs/detect/val. you can see also confusion_matrix.png


yolo v8
wbfw109v2@wbfw109v2-500TFA-500SFA ~/r/intel-edge-academy-6 (main)> yolo help

    Arguments received: ['yolo', 'help']. Ultralytics 'yolo' commands use the following syntax:

        yolo TASK MODE ARGS

        Where   TASK (optional) is one of {'detect', 'segment', 'pose', 'classify', 'obb'}
                MODE (required) is one of {'benchmark', 'predict', 'train', 'val', 'track', 'export'}
                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.
                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'

    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01
        yolo train data=coco8.yaml model=yolov8n.pt epochs=10 lr0=0.01

    2. Predict a YouTube video using a pretrained segmentation model at image size 320:
        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320

    3. Val a pretrained detection model at batch-size 1 and image size 640:
        yolo val model=yolov8n.pt data=coco8.yaml batch=1 imgsz=640

    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)
        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128

    5. Explore your datasets using semantic search and SQL with a simple GUI powered by Ultralytics Explorer API
        yolo explorer data=data.yaml model=yolov8n.pt
    
    6. Streamlit real-time webcam inference GUI
        yolo streamlit-predict
        
    7. Run special commands:
        yolo help
        yolo checks 
        yolo version
        yolo settings
        yolo copy-cfg
        yolo cfg

    Docs: https://docs.ultralytics.com
    Community: https://community.ultralytics.com
    GitHub: https://github.com/ultralytics/ultralytics

    yolo train --data 



Road Traffic Controller
traffic controller Hand Gesture

#!/bin/bash
sudo chmod 664 /mnt/gdrive/book/*

실무에서는
yolo8
  torch

비보호 좌회전 보조 시스템.
비보호 사고량 감소 및 개념 확립.

ADAS 시스템 수신호
  Dataset download
    wget --recursive --no-parent --no-host-directories --cut-dirs=3 --reject "index.html*" https://mwt-www.e-technik.uni-ulm.de/downloads/publicData/Kern2023/misc/


%shell>  # check directory structure
  wget --spider --recursive --no-parent https://mwt-www.e-technik.uni-ulm.de/downloads/publicData/Kern2023/
  rclone tree https://mwt-www.e-technik.uni-ulm.de/downloads/publicData/Kern2023/
  rclone lsf --recursive https://mwt-www.e-technik.uni-ulm.de/downloads/publicData/Kern2023/



🆚 ROI, bounding box
  https://universe.roboflow.com/yolodataset/person-dataset-mvbk4
2024/05/24
  ✔️ latex
    for file in note/sorted_note/*.tex; latexmk -pdf -lualatex -synctex=1 -interaction=nonstopmode -auxdir=/tmp -outdir=(pwd)/build/note $file; end

    latexmk -pdf -lualatex -synctex=1 -interaction=nonstopmode -f -auxdir=/tmp -outdir=$(pwd)/build/note note/sorted_note/about_vector.tex
    for file in note/sorted_note/*.tex; latexmk -pdf -lualatex -synctex=1 -interaction=nonstopmode -f -auxdir=$(pwd)/build/tmp -outdir=$(pwd)/build/note $file; end
    EXINPUTS=.:$(pwd)/note/sorted_note//: latexmk -pdf -lualatex -synctex=1 -interaction=nonstopmode -f -auxdir=/tmp -outdir=$(pwd)/build/note note/sorted_note/*.tex

    env TEXINPUTS=.:$PWD/note/sorted_note//: latexmk -pdf -lualatex -synctex=1 -interaction=nonstopmode -f -auxdir=$PWD/build/note -outdir=$PWD/build/note $PWD/note/sorted_note/about_vector.tex
    >>> 🅾️ env TEXINPUTS=.:$PWD/note/sorted_note//: latexmk -pdf -lualatex -synctex=1 -interaction=nonstopmode -auxdir=$PWD/build/note -outdir=$PWD/build/note $PWD/note/sorted_note/about_vector.tex -f
  ✔️✅ (how); study scanned Book
    * orders
      1. Download Pdf viewer
        * For Windows, https://community.chocolatey.org/packages/okular
          %shell> choco install okular
        * For Linux, https://snapcraft.io/okular
          %shell> snap install okular
      2. Syncrhonize Google Drive
        * For Windows, https://community.chocolatey.org/packages/googledrive
          %shell> choco install googledrive
          and drag and drop any unused files, right-click and set 'Offline Access - ✔️ Available offline'.

        * For Linux
          use Rclone (MIT License)
            https://rclone.org/
            https://en.wikipedia.org/wiki/Rclone
            https://github.com/rclone/rclone
          Install ; https://rclone.org/downloads/
            sudo -v ; curl https://rclone.org/install.sh | sudo bash && \ 
            rclone config
            # ... many settings..; e.g. 19: google drive
          🛍️ e.g. 
            Google cloud API wbfw109v2 Oauth client json file.
              OAuth client created
                Client ID
                    279512517955-mlohe28s7f294fv996t4c6j9b3fjv98i.apps.googleusercontent.com
                Client secret
                    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
                Creation date
                    September 10, 2024 at 10:07:42 AM GMT+9
                The client ID and secret can always be accessed from Credentials in APIs & Services 
            https://rclone.org/drive/#making-your-own-client-id
            https://console.cloud.google.com/apis/library/drive.googleapis.com?project=inner-geography-435200-k4&supportedpurview=project
            https://en.wiktionary.org/wiki/consent
            https://developers.google.com/drive/api/guides/api-specific-auth
            https://developers.google.com/identity/protocols/oauth2/
            https://support.google.com/cloud/answer/10311615#user-type
          Syncrhonize  
            # ??? not use mount location as /mnt/. it requires permission when save file in pdf viewer Ockular
            🛍️ e.g. %shell> sudo rclone --config ~/.config/rclone/rclone.conf sync gdrive:book ~/drive/gdrive/book
            //📍 쓰기 권한 부여하기!!
              sudo chown -R $USER:$USER ~/drive
              sudo chmod -R u+w ~/drive
              

        ❓...1주일마다 토큰 발급받아야하나? dd.. 인증화면이뜬다고 한다.. rclone 에서 파일동기화 또는 접근하는 명령 사용 시..
          또는 수동 갱신 명령 sudo rclone sync gdrive:book /mnt/gdrive/book
        >>> sudo rclone sync gdrive:book /mnt/gdrive/book
        # Check the list of all configured remotes (such as Google Drive, OneDrive, etc.)
        sudo rclone listremotes --config ~/.config/rclone/rclone.conf
      
        ### synchronize when you want
        ## ⭕ firstly use with flag --dry-run for safe operation
        ## Use this when you want to download or update the local folder with the latest changes from Google Drive.
        # upload from local file to remote file
        # sudo rclone sync ~/drive/gdrive/book gdrive:book --config ~/.config/rclone/rclone.conf

        # Use this when you want to upload or update the Google Drive folder with the latest changes from the local folder.
        

        rclone mount는 원격 드라이브를 파일 시스템처럼 다루지만, 파일 단위로 데이터를 처리합니다. 즉, PDF 파일을 수정할 때마다 전체 파일이 다시 업로드됩니다. 따라서 PDF 파일에 주석을 추가하거나 수정하면, 파일 전체(예: 300MB)를 다시 업로드하게 됩니다. rclone mount는 파일의 일부만 변경하는 방식으로 동작하지 않기 때문에, 주석만 수정된 경우에도 전체 파일을 다시 동기화합니다.  
        일부 클라우드 스토리지 서비스는 네이티브 클라이언트(예: Google Drive의 공식 클라이언트)에서 차등 동기화(파일의 일부만 변경된 경우, 그 부분만 업로드)를 지원할 수 있지만, rclone에서는 이러한 기능이 제공되지 않습니다.

        🚨 (issue): bug; Could not open file:///home/wbfw109v2/drive/gdrive/book/...
          만약 Snap으로 설치된 Okular에서 이러한 문제가 발생했다면, Snap의 보안 샌드박스 때문일 수 있습니다. 이 경우 Snap 대신 apt로 설치된 버전을 사용하는 것이 좋습니다.
            ...
          sudo snap remove okular
          sudo apt update
          sudo apt install -y okular
        O 맞음;;
  >>>>>
    동작판별을 위한 데이터셋 다운로드 및 사용방법 체크
    ⚓ Hierarchical Data Format (HDF) ; https://en.wikipedia.org/wiki/Hierarchical_Data_Format
    * Action recognition

    * Action recognition
      - PoseC3D: A newer approach that uses a combination of pose information and 3D convolutions to detect actions based on pose sequences.
        It's a top choice for cases where the focus is on human movements rather than environmental context.

    rtxa 6000...  
      빌릴 수 있다고 한다.


tech_stack
  remote_acces_tools
    anydesk.txt
  langugae
    c++
    


📰면접.. NVIDIA TensorRT vs. Cuda vs OpenVINO
  CUDA is a parallel computing platform and programming model created by NVIDIA.
  TensorRT is an SDK designed specifically for deep learning inference acceleration on NVIDIA GPUs.

  TensorRT
  pytorch -> .pt
  keras -> .keras

🚣 Labeling tool
  https://www.cvat.ai/



https://docs.opencv.org/4.x/d4/dd5/highgui_8hpp.html
  https://docs.opencv.org/4.x/d7/dfc/group__highgui.html#ga89e7806b0a616f6f1d502bd8c183ad3e 
  typedef void(* 	cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)
    Callback function for mouse events. see cv::setMouseCallback. More...
  typedef void(* 	cv::TrackbarCallback) (int pos, void *userdata)
    Callback function for Trackbar see cv::createTrackbar. More...
https://docs.opencv.org/4.x/dc/d4d/tutorial_py_table_of_contents_gui.html
  https://docs.opencv.org/4.x/db/d5b/tutorial_py_mouse_handling.html
  https://docs.opencv.org/4.x/d9/dc8/tutorial_py_trackbar.html



Ctrl + Alt + -
Ctrl + Shift + -

>>>>>>>> in fish, poetry add (grep -v '^#' requirements.txt)

https://github.com/openvinotoolkit/openvino_notebooks/tree/2023.1/notebooks/001-hello-world

poetry add openvino>=2024.2.0 nncf>=2.11.0
poetry source add pytorch https://download.pytorch.org/whl/cpu
poetry add --source pytorch "surya-ocr==0.4.0" torch datasets "gradio>=4.19" Pillow


poetry add openvino nncf
poetry add torch datasets
poetry add openvino nncf surya-ocr torch datasets gradio pillow

sudo apt install python3-tk
import matplotlib
matplotlib.use("TKAgg")


os.environ["GIT_CLONE_PROTECTION_ACTIVE"] = "false"

%pip install -q "openvino>=2024.2.0" "nncf>=2.11.0"
%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu "surya-ocr==0.4.0" torch datasets "gradio>=4.19" Pillow


????????????
AVX-512 ????
  ROcm

Distribution  이 pip, openvino archives, apt, github 로  많던데 뭐로 설치 해야 하지? https://docs.openvino.ai/2024/get-started/install-openvino.html?PACKAGE=OPENVINO_BASE&VERSION=v_2024_4_0&OP_SYSTEM=LINUX&DISTRIBUTION=ARCHIVE
Machine leraning Pipeline; MLOps
  DL Type
  Data preparation
  Train
  Optimization
  Deployment
Open VINo ; https://en.wikipedia.org/wiki/OpenVINO
  https://github.com/openvinotoolkit/openvino_notebooks/tree/2023.1/notebooks/004-hello-detection
  https://github.com/openvinotoolkit/openvino_notebooks/tree/2023.1/notebooks/003-hello-segmentation
  https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/vehicle-detection-and-recognition/vehicle-detection-and-recognition.ipynb
  Monodepth Estimation with OpenVINO ; https://docs.openvino.ai/2024/notebooks/vision-monodepth-with-output.html
  Image Background Removal with U^2-Net and OpenVINO ; https://docs.openvino.ai/2024/notebooks/vision-background-removal-with-output.html
  Background removal with RMBG v1.4 and OpenVINO ;   https://docs.openvino.ai/2024/notebooks/rmbg-background-removal-with-output.html
  Live Object Detection with OpenVINO™ ; https://docs.openvino.ai/2024/notebooks/object-detection-with-output.html

  https://github.com/openvinotoolkit/open_model_zoo
  Used Docs in classrooms
    https://github.com/kccistc/intel-05
    openvinotoolkit/openvino_notebooks: 📚 Jupyter notebook tutorials for OpenVINO™
    
    git clone --recurse-submodules https://github.com/kccistc/intel-05.git
    git clone --dept=1 https://github.com/openvinotoolkit/openvino_notebooks.git

  ⚓ GitHub OpenVino Notebooks ; https://github.com/openvinotoolkit/openvino_notebooks
    👍 notebooks search by category ; https://openvinotoolkit.github.io/openvino_notebooks/?search=hello+image
      from https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks
      
    # Hello Image Classification
    Line-level text detection with Surya ; https://docs.openvino.ai/2024/notebooks/surya-line-level-text-detection-with-output.html

    Getting Started
    Convert & Optimize
    Model Demos
    Model TrainingLive demos
  Open Visual Inferencing and Neural network Optimization
  ONNX -> IR 포맷으로 모델 저장.
  intel N100
  https://coral.ai/

  hOW oPENvINO WORKS?
    MODEL
      ...model training..
      Open Model Zoo
      Model compression

    OPTIMIZE
      Model Conversion API & Tools
      Training optimiztion with NNCF
      Intel Developer Cloud for the Edge
    DEPLOY
      OpenVINO Model Server
      OpenVINO Runtime
        Primary CPU and GPU Options
        xeon, core, atom, arm, iris, arc, gpu
        Secondary VPU and FPGA Options
          VPU .. https://en.wikipedia.org/wiki/Movidius
          👍📰 FPGA // 프로그래밍을 거쳐서 회로를 구성할 수 있다고 한다.

      Intel, arm coretex
  Benefits of OpenVINO
    6세대이상부터 지원 가능.. 행렬연산이 가능하다고 함?
    10 ~ 11 세대에서 instruction set 이 누락된 제품도 있어서 잘 봐야 한다고 함.
  Caching model
    GPU 등과 같이 loading time 이 긴 device 들은 cache 기능을 사용하여 2번째 사용부터는 loading 타임을 최소화 할 수 있다.
    isReadNetwork
    ...
cat /proc/cpuinfo




%shell> lspci

차량 CAN 통신
  acc acc active.
  조향각
  NVIDIA Jetson TX2 
  외장 배터리..
  ROS
  👍 스테레오스코피 카메라를 활용한 비전 기술.. 로봇 회사들
  질감 처리도 필요한가?
  Image Background Removal with U^2-Net and OpenVINO
>>>>>>> ⚙️ settings: Firefox - Add-ons and themes     // change Menu bar color.
  Manage Your Themes
    https://addons.mozilla.org/en-US/firefox/addon/beautiful-dark-wall/?utm_source=addons.mozilla.org&utm_medium=referral&utm_content=search
>>>>>>> 0 settings: Ubuntu - Okular change Menu bar titles
  code  ~/.config/okularrc
  code /usr/share/plasma/desktoptheme/breeze-dark/colors

  sudo apt install kde-style-breeze kde-standard plasma-workspace


configure sddm
  gdm3 vs sddm

https://www.reddit.com/r/kde/comments/7ww4ja/changing_okular_colour_scheme/

poetry show openvino-dev

Released: Sep 19, 2024
  %shell> omz_
  omz_downloader --print_all
  omz_downloader --help

import https://docs.python.org/3/library/tkinter.html
  it requirses.. sudo apt-get install python3-tk
  poetry add ultralytics 
    https://github.com/ultralytics/assets/releases/

  /tmp/ipykernel_59372/2076527990.py:15: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.
    cmap = matplotlib.cm.get_cmap(colormap)
  Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolo_nas_s.pt to 'yolo_nas_s.pt'...
  100%|██████████| 83.3M/83.3M [04:46<00:00, 305kB/s]
  WARNING ⚠️ yolo_nas_s.pt appears to require 'super_gradients', which is not in Ultralytics requirements.
  AutoInstall will run now for 'super_gradients' but this feature will be removed in the future.
  Recommend fixes are to train a new model using the latest 'ultralytics' package or to run a command with an official Ultralytics model, i.e. 'yolo predict model=yolov8n.pt'
  requirements: Ultralytics requirement ['super_gradients'] not found, attempting AutoUpdate...


def normalize_minmax(data):
    return (data - data.min()) / (data.max() - data.min())


https://matplotlib.org/stable/api/cm_api.html#matplotlib.cm.get_cmap
  https://matplotlib.org/stable/api/_as_gen/matplotlib.colors.Colormap.html#matplotlib.colors.Colormap

dron gesture
  e.g. https://github.com/kinivi/tello-gesture-control
    https://developers.googleblog.com/en/drone-control-via-gestures-using-mediapipe-hands/

pyserial
paddle ocr, naver api, droidCam
RestNet50 ??
  🌟 사람 특징>> 뽑아내서 내적으로 유사도를 계산해서 CCTV 에 비치는 
    Cos 특성: cos 유클리드 값으로 .. ResNet 으로 알아낼 수 있나?
    저층에서 고층으로 가는구조.
      저층. 중층. 외형.
    이 데이터를 벡터 (특징)으로 처리해주는것.
    다양한 벡터값을 한사람으로 인식하도록 클러스터링하는 것.
    클러스터링이란? 이 데이터를 가지고, ...
  >> 🪱 Anomaly Detection
    https://github.com/kccistc/intel-05/blob/miniProject_09_suit/class01/mini-project/09_suit/source_09_suit_final.py
duck typing.. threading

✅ (how); to use Remote Power On/Off and connect Remote Desktop 📅 2024-10-02 10:29:22
  [Remote Power On/Off]     // Optional
    - check whehter your BIOS support AC back, and turn on
      %shell> sudo apt updtae && sudo apt install -y cpu-x
    - and ...
  connect Remote Desktop
    Any desk account ; https://support.anydesk.com/knowledge/account-creation
      wbfw109v2@gmail.com     // a..
    https://anydesk.com/ko/downloads/linux
    # http://deb.anydesk.com/howto.html
    - add repository key to Trusted software providers list
      %shell> sudo wget -qO - https://keys.anydesk.com/repos/DEB-GPG-KEY | sudo apt-key add -
    - add the repository:
      %shell> echo "deb http://deb.anydesk.com/ all main" | sudo tee /etc/apt/sources.list.d/anydesk-stable.list
    - update apt cache and install anydesk
      %shell> sudo apt update -y && sudo apt install -y anydesk
    
    - run anydesk
      %shell> anydesk
    - settings
      Security - Permissions
        - ✔️ Enable unattended access
          Password: ...
          Confirm Passsword: ...
          Permissions Profile: Full Access
    # 607 호 desktop anydesk address: 1 896 175 971
  connect Remote by SSH
    - check remote desktop public IP
      - in Browser: https://www.whatismyip.com/
      - as command
        # curl icanhazip.com
        # python3 -c "import requests; print(requests.get('https://ifconfig.me').text)"
        curl ifconfig.me
    - https://forcloud.tistory.com/210
    - settings
      Security - Permissions
        - ✔️ Create TCP tunnels
    - check what is your Local port
      Connection - General - Local port


https://velog.io/@minkyu4506/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-EfficientFormer-Vision-Transformers-at-MobileNet-Speed-%EB%A6%AC%EB%B7%B0 

Arduino
  ls -al /dev/ttyACM0
  Clone code
    git clone --recurse-submodules https://github.com/kccistc/intel-05.git
        --recurse-submodules option 없이 clone 한 경우, 아래를 통해 submodule update
    📍 git submodule update --init --recursive
  sudo chmod a+rw /dev/ttyACM0
  # ??? ⚠️ You need to make flash every time connecting arudino to Windows
  sudo chmod -R 777 /dev/ttyACM0

  ls -al /dev/ttyACM0
  # https://www.kernel.org/doc/html/latest/search.html?q=dialout not found..
  # https://wiki.debian.org/SystemGroups#Other_System_Groups dialout
  sudo usermod -aG dialout $USER

  ... cd arduino/...
  make init && make build && make flash
  # flash; (transitive, computing) To write to the memory of (an updatable component such as a BIOS chip or games cartridge).    https://en.wiktionary.org/wiki/flash
  cd ../ (smart-factory)
    bash build.sh

The tee command in Unix-like systems gets its name from the T-splitter used in plumbing, which splits the flow of water into two directions. Similarly, the tee command in Linux splits the output of a program into two: one going to standard output (usually the terminal) and the other going to a file or another command.

Maximal Softmax Prediction (MSP)
Mahalanobis Distance
Outlier Exposure

❓ OOD Detection
  https://velog.io/@yetsyl0705/Out-of-Distribution-OOD
Range-Doppler Maps

I wonder how MMPose + MoViNet can be used simultaniosuly.
  you said MoViNet is not based on Pose. but MMPose estimates Human Pose.
  but your recommendation uses "MMPose + MoViNet"


1. Person recognition
  - YOLO-NAS

2. Pose Recognition
  - MMPopse

3. Action Recognition
  - PoseC3D


MMPopse Movinet vs PoseC3D
hand signals and body pose are critical.
MoVinet
but some signals could become less distinct or harder to differentiate, especially quick gestures like raising or waving a hand.
Requires
Performance: 30FPS

https://en.wikipedia.org/wiki/Vienna_Convention_on_Road_Signs_and_Signals
📍 Jetn nano
  우분투에서 SD 카드 리더기를 연결하면 자동으로 특정 디렉토리에 마운트됩니다. 일반적으로는 /media/사용자이름/ 경로 아래에 SD 카드가 마운트됩니다. 예를 들어, 사용자 이름이 user이고 SD 카드의 레이블이 MySDCard인 경우, 마운트 지점은 /media/user/MySDCard가 될 것입니다.
🔳 Jetson Nano History
  iwlwifi-8000C-*.ucode failed with error -
  failed to read out thermal zone
  imx219 7-0010: error during i2c read probe 및 no acknowledge from address 0x50
  SQUASHFS error: unable to read xattr id index table




젯슨 나노에서 작업 - ⚓ https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#intro
  #️⃣ Write Image to the microSD card ; https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#write
  #️⃣ Setup and First Boot ; https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#setup
    Keyboard Shortcut ; https://miro.medium.com/v2/resize:fit:4800/format:webp/1*M9VeLZpfsPQSsnd3kEocvw.png
https://developer.nvidia.com/embedded/l4t/r32_release_v7.1/jp_4.6.1_b110_sd_card/jeston_nano/jetson-nano-jp461-sd-card-image.zip
  Jetson Nano Developer Kit SD Card Image (ver. 4.6.1) ; https://developer.nvidia.com/embedded/null 📅 2022/02/23
    📝 4.6.1 is lastest version for Jetson Nano. successor models (e.g. Jetson Orin Nano) use version 5.x.
    https://developer.nvidia.com/embedded/jetpack

    https://yeonsozzz.tistory.com/36
  # 👨 Manuallly.. check your mount drive. e.g. my SDCARD_DRIVE is sdc
  # 🪱 lsblk ; list block devices
  %shell> lsblk
  set SDCARD_DRIVE /dev/sdc
  # 🪱 dd ; Data Definition ; https://en.wikipedia.org/wiki/Dd_(Unix)
  # if; input file. of; output file. bs; block size
  sudo dd if=$HOME/Downloads/jetson-nano-jp461-sd-card-image/sd-blob-b01.img of=/dev/sdc bs=1M status=progress

  WIFI:
    - AP 명 : choi 혹은 choi_5G
    - security: WPA & WPA2 Perosnal (Wi-Fi Protected Access 2)
    - PW : s1234567890
  userName: jetsonnano
  Select Nvpmodel Mode (NVIDIA Power Model)
    MAXN


  📰 TODO >>>>>>>>>>>> https://pajamacoder.tistory.com/21
    가장 먼저 한일은 sudo 명령 없이 도커를 사용하기 위해 docker group에 user를 포함시키는 일이다. nvidia에서 제공하는 jetson nano os 를 설치 하고 나면 기본적으로 docker는 설치 되어있지만 user를 자동으로 docker그룹에 포함시지는 않는다.
    

    display message
    dmesg | tail | awk '$3 == "sd" {print}'
    sudo eject /dev/sd<x>

jetson nano server
sudo apt install ssh ssh-server
sudo systemctl enable ssh
systemctl status  
system control의

>> ycoto -> https://developer.ridgerun.com/wiki/index.php/Yocto_Support_for_NVIDIA_Jetson_Platforms_-_Setting_up_Yocto
🆚 is a relationship, hsa a relationship
  https://stackoverflow.com/questions/36162714/what-is-the-difference-between-is-a-relationship-and-has-a-relationship-in

발생한 문제가 무엇이고, 

➡️ pytorch torchrun
  torch.distributed.elastic ; https://pytorch.org/docs/stable/distributed.elastic.html
    # Usage
      Quickstart ; https://pytorch.org/docs/stable/elastic/quickstart.html
      Train script ; https://pytorch.org/docs/stable/elastic/train_script.html
      Example ; https://github.com/pytorch/elastic/tree/master/examples

    # API
      torchrun (Elastic Launch) ; https://pytorch.org/docs/stable/elastic/run.html
      
* Seller from https://developer.nvidia.com/buy-jetson
  NVIDIA® Jetson Nano™ Developer Kit - B01 ; https://www.seeedstudio.com/NVIDIA-Jetson-Nano-Development-Kit-B01-p-4437.html
  NVIDIA® Jetson Orin™ Nano Developer Kit ; https://www.seeedstudio.com/NVIDIAr-Jetson-Orintm-Nano-Developer-Kit-p-5617.html

* Requirements
  Hardware Requirements
    - Embedded Board:   [NVIDIA] Jetson Nano (4 GiB)                        1 ₩ 453,330 ; https://prod.danawa.com/info/?pcode=17458712&keyword=NVIDIA+Jetson+NANO+Developer+Kit&cate=112751
    - ⚓ Arduino UNO R3 ; https://docs.arduino.cc/hardware/uno-rev3/
        Datasheet ; https://docs.arduino.cc/resources/datasheets/A000066-datasheet.pdf
  Software Requiements
    - Selectable deployement (Exculsive): [OpenVino, TensorRT]
* Architecture
  Hardware
    - Embedded Board:   [NVIDIA] Jetson Nano (4 GiB)                        1 ₩ 453,330 ; https://prod.danawa.com/info/?pcode=17458712&keyword=NVIDIA+Jetson+NANO+Developer+Kit&cate=112751
    - Camera:           [Intel] Depth Camera D435                           1 ₩ 541,410 ; https://prod.danawa.com/info/?pcode=5797839&keyword=%EC%9D%B8%ED%85%94+%EB%A6%AC%EC%96%BC%EC%84%BC%EC%8A%A4+%EC%B9%B4%EB%A9%94%EB%9D%BC&cate=112810
    - Display:          라즈베리파이/젯슨나노/RDK-X7용 7인치 정전식 터치 스크린 1 ₩ 88,000 ; https://www.devicemart.co.kr/goods/view?no=1273487
  Software
    - Python
      📦 OpenVino
      📦 Pytorch

      # Optimization
        📦 OpenVINO ; https://docs.openvino.ai/2024/index.html
          OpenVINO 🔪 Python API Docs ; https://docs.openvino.ai/2024/api/ie_python_api/api.html
        📦 TensorRT ; https://developer.nvidia.com/tensorrt
          TensorRT 🔪 Python API Docs ; https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/index.html

    - OpenMMLab project 🔪 MMPose
      MMPose is an open-source 🚣 toolbox for pose estimation based on PyTorch.
      https://github.com/open-mmlab/mmpose?tab=readme-ov-file
      https://mmpose.readthedocs.io/en/latest/
* Paper
  📝 Paper review urls
    ⚓👍📰 good Paper review: https://ai-easy.tistory.com/
  ⚓ PYSKL: Towards Good Practices for Skeleton Action Recognition ; https://arxiv.org/abs/2205.09443
    ```PoseC3D has good spatio- temporal modeling capability and achieves state-of-the-art recognition performance on 6 of 9 benchmarks.```
* Model Pipeline
  - YOLO11m-pose 71.7 FLOPS  /  4(Quantization)  *  15 (FPS) = 268.87500 gFLOPS
    *⚙️ Tracker settings
      https://docs.ultralytics.com/modes/track/#tracker-selection
      https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/trackers/botsort.yaml
    * outputs
      classses id ; https://docs.ultralytics.com/tasks/pose/
        0: Nose
        1: Left Eye
        2: Right Eye
        3: Left Ear
        4: Right Ear
        5: Left Shoulder
        6: Right Shoulder
        7: Left Elbow
        8: Right Elbow
        9: Left Wrist
        10: Right Wrist
        11: Left Hip
        12: Right Hip
        13: Left Knee
        14: Right Knee
        15: Left Ankle
        16: Right Ankle
      # Keypoints ; https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco-pose.yaml
        kpt_shape: [17, 3] # number of keypoints, number of dims (2 for x,y or 3 for x,y,visible)

* Optimizations
  - 
FAQ
  ❔ Pose-based vs. Skeleton-based Action Recognition 📅 2024-10-02 15:25:28

    ### Pose-based vs. Skeleton-based Action Recognition

    #### **Pose-based Recognition**:
    - **Definition**: Uses **keypoints** or **landmarks** (joints) of the human body, such as shoulders, elbows, and knees, either in 2D or 3D coordinates.
    - **Data**: Works with the **positions of joints** over time, treating each keypoint individually.
    - **Models**: Commonly uses **CNNs** or **3D CNNs** to analyze pose sequences (e.g., **PoseC3D**).

    #### **Skeleton-based Recognition**:
    - **Definition**: Models the human body as a **graph** where **joints are nodes** and **bones are edges**.
    - **Data**: Focuses on the **spatial connections** between joints and captures their relationships over time.
    - **Models**: Uses **Graph Convolutional Networks (GCNs)** to process the **skeleton structure** (e.g., **MS-G3D**).

    #### **Key Differences**:
    - **Pose-based**: Works with **keypoints** independently, focusing on the temporal evolution of joint positions.
    - **Skeleton-based**: Uses a **graph structure** to model the relationships between joints and bones, offering more detailed spatial insights.

    ---

    ### Key Additions:
    - **Pose-based models** treat joints as independent **keypoints**, so the model doesn't inherently know how joints are connected (like the shoulder to the elbow). The AI may learn these relationships through training but doesn't explicitly model them.
      
    - **Skeleton-based models** explicitly model the relationships between joints as **graph edges** (e.g., shoulder connected to elbow). This makes it more effective at understanding **relative distances** and the **structure** of the body, which improves action recognition.

    - **Relative distance** between joints is critical in both approaches, but **skeleton-based models** handle these connections explicitly, making them more suitable for recognizing complex or multi-joint actions.


  ❔ Summary of Single-board Computers (SBCs) for AI Processing (Recent Release Order) 📅 2024-10-02 01:16:00
    ### Summary of Single-board Computers (SBCs) for AI Processing (Recent Release Order)


    | **Device**                                   | **Price** | **TFLOPS**   | **Release Date (ISO)** | **Supports OpenVINO Optimized Models** | **Comparison (Jetson Nano)**                                                                             |
    | -------------------------------------------- | --------- | ------------ | ---------------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------------- |
    | **Intel UP Xtreme i12**                      | $700~$900 | 2~4 TFLOPS   | 2023-02-10             | Yes                                    | Much more powerful, but higher cost and power consumption.                                               |
    | **Intel NUC 12 Pro**                         | $600~$800 | 2~3 TFLOPS   | 2022-06-15             | Yes                                    | Higher performance and price, suitable for various IoT and AI applications.                              |
    | **Intel UP Squared Pro 7000**                | $400~$600 | 1.5~2 TFLOPS | 2022-04-05             | Yes                                    | Offers decent performance at low power and cost.                                                         |
    | **NVIDIA® Jetson Orin™ Nano Developer Kit**  | $499      | 40 TOPS      | 2022-09-20             | No                                     | 100x the performance of Jetson Nano, higher price, for high-performance AI tasks.                        |
    | **Intel Elkhart Lake AAEON PICO-EHL4**       | $300~$400 | 1~1.5 TFLOPS | 2021-12-01             | Yes                                    | Similar performance to Jetson Nano, for low-power AI applications.                                       |
    | **Intel NUC 11 Pro Kit**                     | $500~$700 | 2 TFLOPS     | 2021-03-15             | Yes                                    | Better performance, but more expensive and higher power consumption than Jetson Nano.                    |
    | **NVIDIA® Jetson Nano™ Developer Kit - B01** | $149      | 0.472 TFLOPS | 2019-03-18             | No                                     | Great price-to-performance ratio, ideal for beginners and lightweight AI tasks.                         |

    ---

    ### Intel Lunar Lake Overview

    Intel's **Lunar Lake** was launched on **2024-09-24** and is designed for **up to 120 TOPS** in AI performance, with **48 TOPS** from the **NPU** alone. This makes it highly suitable for **AI-based edge computing**. Compared to older Intel models like **Elkhart Lake** and **Alder Lake**, which lacked dedicated AI acceleration, Lunar Lake brings significant improvements in performance-per-watt for AI tasks.

    However, there are **no Lunar Lake-based embedded boards** available yet, similar to NVIDIA's **Jetson Nano** series. Jetson remains a leading choice for **real-time AI inference** and **edge computing**, with the Jetson Nano and Jetson Orin Nano being popular models.

  ❔ Project Application Review: OpenVINO vs MediaPipe Framework 📅 2024-10-02 00:22:47
    # Project Application Review: OpenVINO vs MediaPipe Framework

    ## 1. OpenVINO
    - **Purpose**: OpenVINO is a platform designed to provide **optimized deep learning inference** on **Intel hardware** (CPU, GPU, VPU, etc.).
    - **Key Features**:
      - Converts models from **TensorFlow, ONNX**, etc., into **IR (Intermediate Representation)** format for optimized inference on **Intel-based hardware**.
      - Built for **high-performance inference**, suitable for both **edge and server environments**.
      - Supports a wide range of **Intel platforms** (CPU, iGPU, VPU), and is especially designed to maximize **real-time inference** performance.

    ## 2. MediaPipe Framework
    - **Purpose**: MediaPipe is a **framework for managing model pipelines** and **real-time data processing** across various platforms (Android, iOS, embedded devices).
    - **Key Features**:
      - Primarily supports **TensorFlow Lite** models and is optimized for **mobile and embedded devices** with a focus on **real-time processing**.
      - It optimizes the **data flow between models** through **asynchronous pipeline management**, handling real-time input data efficiently.
      - MediaPipe is designed for **lightweight environments** where real-time data streams are a priority, but is heavily optimized for **TensorFlow Lite** models.

    ---

    ## Project Application Review

    ### 1. **OpenVINO Requirements**
    - **The current project's software requirements align with OpenVINO**.
    - **OpenVINO** provides **optimized inference for Intel hardware**, while **MediaPipe is incompatible** as it primarily works with **TensorFlow Lite** and other lightweight formats.
    - **Conclusion**: Since the project’s software requirements necessitate **OpenVINO**, the **MediaPipe Framework is not suitable**, and **OpenVINO** should be used.

    ### 2. **Synchronous Process between Two Inferences**
    - The project workflow involves **pose recognition**, followed by **action recognition** based on joint position data, which requires the **pose data** to be processed first before action recognition can occur.
    - **MediaPipe’s asynchronous pipeline** excels at parallel processing between models, but the project requires a **synchronous process** between the two models.
    - **Conclusion**: Since **synchronous inference** is necessary, **MediaPipe’s asynchronous nature** does not align with the project’s requirements, and using a **MediaPipe pipeline would not be effective** in this case.
  ❔ Project Application Review: Why Hand Landmark Detection is Necessary 📅 2024-10-02 02:39:37
    # Project Application Review: Why Hand Landmark Detection is Necessary

    ## Why Hand Landmark Detection is Essential for the Project

    1. **Gesture Differentiation**:
      - Gestures like **"Push Away"** and **"Come Closer"** rely heavily on **hand orientation** (palm facing the camera or not). Simply using joint position data may not capture these subtle differences.
      - Hand Landmark Detection provides detailed keypoints of the hand, which help in recognizing the **front vs. back of the hand**.

    2. **Precise Hand Pose Data**:
      - **MediaPipe Hand Landmark Detection** offers high precision tracking of hand keypoints (finger tips, knuckles, etc.).
      - This data can be integrated with body joint data to improve the accuracy of **gesture recognition** in complex scenarios.

    3. **Improved Action Recognition**:
      - By using **hand landmark data** alongside **joint data**, the model can learn more nuanced movements, such as **hand rotations** and **finger positions**, which are crucial for distinguishing between similar gestures.

    ## Conclusion
    Using **Hand Landmark Detection** in combination with **joint position data** allows for a more robust and accurate gesture recognition system, making it essential for the successful implementation of your traffic signal recognition project.

  ❔ Why MMPose is a Better Choice than MediaPipe Pose 📅 2024-10-02 00:22:50
    # Why MMPose is a Better Choice than MediaPipe Pose

    ### 1. **Higher Joint Accuracy**
      - MMPose provides **more accurate joint coordinates** through various backbones (ResNet, HRNet, etc.), making it better for recognizing traffic signals from different angles as the vehicle moves.

    ### 2. **Handling Complex Poses and Angle Variations**
      - When a vehicle observes traffic signals from various angles, **pose recognition across different perspectives** becomes crucial. MMPose excels at managing such changes.

    ### 3. **Scalability and Customization**
      - MMPose allows for **retraining with custom datasets**, enabling fine-tuning for specific environments or actions.

    ### 4. **Precision in Joint Data**
      - For traffic signal recognition, **precise joint distances and changes** are essential. MMPose provides more detailed joint information, making it ideal for such tasks.

  ❔ About Parallel Processing 📅 2024-10-01 23:17:21
    in OpenVINO, https://ilya-lavrenov.github.io/openvino/openvino_docs_OV_UG_Python_API_exclusives.html#release-of-gil
      Some functions in Python API release the Global Lock Interpreter (GIL) while running work-intensive code
      #️⃣ List of Functions that Release the GIL
        🚣 openvino.runtime.Core.compile_model

    In MediaPipe, https://ai.google.dev/edge/mediapipe/framework/getting_started/python_framework
      ... MediaPipe framework sits on top of the 🚣 pybind11 library. The C++ core framework is exposed in Python via a C++/Python language binding.
        📝 Pybibnd ; https://pybind11.readthedocs.io/en/stable/index.html
      ... In the MediaPipe Framework, all processing takes places within the context of a CalculatorGraph.
      
의사 결정
  - Prepare model
    - 현재 하드웨어 요구사항에서 정상적으로 끊김없이 잘 작동할 수 있는 모델을 사용해야 한다.
    - about Action Recognition model
      - 관절 위치와 변화 기반 행동 인식
        이미지에서 배경보다는 포즈 데이터가 중요하므로 하므로, 행동 인식에 대한 모델은 Based on pose data (관절 위치) 이 되어야 한다. Based on vison 보다는.
      - 시공간적 정보를 담을 수 있는 모델을 사용해야 한다.
        제스처 인식은 단순히 하나의 프레임에서 포즈를 분석하는 것뿐만 아니라, 연속된 시간의 프레임에서 관절의 위치가 어떻게 변화하는지를 학습하는 것이 중요.
        🛍️ e.g.
          - Spatio-Temporal Multilayer Perceptron (ST-MLP)
          - CNN (Convolutional Neural Network)
          - RNN (Recurrent Neural Network)
          - LSTM (Long Short-Term Memory)
          - Transformer 기반 모델
          - ... 이전에 나온 모델들의 단점을 보완한 모델들
      - 포즈 인식 (관절 위치 탐지) 모델
        This reduces the complexity by focusing on the skeletal motion rather than the entire image.
          🛍️ e.g. 
            - MobileNet 기반 MMPose
            - ResNet 기반 MMPose
            - HRNet 기반 MMPose
            - Lite-HRNet 기반 MMPose
            - MediaPipe Pose
        filter: lightweight
          🛍️ e.g. 
            - MobileNet 기반 MMPose
            - MediaPipe Pose
      - 행동 인식 모델 (기본적으로 행동 인식을 지원하는 모델)
          🛍️ e.g. 
            EfficientGCN
            SGN
            MS-G3D
            DynamicGCN
            PA-ResGCN-b19
            4s-Shift-GCN
            2s-AGCN
            NAS-GCN
        filter: lightweight
          🛍️ e.g. 
            EfficientGCN ; https://github.com/attention-eq-everything/effgcn_cam
            SGN ; https://github.com/microsoft/SGN?tab=readme-ov-file

          These model uses NTU RGB+D 60 Dataset


    * Optimization

        // TensorRT 등으로 최적화 필요. 그냥 임베디드에서 돌리기에는 무거움.

      - 경량 Transformer 기반 모델
        🛍️ e.g.
        - EfficientFormer
        - EdgeFormer 
    - 정확도와 실시간 성능과의 균형을 맞추기 위해, 서로 다른 관절의 복잡한 공간관계를 더 잘 처리할 수 있는 EfficientFormer 를 선택

  대규모 데이터셋을 다루는 경우,
    Action Transformer
    ViViT (Video Vision Transformer): 


  리소스가 제한된 경우.

📍 전 커밋에서 특정 파일의 내용을 검색하려면
  git log -S "from pytubefix import YouTube"

Kaggle with ssh
  https://github.com/Kaggle/kaggle-api
  https://github.com/buidai123/Kaggle_VSCode_Remote_SSH


🆚 git add   vs git restore --staged
git push origin --delete <잘못된-브랜치-이름>
git push origin HEAD
   # 현재 체크아웃된 브랜치를 원격 저장소에 동일한 이름으로 푸시
  <refspec>은 로컬의 참조(브랜치, 태그 등)에서 원격의 참조로 어떤 것을 푸시할지를 결정하는 데 사용됩니다. Git에서 HEAD는 현재 체크아웃된 브랜치를 참조하는 이름이기 때문에, git push origin HEAD 명령어에서 HEAD는 로컬 브랜치를 원격 저장소에 푸시하는 대상 참조로 동작합니다.


https://github.com/ultralytics/ultralytics
  #️⃣ Integrations
https://openvinotoolkit.github.io/openvino_notebooks/?search=conver

How Does Action Recognition Work with Sequences?
  Multiple Frames: These models require a sequence of frames (for example, 16, 30, or even more frames) to analyze both spatial and temporal patterns (i.e., how body parts move over time).
  Sliding Window: The model might use a sliding window approach, meaning it continually analyzes a fixed number of frames (e.g., 16 frames) and shifts forward as new frames come in.

Mediapipe Object Detection vs YOLOv11: ➡️ 👍 YOLOv11
  YOLOv11 (nano/tiny versions) is likely better if you need highly optimized person detection and can afford a bit more latency compared to Mediapipe. YOLOv11 models can also be customized for specific tasks.

Action Recognition: MS-G3D vs. ST-GCN: ➡️ 👍 MS-G3D
  For skeleton-based action recognition, both models are strong contenders.
  MS-G3D provides more multi-scale temporal and spatial analysis, while ST-GCN is more lightweight and may offer higher FPS in real-time applications.


  ⚓ Softmax function ; https://en.wikipedia.org/wiki/Softmax_function
  (Subscript | 밑수) 와 (Superscript | 윗수)
  ⚓ Attention (machine learning) ; https://en.wikipedia.org/wiki/Attention_(machine_learning)#Scaled_dot-product_attention
    #️⃣ Self-attention ; https://en.wikipedia.org/wiki/Attention_(machine_learning)#Self-attention
  ⚓ Word embedding ; https://en.wikipedia.org/wiki/Word_embedding

  디코더는 기본적으로 RNNLM(RNN Language Model)입니다. RNNLM의 개념을 기억하고 있다면 좀 더 이해하기 쉽습니다. 디코더는 초기 입력으로 문장의 시작을 의미하는 심볼 <sos>가 들어갑니다. 디코더는 <sos>가 입력되면, 다음에 등장할 확률이 높은 단어를 예측합니다. 
  https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html

  👍
  [트렌스포머 모델 이해하기] Self-Attention에서 Q, K, V(Query, Key, Value)의 의미 ; https://cn-c.tistory.com/68#%EB%AA%A8%EB%93%A0%20%EB%8B%A8%EC%96%B4%EB%8A%94%20%EC%A7%88%EB%AC%B8(Query)%EC%9D%B4%EC%9E%90%20%EB%8B%B5%EB%B3%80(Key)%EC%9D%B4%EB%8B%A4-1
  https://velog.io/@sjinu/%EA%B0%9C%EB%85%90%EC%A0%95%EB%A6%AC-Attention-Mechanism


  c) Separable Convolution (SepLayer)
  일반적인 컨볼루션 레이어는 많은 계산을 요구합니다. Separable Convolution은 이러한 계산량을 줄이기 위해 컨볼루션을 두 단계로 나눕니다:
  Depth-wise Convolution: 각 채널에 대해 개별적으로 컨볼루션을 수행합니다.
  Point-wise Convolution: 결과를 결합하여 출력 채널을 만듭니다.
  이렇게 나누어서 처리하면 계산량이 줄어들고, 모델이 더 빠르고 효율적으로 동작할 수 있습니다.
  d) Bottleneck Layer (BottleLayer)
    일반적인 CNN 에서 torch 로 layer 를 구성할 때 코드는 보통 어떻게 짜나?

    - Multiple Input Branches (MIB) 아키텍처
    - 다양한 종류의 컨볼루션 레이어:
      Graph Convolution (GCN) 레이어
      Temporal Convolution (TC) 레이어
      Separable Convolution (SepLayer)
      Expanded Separable Layer (EpSepLayer)
      Bottleneck Layer (BottleLayer)
      Sandglass Layer (SGLayer)
      등의 구조를 GCN에 적용해 모델의 효율성을 높입니다.
    - Compound Scaling 전략:
      EfficientGCN-B0, EfficientGCN-B2, EfficientGCN-B4 같은 다양한 크기의 모델


  EfficientGCNv1
    https://github.com/zyxjtu/EfficientGCNv1
    ACMMM 2020 ; https://dl.acm.org/doi/abs/10.1145/3394171.3413802
    Arxiv Preprint ; https://arxiv.org/pdf/2010.09978.pdf

  EfficientGCN 2
    https://github.com/attention-eq-everything/effgcn_cam
    IEEE T-PAMI; https://ieeexplore.ieee.org/abstract/document/9729609
    Arxiv Preprint ; https://arxiv.org/pdf/2106.15125

  MLP (Multi-layer Perceptron): 여러 개의 층으로 이루어진 기본적인 신경망 구조입니다. 채널 단위나 공간적 차원에서 각각 독립적으로 연산을 수행하는 방식으로 사용됩니다.
  SENet (Squeeze-and-Excitation Networks): 각 채널의 중요도를 학습하여 적응적으로 조절하는 네트워크입니다. AGC-LSTM과 MS-AAGCN도 이런 방식으로 채널별 중요도를 독립적으로 학습합니다.
    - AGC-LSTM: Attention-Guided Convolution Long Short-Term Memory
      LSTM 기반의 골격 행동 인식 모델로, 관절 간의 관계를 학습하는 데 MLP와 SENet 구조를 결합하여 채널마다 독립적으로 학습합니다.
    - MS-AAGCN: Multi-Scale Adaptive Graph Convolutional Network
      Spatial Graph Convolution을 사용한 모델로, 주로 공간적 차원에서 학습하고 다른 차원을 평균 처리하는 구조입니다.
  LSTM (Long Short-Term Memory):
    순환 신경망(RNN)의 한 종류로, 시퀀스 데이터를 처리할 때 과거의 정보를 기억하면서 학습할 수 있도록 설계된 모델입니다.
    특히 긴 시퀀스 데이터를 처리할 때 뛰어난 성능을 보이며, 주로 시간에 따른 데이터 처리에 사용됩니다.
  . EfficientGCN-B0, B2, B4
    ; Base 의 약자. 


  Data input
    애초에 여러 프레임을 입력으로 받는다.
    시간적 정보가 복잡해지면, 더 많은 프레임을 통해 모델이 정확하게 행동을 인식할 수 있습니다. 여기서 성능은 **정확도(Accuracy)**를 의미하며, 더 많은 시간적 데이터를 사용할수록 행동의 세부적인 변화까지도 학습할 수 있어 정확도가 향상됩니다.
    그러나 프레임 수가 늘어날수록 추론 시간과 메모리 사용량이 증가합니다. 각 프레임에 대해 추가적인 연산을 수행해야 하기 때문에 TC 레이어가 더 많이 필요해지고, 이는 계산 비용을 증가시킵니다.
    Jetson Nano와 같은 임베디드 하드웨어에서는 처리 속도와 메모리가 제한적이기 때문에, 적절한 프레임 수를 선택하는 것이 중요합니다. YOLO, MediaPipe, EfficientGCN 등의 모델을 동시에 사용해야 한다면, 시간적 프레임 수는 적절히 줄이면서도 중요한 행동을 충분히 인식할 수 있도록 조정하는 것이 좋습니다. Jetson Nano의 성능을 고려하면, 초당 15~30 프레임으로 학습 및 추론을 진행하는 것이 적합할 수 있습니다.
    2. EfficientGCN의 N 개 프레임 설정 가능 여부
      EfficientGCN에서 N개의 프레임을 입력으로 설정할 수 있습니다. N은 모델이 행동을 인식할 때 사용하는 시간적 정보의 범위를 결정합니다. 즉, 한 번의 학습 또는 추론에서 몇 개의 연속된 프레임을 처리할지를 N으로 설정할 수 있습니다.

      프레임 수를 조정하는 이유:
      더 많은 프레임(N)이 사용되면 모델은 더 긴 시간 동안의 행동을 인식할 수 있어 정확도가 높아질 수 있습니다. 그러나 계산 비용이 증가하게 됩니다.
      4개의 모델 사용 시 프레임 수 조정:
      Jetson Nano에서 4개의 모델(YOLO, MediaPipe Hand Landmarks, Pose, EfficientGCN)을 사용할 때는 하드웨어 성능에 따라 프레임 수를 조절해야 합니다.
      권장 프레임 수는 8~16 프레임 사이가 적합할 수 있습니다. 이는 충분한 시간적 정보를 제공하면서도 Jetson Nano의 성능 한계를 넘지 않도록 설정하는 것입니다.
    EfficientGCN-B0
      x-sub120 데이터셋에 대해 정확도 86.6, x-set120 데이터셋에 대해 정확도 85.0

    90.2 2.73 1× 0.29 1×


  Cross-subject와 Cross-setup은 행동 인식에서 사용하는 두 가지 평가 설정입니다.
    Cross-subject: 훈련 데이터와 테스트 데이터가 다른 주체들로부터 나온 데이터를 사용하는 방식입니다. 훈련 시 사용된 사람과 테스트 시 사용된 사람이 다르기 때문에 모델이 얼마나 일반화되었는지 평가하는 데 사용됩니다.
    Cross-setup: 다른 환경 설정이나 다양한 상황에서 모델을 테스트하는 방식입니다. 이는 다양한 배경이나 환경에서의 행동 인식 성능을 평가합니다.

  , Table 7은 X-sub 벤치마크에서의 성능을 더 구체적으로 평가한 것입니다.
  FLOPs는 특정 데이터셋에서 측정된 연산량을 나타내며, 다른 데이터셋에서도 큰 차이 없이 적용될 수 있습니다. 즉, FLOPs는 주어진 모델의 구조에 따라 거의 고정적입니다.

  ❓ 5. 커널의 길이와 골격 모델에서의 의미
    커널은 일반적으로 합성곱 연산에서 사용하는 필터를 의미합니다. 커널의 크기와 가중치는 네트워크가 학습하는 중요한 파라미터 중 하나입니다.

    L x 1 Conv에서 L은 커널의 길이를 의미합니다. 예를 들어, L=3이면 3개의 값을 합성곱으로 처리한다는 뜻입니다.

    골격 기반 행동 인식에서의 커널은 관절 간의 공간적 관계를 학습하는 필터로 작동합니다. 커널은 각 관절이 다른 관절과 어떤 관계를 가지는지 학습하며, 그 크기와 길이는 학습하는 행동 패턴의 복잡도에 따라 다릅니다.
    L×1 Conv는 1D 합성곱을 의미하며, 시간적 차원에서 연산을 수행합니다.

    C_in, Cout는 각각 입력 채널 수와 출력 채널 수를 나타냅니다.

    /2는 합성곱 연산 후 출력 크기를 절반으로 줄이는 연산을 의미합니다.

  EfficientGCN에서는 SGLayer를 비롯한 다양한 레이어가 사용됩니다.
    모델의 구조에 따라 BasicLayer, BottleLayer, SepLayer, EpSepLayer, SGLayer 중에서 선택적으로 사용됩니다.
    SGLayer는 효율적인 공간적, 시간적 정보를 학습하는 레이어로, depth-wise convolution과 point-wise convolution이 결합된 구조를 가집니다.

  1. 골격 기반 행동 인식 모델에서 Depth-wise Convolution 사용 이유
    골격 기반 행동 인식 모델에서는 관절 위치 데이터(Joint), 속도 데이터(Velocity), 뼈 정보(Bone) 등이 채널로 들어갑니다. 이 각 채널은 행동 인식을 위한 서로 다른 특징을 나타냅니다.

    각 채널 간 상호작용이 상대적으로 적은 이유:
      관절 위치(Joint)는 각 관절의 3D 좌표입니다.
      속도(Velocity)는 시간에 따른 관절의 이동을 나타냅니다.
      뼈 정보(Bone)는 관절 간의 거리 및 각도를 나타냅니다.
      이러한 채널들은 서로 독립적인 정보를 제공하며, 관절의 물리적인 위치나 이동 속도, 관절 간의 관계를 개별적으로 학습하는 것이 더 중요합니다. 그래서 각 채널을 독립적으로 처리하는 Depth-wise Convolution이 유리할 수 있습니다. 채널 간의 상호작용이 필수적이지 않기 때문에, 연산량을 줄이면서도 각 채널의 중요한 정보를 학습할 수 있습니다.


  깊이(Depth) 해석
    **Depth(깊이)**는 일반적으로 네트워크의 레이어 수를 의미하지만, 구체적인 의미는 모델에 따라 다를 수 있습니다.
    골격 기반 행동 인식에서는 네트워크가 얼마나 깊이 있는 특징을 학습하는지를 나타냅니다. 더 많은 레이어를 쌓으면 모델이 더 복잡한 패턴을 학습할 수 있습니다.
    Depth는 보통 하이퍼파라미터로 설정되며, 학습 과정에서 최적의 레이어 수를 찾기 위해 조정됩니다. 보통 많은 Depth는 더 나은 성능을 제공하지만, 계산량이 증가하여 연산 시간이 길어집니다.

  내가 이해한 것을 검토해줘.
  (a) 에서는 Df*Df*Cin  (초기(?) 피쳐 맵이 각 채널 (R, G, B) 마다 있으므로) 입력 1과  Dk*Dk*Cout(필터 개수) 를  컨볼루션 해서 Df*Df*Cout (피쳐 맵이 필터 수마다 있으므로) 이 되는거야.
  (b) 는 애초에 정의가 필터 수를 고려하지 않고, 각 깊이 (채널) 별로 독립적으로 Df*Df 들에 대해 컨볼루션을 하므로 Dk*Dk*C_in (그래서 커널 입력에서 Cin 이 동일하다!)

  (c) 는 애초에 정의가 필터 수를 고려하여 합치는 연산이므로, 각 채널의 동일한 포인트 (1*1)에 대해 convolution 을 수행하고 Cout 필터 수만큼 또 반복한다. (추가 차원) 그래서 결과가 Df*Df*Cout 이 된다.

https://roboflow.com/model-task-type/object-detection?name=mdia

Mediapipe
  The input sizes for MediaPipe object detection models vary, with common sizes being 256x256, 320x320, and 448x448 pixel

Ascii Art -> sudo apt install -y jp2a 
Yolo11n ; https://github.com/ultralytics/ultralytics/blob/main/docs/en/tasks/detect.md
  6.5 GFLOPS
  
  YOLO11x 	640 	54.7 	462.8 ± 6.7 	11.3 ± 0.2 	56.9 	194.9

https://stackoverflow.com/questions/75691440/how-to-move-yolov8-model-onto-gpu
자동차의 ECO 장치.

HandLandmarker (full)
Inputs
  ● A video stream or an image of arbitrary size.
  Channels order: RGB with values in [0.0, 1.0]

Singleton PAttern, Observer pattern..
  Observer pattern 은 임베디드에서 자주 사용한다고 함.


TDD Cycle
  Fail - Pass - Refactoring
  Fail, Pass 두 개 모두 작성하기.
  Refactoring 은 동일한 테스트에 대해 다양한 입력에 대해 확장할 때 사용
  mock injection
BDD 는 아직 현업에서 인기를 끌지 못하고 있다고 한다. 📅 2024-10-07 11:52:50
pytest-mock
  various currencies.


YOLO11l	EfficientGCN-B4	INT8	4	20	681.7

https://en.wikipedia.org/wiki/Reinventing_the_wheel

🚨 (issue); %shell> git clone https://github.com/attention-eq-everything/effgcn_cam
  Cloning into 'effgcn_cam'...
  remote: Enumerating objects: 75, done.
  remote: Counting objects: 100% (75/75), done.
  remote: Compressing objects: 100% (49/49), done.
  error: RPC failed; curl 92 HTTP/2 stream 5 was not closed cleanly: CANCEL (err 8)
  # This indicates that the RPC (Remote Procedure Call) failed due to an issue with the curl library.
  # Specifically, HTTP/2 stream 5 was not properly closed, and the operation was canceled (CANCEL with error code 8).
  # This usually points to an unexpected interruption in the network connection.

  error: 6835 bytes of body are still expected
  # This means Git expected 6,835 more bytes of data, but it did not receive them from the remote server.
  # This often happens when the connection is dropped before all the data is transmitted.

  fetch-pack: unexpected disconnect while reading sideband packet
  # This indicates that Git encountered an unexpected disconnect while reading a sideband packet,
  # which is part of the data transmission process. It suggests that the connection was lost abruptly.

  fatal: early EOF
  # "EOF" stands for "End Of File". This error means that Git reached the end of the input stream unexpectedly.
  # The data transmission was cut off before all the expected data was received.

  fatal: fetch-pack: invalid index-pack output
  # This error occurs because the index-pack process received incomplete or corrupted data.
  # Git was unable to process and package the data correctly due to the premature disconnect.

  >>> solution: retry clone


요즘은 파인튜닝을 이용해서 학습이 되서, 많은 데이터가 필요하지 않을 수 있닥도 한다.


cd ~/repo/effgcn_cam

CVAT  ; https://app.cvat.ai/
양품 불량 구분



MSR Action3D Dataset
  https://uowmailedu-my.sharepoint.com/personal/wanqing_uow_edu_au/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fwanqing%5Fuow%5Fedu%5Fau%2FDocuments%2FResearchDatasets%2FMSRAction3D&ga=1


pytorch nn (neural network) 사용주인듯.

microsoft-edge --version


Kinetics (Kinetics Human Action Video Dataset) ; https://github.com/cvdfoundation/kinetics-dataset
  Kinetics paper ; https://paperswithcode.com/dataset/kinetics
  kinetics-400, kinetics-600, kinetics-700

  # Dataset Loaders
    https://pytorch.org/vision/stable/generated/torchvision.datasets.Kinetics.html

https://fishshell.com/docs/current/cmds/and.html

📰 Datumaro (Dataset Management Framework)
  Dataset
    Train, vAl, Test 
      test set.. 지표..


nturgbd_skeletons_s001_to_s017.zip ; https://drive.google.com/open?id=1CUZnBtYwifVXS21yVg62T-vrPVayso5H
nturgbd_skeletons_s018_to_s032.zip ; https://drive.google.com/file/d/1tEbuaEqMxAV7dNc4fqu1O4M7mC6CJ50w/view

scp 사용하기.
  🛍️ e.g. 
  scp -r ./cvat/ gabriely@61.108.166.16:/home/gabriely/tests
  https://pypi.org/project/datum/
OTX example
  Build task
    Find template what we want to train
      $ otx find --template --task DETECTION
    Build task to create model
      Example)
      $ otx build Object_Detection_YOLO_X \
        --train-data-roots ../datumaro/export-coco/ \
        --val-data-roots ../datumaro/export-coco/
  Train
    Start training with parameters
      otx train params --learning_parameters.num_iter 8 \
        --learning_parameters.batch_size 1
      HPO is optional. (Hyperparameter optimization)
      # otx train --enable-hpo
      torch vision
    Start training with configuration files, after updating configuration
      data.yaml
      configuration.yaml
      po_config.yaml
  Evaoluation
  Export & Optimize
    Exports a trained model to the OpenVINO to run it on Intel hardware.
    Optimizes model using NNCF/POT depending on the model format.
  Deploy
    Create openvinio.zip with a demo appliation and exported model.

  otx 설치 된다음에도 오류가 뜨면 mmcv full 재설치?

⚓ Action Recognition Datasets: "NTU RGB+D" Dataset and "NTU RGB+D 120" Dataset ; https://rose1.ntu.edu.sg/dataset/actionRecognition/
https://rose1.ntu.edu.sg/challenge/ActionRecognitionChallenge/
  .skeleton 파일 구조 ; https://github.com/shahroudy/NTURGB-D/tree/master/Python
  https://github.com/shahroudy/NTURGB-D/tree/master

우분투 클립보드


EfficientGCN, SGN 은 모두 .skeleton 데이터를 가지고 학습시켰다고 했잖아. 맞아?
근데 보통 일반 카메라에서는 사진 프레임만 주잔아. 그럴 경우 어떻게 추론하지? 나는 mediapipe pose, mediapipe hand landmarks 두 개의 모델에서 나온 output 을 EfficientGCN 의 입력으로 사용하고싶어.

그리고 새로운 동작을 EfficientGCN 에서 학습시키려면 어떻게 해야하지? 이 때도 마찬가지로 사진 프레임만 있잖아. (Fine-tuning 이라고 하나?)
  그리고 예를들어서 .h5 데이터셋에 해당 프레임이 어떤 동작인지에 대해 라벨링 되어있는 데이터셋의 경우는 또 어떻게해?


1. Mediapipe Pose detection, Mediapipe Hand landmark detection 으로부터 탐지한 

https://github.com/open-mmlab/mmaction2

https://github.com/MVIG-SJTU/AlphaPose

poseconv3d FLOPS https://ar5iv.labs.arxiv.org/html/2104.13586
  Table 11: PoseConv3D instantiated with: C3D, X3D, SlowOnly. 
👍 ⚓ posec3d ; https://arxiv.org/abs/2104.13586
  https://arxiv.org/pdf/2104.13586
https://github.com/kennymckormick/pyskl/tree/main/configs/posec3d
https://mmaction2.readthedocs.io/en/latest/model_zoo/skeleton.html#posec3d
https://github.com/open-mmlab/mmaction2/tree/main/configs/skeleton/posec3d

https://github.com/kennymckormick/pyskl/tree/main/configs/posec3d

>> stm32 f103rb
https://github.com/kennymckormick/pyskl/tree/main?tab=readme-ov-file
⚓ SOTA (State of the Art) ; ...
  특정 분야나 문제에 대한 최고 수준의 성능을 달성한 모델 또는 기술
Customizing  ; https://github.com/open-mmlab/mmaction2/blob/main/configs/skeleton/posec3d/custom_dataset_training.md
  https://github.com/kennymckormick/pyskl/blob/main/configs/posec3d/x3d_shallow_ntu60_xsub/joint.py
    model = dict(
        type='Recognizer3D',
        backbone=dict(
            type='X3D',
            gamma_d=1,
            in_channels=17,
            base_channels=24,
            num_stages=3,
            se_ratio=None,
            use_swish=False,
            stage_blocks=(2, 5, 3),
            spatial_strides=(2, 2, 2)),
        cls_head=dict(
            type='I3DHead',
            in_channels=216,
            num_classes=60,
            dropout=0.5),
        test_cfg=dict(average_clips='prob'))

  joint limb

https://github.com/kennymckormick/pyskl/tree/main
https://github.com/kennymckormick/pyskl/blob/main/configs/posec3d/x3d_shallow_ntu60_xsub/limb.py


>>>>>>>>>>>> in mmaction2/
  poetry init


poetry lock --no-update
poetry show --tree
poetry add mmengine openmim "mmcv<2.2.0"

mim download mmaction2 --config tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb --dest .
# The demo.mp4 and label_map_k400.txt are both from Kinetics-400
python demo/demo.py tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb.py \
    tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb_20220906-2692d16c.pth \
    demo/demo.mp4 tools/data/kinetics/label_map_k400.txt
https://github.com/open-mmlab/mmaction2
  ⚠️ poetry add mmengine openmim
    ... mmaciton docs 에는 없었는데.. github 를 우선적으로 보도록 하자. 문서 갱신이 안되는듯 보임.


set DISPLAY (tailscale status | grep "active; direct" | awk '{print $1}'):0


iftop

⚓ Docker
  Windows - Docker - Settings - Resources - WSL Integration
    Resources WSL Integration
      - ✔️ Enable integration with my default WSL distro
      - ✔️ Ubuntu-24.04
  "NVIDIA Container Toolkit makes host OS GPU drivers accessible to containers, allowing them to run different CUDA versions while ensuring compatibility and GPU acceleration.
  >Dev Containers: New Dev Container...
    https://containers.dev/templates
  # Ubuntu에서 Docker 그룹에 추가된 사용자 목록을 확인
    getent group docker


  Ubuntu..
    https://docs.docker.com/engine/install/linux-postinstall/
    sudo usermod -aG docker $USER

1. MMACtion2 (toolbox)
  - Posec3D 모델 사용 예정
    Backbone X3D s (FLOPS: 0.6G) ; https://arxiv.org/pdf/2104.13586
    이를 사용하기 위한 방법 찾야아 함.
  - 파일로 카메라 실시간 프레임으로 테스트
  - 포즈 모델을 따로 써야하는지?

  
2. Docker
  >> [O] 컨테이너 내에서 호스트의 GPU 사용하는 방법
  >> 카메라 장치 권한 허용하는 방법 찾아야 함.
  >> *volume mount 방식 export/import 또는 SCP 로 호스트에 백업하는 방법 찾야아 함.


>> 면접: bitbake, yocto bblayer (BitBake layer)
👍 
⚓ mpv ; https://en.wikipedia.org/wiki/Mpv_(media_player)
  ⚓ Keyboard shortcut ; https://mpv.io/manual/master/
    # Keyboard Control
    # [ and ]
      Decrease/increase current playback speed by 10%.
    # l
      Set/clear A-B loop points. See ab-loop command for details.


⚓ CCID (chip card interface device) ; https://en.wikipedia.org/wiki/CCID_(protocol)

sudo apt install libnvidia-encode1

🚨 (issue); mpv browse_out/*.mp4
  [vaapi] libva: vaGetDriverNames() failed with unknown libva error
  ⚠️⭕ reqiuired.. 📅 2024-10-10 13:04:51
  # to use Hardware acceleration
    sudo apt install nvidia-cuda-toolkit
    mpv --hwdec=vdpau browse_out/*.mp4

    🆚 VDPAU, VA-API, NVDEC
      VDPAU (Video Decode and Presentation API for Unix):
        Developed by NVIDIA for video decoding on NVIDIA GPUs.
        Optimized for NVIDIA hardware.

      VA-API (Video Acceleration API):
        Developed by Intel, mainly for Intel integrated GPUs and some AMD GPUs.
        Limited compatibility with NVIDIA GPUs.

      NVDEC:
        NVIDIA-specific API for hardware decoding on modern NVIDIA GPUs.
        Operates through CUDA and works with NVIDIA's official drivers and SDK.

    
🆚💡 Hypervisor vs WSL2
  - Hardware-level virtualization
  - Creates and runs virtual machines
  - Type1: Native/Bare-metal.
  - Type2: (VMWare, QEMU, ...) ?
  BootLoader 가 하드웨어를 운영체제가 쓸 수 있도록 초기화를 한다. 부팅 시간이 필요하다는 것이 단점. 30~60초 필요.
  좋지 않다는 것은 아니다. Windows 도 이것 기반으로 돌아가고 있음.
  >> 고급 차량에는 Hypervisor 에 여러 OS 를 사용하기 위해 사용된다고도 함.
    계기판. 디지털 클러스터.
    ⭕ 트렌드 ㅇㅇ. 하이퍼바이저가 지원이 되어야 함. RTOS (하드 리얼타임 처리용), OS (그래픽 처리용), firmare ...
  베어메탈 OS 는 처리하기 어렵고
  RTOS: Hard-real-time
    vs 일반 OS: Soft-real-time
  
  인테로 마찬가지로 x86..에서도 그럼. 

Container
  - OS-level virtualization.
  - isolate app from its environment
  - 운영체제가 공유되고 있다.



>> 그러니까 고성능이 필요한 시스템에서는
도커가 리눅스/유닉스에서 쓰고 
  .. 도커가 대기업에서는 안쓰인다. 유료. containerd

  Container Management 🔪 Podman
  Image Build 🔪 Kaniko   --> Buildsh 로 변경?..
  Image Management 🔪 Skopeo
  Kubernetes Application Management 🔪 Helm

lab 실에서 spin-out
  도구 중 어떤 것이 더 보안성이 높은지는 사용 환경에 따라 달라집니다. 클라우드 및 Kubernetes 환경에서는 Kaniko, 로컬 및 레드햇 기반 환경에서는 Buildah가 더 보안성이 높다고 할 수 있습니다.


리얼 C 인포테인먼트??? 
Key/VAlue
  ETCD
    Distributed Key/alue Store
    Like a Directiory Tree
    JSON/RST API
  Memachced, Redis.
mqtt  vs kafka? ZeroMQ vs 
Image Build 🔪 Kaniko   --> Buildsh 로 변경?..


⌨️ 웹 페이지 로컬 캐시 클리어: Ctrl + Shift + R: 현재 페이지를 강제로 새로고침(캐시 무시)합니다.


IPC (Inter process communication)
  pipe1 pipe2
  messge queue
  shared memory

  tcp/ip // 의견에 따라 달림. IPC에 넣을지 안넣을지.


mqtt  vs kafka

dockerc (도커 회사가 만든...) vs docker.io (데비안이 만든.. 도커의 100% 기능을 쓰지 못할 수 잇다.)

LaTeX
  BibTeX ; BibTeX는 LaTeX와 함께 사용하는 참고문헌 관리 도구입니다. ; https://en.wikipedia.org/wiki/BibTeX

# Modify the Config

  ⭕ Recommend ...

  # Modify Dataset

  # Modify Runtime Config

  # Modify Model Config
Files...
  configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py
  configs/_base_/schedules/sgd_100e.py

🛍️ e.g. tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb
  wget https://download.openmmlab.com/mmaction/kinetics400_tiny.zip
  mkdir -p data/
  unzip kinetics400_tiny.zip -d data/

  python tools/visualizations/browse_dataset.py \
      configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py \
      browse_out --mode pipeline

  configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py
    dataset_type = "VideoDataset"
    data_root = "data/kinetics400_tiny/train"
    data_root_val = "data/kinetics400_tiny/val"
    ann_file_train = "data/kinetics400_tiny/kinetics_tiny_train_video.txt"
    ann_file_val = "data/kinetics400_tiny/kinetics_tiny_val_video.txt"
    // requirees moviepy 


  python tools/train.py configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py

  python tools/test.py configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py \
      work_dirs/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb/best_acc_top1_epoch_5.pth


  #🚣 Model 종류 확인
    %shell> tree -L 2 configs
    # >> configs/skeleton/posec3d ; https://github.com/open-mmlab/mmaction2/tree/main/configs/skeleton/posec3d

🛍️ e.g. Skeleton-based
  1. Preparing Skeleton Dataset ; https://github.com/open-mmlab/mmaction2/tree/main/tools/data/skeleton
python tools/train.py ${CONFIG_FILE} [optional arguments]

  wget https://download.openmmlab.com/mmaction/kinetics400_tiny.zip
  mkdir -p data/
  unzip kinetics400_tiny.zip -d data/


PoseC3D ; https://openaccess.thecvf.com/content/CVPR2022/papers/Duan_Revisiting_Skeleton-Based_Action_Recognition_CVPR_2022_paper.pdf
  Figure 1. PoseConv3D takes 2D poses as inputs. In general, 2D poses are of better quality than 3D poses. We visualize 2D posesestimated with HRNet for videos in NTU-60 and FineGYM in (a).
  Apparently, their quality is much better than 3D poses collected by sensors (b) or estimated with state-of-the-art estimators (c).
    📝 Noise and Artifacts in 3D Poses: 3D pose estimation, whether collected by depth sensors (like Kinect) or estimated using state-of-the-art methods (like VIBE)
      , can introduce noise and inaccuracies due to hardware limitations or model errors, especially in real-world conditions.

  Scalability: It efficiently handles scenarios involving multiple persons without increased computational costs, unlike GCNs, which scale linearly with the number of persons.
    because of PoseConv3D use 2D Hitmap volume...?


모든 행동인식 모델은 연속적인 프레임에 대해 판단해야 하는데, 동일한 사람인지 어떻게 추적하고 이를 입력으로 넣어서 판단하는지.. 사람 Tacking 알고리즘 필요함. pose data?

Yolo Deep SORT ? 트래킹 알고리즘?

1. 3D 골격 기반 GCN 모델에 대한 의견
  단점
    높은 비용 및 복잡성:
        3D 골격 기반 시스템을 구축하려면 전용 하드웨어가 필요하며, 이 하드웨어는 상대적으로 높은 비용과 설치 복잡성을 동반합니다.
        실내와 같이 제어된 환경에서는 높은 성능을 발휘할 수 있지만, 야외 환경이나 대규모 설치에서는 비용과 유지보수 측면에서 부담이 될 수 있습니다.
1. 3D 센서의 거리 한계
  Microsoft Kinect와 같은 일반적인 3D 센서:
      보통 0.5m에서 4.5m 사이에서 최적의 성능을 발휘합니다. 이 범위를 넘어가면, 깊이 정보의 정확도가 떨어지거나 탐지가 어려워질 수 있습니다.
  Intel RealSense:
      모델에 따라 조금씩 다르지만, 최대 탐지 거리는 보통 4m 전후입니다. RealSense의 고급 모델(예: D455)은 이보다 조금 더 멀리 탐지할 수 있지만, 여전히 제한이 있습니다.
  Time-of-Flight(TOF) 카메라:
      대부분의 TOF 카메라는 일반적으로 10m 이하에서 작동하며, 이보다 먼 거리에서는 신호의 정확도가 떨어질 수 있습니다.

📰 멀티뷰 2D 카메라 ??
  3D 기반 솔루션은 정확성과 정밀한 데이터 분석이 필요한 경우에 선호됩니다. 예를 들어:
      스포츠 분석: 선수의 자세나 동작을 정밀하게 추적해야 할 때, 3D 데이터가 유리합니다.
      의료 및 재활: 환자의 움직임을 정확히 측정하고 분석해야 할 때, 3D 솔루션은 필수적입니다.
      로봇 공학: 자율 주행 로봇이나 로봇 팔과 같은 시스템에서 정밀한 동작을 인식하는 데 3D 정보는 매우 중요합니다.

  2D 기반 솔루션은 낮은 비용과 대규모 적용이 필요한 경우에 적합합니다. 예를 들어:
      스마트 시티 및 교통 관리: 대규모의 인구 밀집 지역에서 사람의 움직임을 실시간으로 모니터링하고 관리하는 데 2D 기반 솔루션은 매우 효율적입니다.
      소매 및 상업 시스템: 쇼핑몰이나 상점에서 고객의 행동을 분석하는 데도 저비용으로 접근 가능한 2D 시스템이 주로 사용됩니다.
      모바일 장치 및 IoT 시스템: 리소스가 제한된 장치에서 실시간으로 인식할 때는 가벼운 2D 모델이 더 유리합니다.

  
  
Monodeapth 로 측정
  Depth anything ; https://depth-anything.github.io/
    물체 사이에 절대적인 거리를 알고 있다는 가정하여 어느정도 절대거리를 예측할 수는 있다고 한다?
https://github.com/kccistc/openvino/tree/main/Tutorials/depth_anything

Despite the considerable improvements, the settings of
different GCN approaches do not align well.










===
Global Motion Compensation (GMC) is a technique used to compensate for the motion of the camera or the entire scene in tracking algorithms. It helps in isolating the motion of tracked objects from global motion, such as camera panning, tilting, or zooming, which could otherwise distort the motion patterns of the objects in the scene.

Dynamic device management
⚓ udev ; https://man7.org/linux/man-pages/man7/udev.7.html
  #📎 DESCRIPTION 📅 2024-10-11 15:09:24
    The kernel usually just assigns unpredictable device names based on the order of discovery.
    Meaningful symlinks or network device names provide a way to reliably identify devices based on their properties or current configuration.

    🪱 The udev daemon, systemd-udevd.service(8), ...  When udev receives a device event, it matches its configured set of rules against various device attributes to identify the device.
    🪱 udev database

  # Keys
    # ATTRS{filename}

    The ENV, GROUP, MODE, NAME, OWNER, PROGRAM, RUN, SECLABEL, and SYMLINK fields support simple string substitutions. 
⚓ udevadm ; https://man7.org/linux/man-pages/man8/udevadm.8.html
  #🚣 Table 1. udevadm info output prefixes

아래꺼 curl -fsSL ~ 부분 자동화 스크립트 작성 팔요 ===
✅ (how); settings for Embedded development 📅 2024-10-11 14:17:51
  - install PlatformIO Extension in VSCode
  - install udev ; https://docs.platformio.org/en/latest/core/installation/udev-rules.html
    | https://en.wikipedia.org/wiki/Udev
    🚣 Linux users have to install udev rules for PlatformIO supported boards/devices.
    %shell>
      # udb: user device or userspace/dev
      # https://raw.githubusercontent.com/platformio/platformio-core/develop/platformio/assets/system/99-platformio-udev.rules
      curl -fsSL https://raw.githubusercontent.com/platformio/platformio-core/develop/platformio/assets/system/99-platformio-udev.rules | sudo tee /etc/udev/rules.d/99-platformio-udev.rules
      # udevadm: udev administator
      sudo udevadm control --reload-rules
      sudo udevadm trigger
  - %vscode> PlatformIO: PlatformIO Home
    Quick Access - New Project
      - Project Wizard
        - Name: signal-master
        - Board: Arduino Uno
        - Framework: Arudino
        - Location: <project_root>
  ##  https://docs.platformio.org/en/latest/tutorials/espressif32/arduino_debugging_unit_testing.html ~
  # Compiling and Uploading the Firmware: Compile, Upload, Minitor
  # 📰 Adding Bluetooth LE features

  - check conneted device
    %shell> udevadm info --query=property --name=/dev/ttyACM0 | grep --extended-regexp 'ID_VENDOR_FROM_DATABASE|ID_MODEL_FROM_DATABASE'



%shell>
  lsusb
  dmesg | grep ttyACM

Debugging the Firmware
Setting Up the Hardware
Writing Unit Tests
Adding Bluetooth LE features
Conclusion

  

ESP32는 Espressif Systems에서 개발한 마이크로컨트롤러 칩
JTAG(Joint Test Action Group)는 마이크로컨트롤러나 칩의 내부를 디버깅할 수 있게 해주는 디버깅 인터페이스
Arduino Uno R3의 경우, 보통 JTAG 같은 디버깅 인터페이스를 사용하지 않고, 시리얼 모니터를 통해 코드 실행 결과를 확인하거나 오류를 디버깅합니다. 더 정밀한 디버깅이 필요하다면, 아두이노에 호환되는 디버깅 툴이나 업그레이드된 보드(예: Arduino Zero)를 고려할 수 있습니다.

/dev/ttyACM0는 Linux에서 ACM (Abstract Control Model) 장치로 등록된 USB 시리얼 포트
  ACM은 USB CDC (Communications Device Class) 표준의 일부로, 모뎀이나 시리얼 통신 장치를 지원하기 위해 사용됨.
tty (teletypewriter): 시리얼 포트나 가상 터미널 등을 의미.


Bluetooth module HC-06 FC-114




Command Query Responsibility Segregation 
CQRS, EDA 패턴

Event-Sourcing 패턴
📍📍📍 Design pattern
  https://learn.microsoft.com/en-us/dotnet/architecture/microservices/microservice-ddd-cqrs-patterns/eshoponcontainers-cqrs-ddd-microservice
    📍 It's important to understand that CQRS and most DDD patterns (like DDD layers or a domain model with aggregates) are not architectural styles, but only architecture patterns. 
    Microservices, SOA, and event-driven architecture (EDA) are examples of architectural styles.
    ... [all]
    Different Bounded Contexts (BCs) will employ different patterns. They have different responsibilities, and that leads to different solutions. 
    ... There is only one application architecture: ...


https://developer-wh.tistory.com/entry/%EA%BC%BC%EA%BC%BC%ED%95%98%EA%B2%8C-%EB%85%BC%EB%AC%B8%EC%9D%BD%EA%B8%B0-PYSKL-Towards-Good-Practices-for-Skeleton-Action-Recognition-2022



➡️ TODO >>>>>>>>>>>>>> ; https://code.visualstudio.com/remote/advancedcontainers/develop-remote-host



임베딩은 객체의 시각적 특징을 수치로 변환한 벡터입니다. 고차원의 정보를 저차원 벡터로 변환한다는 점.



https://docs.ultralytics.com/guides/nvidia-jetson/#quick-start-with-docker



$RANDOM은 bash에서 무작위 숫자를 생성하는 예약된 변수입니다. 이를 이용해 12000부터 31999까지의 무작위 포트를 설정합니다.
$MASTER_PORT는 PyTorch 분산 학습에서 노드 간 통신에 사용되는 포트를 지정하는 환경 변수입니다
set -x는 스크립트 실행 시 디버깅 모드를 활성화합니다.
MKL_SERVICE_FORCE_INTEL=1는 MKL이 비 Intel CPU에서도 작동하도록 강제로 설정하는 환경 변수.


>>> KeyPoints ; 25 for NTURGB+D 3D skeleton, 17 for CoCo, 18 for OpenPose, etc. )




To update the inference pipeline and ensure it aligns with your training configuration, I'll use the configuration you've provided and match the inference process for YOLO Pose with the structure you trained PoseC3D on. The key points are:
  Ensure Consistent Input: PoseC3D was trained with 48-frame clips and expects inputs of shape (N, C, T, H, W), where N is batch size, C is channel (17 keypoints), T is the number of frames (48), H and W are height and width.
  Keypoint Preprocessing: During training, the keypoints were processed as (x, y) coordinates, so the inference needs to pass keypoints in the same format.
  Batch Size Alignment: During training, a batch size of 1 was used, so we will ensure the same during inference.

  

프론트에서 가장 한한 것은 웹 어셈블러.
  그래서 Rust 가 더 뜬다고 한다.
  프론트엔드 코어엔지니어들의 한핫 주제라고 한다. ㅎ
  자바스크립트 언어 자체의 한계인 싱글 스레드의 한계를 해결하기 위함
  "Web Accember" github 확인.
  원래는 하드웨어별로 어셈블러를 이해하고 있어야 했는데, Rust가 이를 해결해준다고 한다. 그래서 뜬다고 함. 상당부분 해결을 해줘서.

백엔드에서는
  전세계적으로는 node 가 가장 유명하다고 한다. 우리나라만 자바에 몰려있다고 함.
  자바스크립트/파이썬/자바..
  모바일 앱 프로그래밍이 트렌드가 되었을 때 자바 시장이 되게 커져서그렇다고 함.
  자바스크립트 엔진: V8 엔진... 노드JS 가 각광을 받은 이유.. 이거도 싱글 스레드의 한계가 있다고 함. 

언어는 크게 Memory-Unmanaged 와 Memory-Manged 로 나뉜다.

인터럽트 펜딩.
pid 정밀제어 알고리즘?
🪱 Tensor. 
  In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space. Tensors may map between different objects such as vectors, scalars, and even other tensors.


이재성 강사님. 자율주행 코드
  https://gitlab.com/MAZE-dankook/self-driving-patrol-car
  

⚓ torch.permute ; https://pytorch.org/docs/stable/generated/torch.permute.html
  Returns a view of the original tensor input with its dimensions permuted.
⚓ torch.stack ; https://pytorch.org/docs/stable/generated/torch.stack.html#torch-stack

모니터 절전 모드에서 발생하는 Ubuntu 시스템 오류 해결방법?
  # Disable automatic screen lock when the screen is idle
  # This prevents the screen from locking after a certain period of inactivity.
  gsettings set org.gnome.desktop.screensaver lock-enabled false

  # Disable the monitor power saving mode by setting the idle delay to 0 seconds
  # This stops the monitor from going into power saving mode.
  gsettings set org.gnome.desktop.session idle-delay 0

  # Disable dimming the screen when the system is idle
  # By setting idle-dim to false, the screen will not dim after being inactive.
  gsettings set org.gnome.settings-daemon.plugins.power idle-dim false

  # Disable automatic suspend when the system is plugged into AC power
  # This prevents the system from going into suspend mode when connected to a power source.
  gsettings set org.gnome.settings-daemon.plugins.power sleep-inactive-ac-type 'nothing'

  # Enable automatic suspend when the system is running on battery power
  # This will put the system into suspend mode after a period of inactivity on battery.
  gsettings set org.gnome.settings-daemon.plugins.power sleep-inactive-battery-type 'suspend'

  gsettings set org.gnome.desktop.screensaver lock-enabled true

  gsettings set org.gnome.settings-daemon.plugins.power sleep-inactive-ac-type 'nothing'
  gsettings set org.gnome.settings-daemon.plugins.power sleep-inactive-battery-type 'nothing'
  gsettings set org.gnome.settings-daemon.plugins.power idle-dim false
  gsettings set org.gnome.desktop.session idle-delay 0
  gsettings set org.gnome.desktop.screensaver lock-enabled true


YOLO11n-pose	640	50.0	81.0	52.4 ± 0.5	1.7 ± 0.0	2.9	7.6



쿠팡 - 롯데제과 제휴.
  https://shop.coupang.com/A00148950?source=brandstore_sdp_atf_topbadge&pid=8204987238&viid=85321286829&platform=p&locale=ko_KR
쿠팡 - 오뚜기 제휴..
  https://shop.coupang.com/ottogi?source=brandstore_sdp_atf_topbadge&pid=1083072363&viid=86261967613&platform=p&locale=ko_KR
  https://www.coupang.com/vp/products/1083072363?itemId=2036340586&vendorItemId=86261967613&pickType=COU_PICK&q=%EB%BF%8C%EC%85%94%EB%BF%8C%EC%85%94&itemsCount=36&searchId=9d58e51ff00e41eeaaf4087563489045&rank=0&isAddedCart=




This container can be used to run an application or to separate tools, libraries, or runtimes needed for working with a codebase.
?? 라이브러리..?어떻게?

Workspace files are mounted from the local file system or copied or cloned into the container
??copy clone 차이

settings, tools, and configurations  차이

Other glibc based Linux containers ??

ㅡㅡ working with git?

Picking your quick start
???

Human Activity Recognition

>>>>>>>>>>>>> TODO: VSCode crawling




Forwarded Ports view.

🧮 %vscode>Remote-SSH: Connect to Host...두데
  명령어 아이콘. :abacus:
  📄 ~/.vscode-server/data/Machine
    파일 아이콘 :paper
  📁 파일 폴더 아이콘. :file_folder

📍 면접 상담 📅 2024-10-16 14:53:59
  면접관이 관심 있는 것을 눈치있게.. 그 것을 얘기해봐야 한다.
    그것을 해보았기 때문에, ~를 할 줄 한다. ~에 기여할 수 있을 것 같다. 적용할 수 있을 것 같다.
  예를들어 공정 불량률..
    그래서 딥러닝 이런것을 해보았다.
  

  코덱 알고리즘 개발하는 곳에서 ROS ..이런 걸 강조해봤자 의미 없다.
  자기소개서를 똑같은것을 절대 쓰면 안될듯 한다.
  말로 풀지 말고 그림이나 포트폴리오로 .. 이해할 수 있게. 영상. 등등으로.

  컴퓨터가 느려서 지속적으로 클레임을 했지만 받아들여지지 않아서 나왔지만, 면접관은 관심이 없다. 관심을 가져야 할 이유도 없고. 수습기간은 쓰지도 않도록 한다.
  그냥 당시 AI 가 뜨고 있었고 웹 전망에 대해 .. 어쩌구..
  그래서 나와서 토이 프로젝트를 하면서 
  ..건설 현장...  만약 다시 돌아간다면 그러지 않을 것입니다. 
  
  이후 그러한 개발 환경에 따라, 원격 개발 환경에 대해서도 모색했고, tailscale.. 어쩌구.. remote-ssh, x11 idsplay 설정.. 

  --- 중요한 것
    현업을 할 수 있는 능력을 갖추었느냐
    구조화
    !!! 재현성. reproducible
📍 면접..
  LLM training.. 경험.. sepc..
  bragging..
여행계획 참고: 일본, 오사카, 유티버셜 스튜디오, 교토
  아랴사마 중림? 교토.. 
  유니버셜 스튜디오 재팬(USJ)
  현금 대체수단 구비
  숙소는 웬만하면 한군대.. 최대 두군대가 나음 단기 잊렁에서는

  도쿄 시즈오카, 도쿄. 쇼핑과 덕질. 

  >>> 👍 발표할 때 시선 처리
    가만히 서서 ppt 를 가리키는거보다 움직이면서 하는 것이 더 낫다.
    눈치컷 나한테 우호적인 사람에게 시선처리.
  📍 지원동기보다는, 이사람이 회사에 무엇을 기여할수 있는지를 써야 한다. 사용한 기술 세트 말고.
    쓸거면 정확히 어느 레벨로 하는지 명시해줘야 한다.
📍 presentation skills for persuasive vs informative vs ...
📍 짐 켈러. 최고의 반도체 아키텍처 설계자
  risk b
  젠슨 황
  샘 올트만 OpenAI CEO
  논문 디펜스.
삼성...
  HBM 문제..
  파이낸스가 삼성을 먹어버려서.. ... 개발자들이 잘 안된다고 한다.
  HBM 에 투자가 매출이 DRAM 에서 일어나는 구조적 문제 + 삼성의 구조적 문화
  그래서 내 ~것을 바탕으로 HBM 프로젝트에 가속.. 
  문제가 생기면 끝까지 조사해서 해결하려 하고, 제안하는 그런 모습을 보여주겟다.

  📄 CVPR : CV에선 최고의 컨퍼런스. ㅇ
  📄 CGRAB
역기구학 전기구학?
👍 Canva 보다는 Gamma. 
공급피더

면접: forward" or "publish" 
  https://code.visualstudio.com/docs/devcontainers/containers#_forwarding-or-publishing-a-port
    - Neither specify EXPOSE nor -p
    - Only specify EXPOSE
    - Specify EXPOSE and -p
    - Only specify -p which implicitly does EXPOSE
면접: 모듈 vs 패키지
  ?
AllowStreamLocalForwarding ; https://man7.org/linux/man-pages/man5/sshd_config.5.html
  ⚖️ The available options are yes (the default)
👍 man page
  man(1)
    ⚓ groups ; https://man7.org/linux/man-pages/man1/groups.1.html
    ⚓🚣 getent ; https://man7.org/linux/man-pages/man1/getent.1.html
      get entries from Name Service Switch libraries
      # group
        

  GNU Coreutils ; https://www.gnu.org/software/coreutils/manual/coreutils.html#groups-invocation
    20 User information
      20.1 id: Print user identity
      20.2 logname: Print current login name
      20.3 whoami: Print effective user name
      20.4 groups: Print group names a user is in

      20.5 users: Print login names of users currently logged in
      20.6 who: Print who is currently logged in
      20.7 pinky: Print information about users

>> "호스트"는 네트워크에서 서비스를 제공하는 주체. ssh host 면 서버.

이 화면은 선택적으로 추가할 수 있는 파일이나 디렉터리를 묻는 옵션 창입니다. 여기에서 보여지는 파일은 .github/dependabot.yml입니다.

dependabot.yml 파일은 GitHub의 Dependabot을 설정하는 파일입니다. Dependabot은 프로젝트의 의존성(dependency) 업데이트를 자동으로 확인하고, 새 버전이 있을 때 PR(Pull Request)을 생성해주는 기능을 제공합니다. 주로 프로젝트에서 사용하는 라이브러리나 패키지들이 최신 상태로 유지되도록 도와줍니다.

Optional Files/Directories - .github/dependabot.yml
poetry3-poetry-pyenv
⚓ opt out ; https://en.wiktionary.org/wiki/opt_out

v4l2-ctl --list-formats-ext
v4l2-ctl -d /dev/video0 --all
v4l2-ctl -d /dev/video1 --all # metadata 가 포함되어있음.? 카메라마다 다른듯?
USB Webcam Appears as 2 Devices  문제..
  웹캠이 두 개의 스트림(예: 일반 영상 + 마이크 스트림 또는 두 가지 해상도 옵션)으로 인식될 때 발생

xhost +local:docker
❓ https://huggingface.co/



Devcontainer... 
Step 11/33 : RUN echo "done 0"     && curl https://pyenv.run | bash     && echo "done 1"     && pyenv install ${PYTHON_VERSION}     && echo "done 2"     && pyenv global ${PYTHON_VERSION}     && echo "done 3"     && curl -sSL https://install.python-poetry.org | python3 -     && poetry config virtualenvs.in-project true
 ---> Running in fbeaa359e5e4
unable to find user vscode: no matching entries in passwd file
[2024-10-17T04:31:47.237Z] unable to find user vscode: no matching entries in passwd file
Most NVIDIA Jetson containers run with root privileges by default. If you require non-root users (like vscode), you’ll need to create and configure them manually.

echo $XDG_SESSION_TYPE

https://www.canva.com/

lsusb -v -d 0bda:5411

💡 torch.Tensor**는 PyTorch의 데이터 구조이며, NumPy 배열과는 다른 메모리 관리 방식을 사용합니다.

https://code.visualstudio.com/docs/python/linting
  Linter.. https://marketplace.visualstudio.com/items?itemName=ms-python.mypy-type-checker
  prototype 엔 ㅓㅎ기?


Canva 기초배우기:
  🖇️ 기초 배우기 ; https://www.canva.com/design/DAGT4a4Zfxo/pUXsYyUcLJQRvpweu6g0Lw/edit 📅 2024-10-18 13:12:41
  🖇️ 프레젠테이션 제작 ; https://www.canva.com/design/DAGEJYzMKzk/qv9ABywF5dRLHDJXwO8zjA/remix?title=%ED%94%84%EB%A0%88%EC%A0%A0%ED%85%8C%EC%9D%B4%EC%85%98%20%EC%A0%9C%EC%9E%91 📅 2024-10-18 13:12:41
  소셜 미디어 학습 ; https://www.canva.com/design/DAGT5HklhSQ/8QN3wb3AELtc84k40Gq2cw/edit

양자역학 7 - 제 5차 솔베이 회의(물리학 학회)

PPT 간단 요약
  https://www.canva.com/design/DAGT5Pjd7-s/8_vLHAy9aLdXRKOpcEX3OA/edit


층간소음 -> 이웃사이센터
  https://www.noiseinfo.or.kr/floorinfo/consultrequest.do
  공동주택: 층간소음관뤼원회 구성 의무화 (2024.10.25)

  

calibration은 모델의 output prediction이 실제 확률값과 일치하도록 교정하는 작업을
There are two main uses of the term calibration in statistics that denote special types of statistical inference problems. Calibration can mean
⚓🪱 Calibration (statistics) ; https://en.wikipedia.org/wiki/Calibration_(statistics)
  a reverse process to regression, where instead of a future dependent variable being predicted from known explanatory variables, a known observation of the dependent variables is used to predict a corresponding explanatory variable;[1


1. prctl(PR_SVE_GET_VL) 오류 원인
prctl은 리눅스에서 프로세스의 속성을 제어하는 시스템 호출입니다.
이 오류는 PyTorch가 Jetson Nano와 같은 ARM 시스템의 CPU를 감지하려고 할 때 발생합니다.
**SVE(Scalable Vector Extension)**는 ARM의 특정 확장 기능인데, Jetson Nano에서는 이 기능이 지원되지 않기 때문에 prctl 호출에 실패합니다.



# https://github.com/ultralytics/ultralytics/issues/2964

# PyTorch 2.x 버전은 CUDA 11.x 이상과 호환 ...
# PyPI의 오래된 버전 제공 중단...: PyTorch의 이전 버전들은 더 이상 PyPI에서 제공되지 않을 수 있습니다...

in Jetson nano
  # already installed
  sudo apt-get install nvidia-container-runtime
  # already configured
  code /etc/docker/daemon.json
  {
      "runtimes": {
          "nvidia": {
              "path": "nvidia-container-runtime",
              "runtimeArgs": []
          }
      }
  }

  https://github.com/nvidia/nvidia-container-runtime#daemon-configuration-file
  https://stackoverflow.com/questions/59008295/add-nvidia-runtime-to-docker-runtimes

  runc와 NVIDIA 런타임의 차이 및 기본 런타임 설정 필요성
  1. runc란 무엇인가?
  **runc**는 Open Container Initiative (OCI) 표준을 따르는 가벼운 컨테이너 런타임입니다.
  runc는 Docker 및 다른 컨테이너 엔진에서 컨테이너의 실제 실행을 담당하는 런타임으로, 리눅스 컨테이너(LXC) 기술을 기반으로 작동합니다.
  기본 런타임으로 설정된 경우, GPU와 같은 특수 하드웨어에 접근할 기능이 없기 때문에 NVIDIA GPU와 관련된 기능은 사용할 수 없습니다​
  NVIDIA


vscode@97eac1039b5e /w/signal-masters> sudo find /usr -name 'libnvidia-ml.so*' ! -path '*stubs*'
vscode@97eac1039b5e /w/signal-masters> 

https://python-poetry.org/docs/repositories/
  poetry source
  
  poetry source add --priority=explicit pytorch-cu102 https://download.pytorch.org/whl/cu102
  poetry add --source pytorch-cu102 torch==1.7.0+cu102 torchvision==0.8.1+cu102

https://pytorch.org/get-started/previous-versions/#v1121
  🔑 version: ~CUDA 10.2
https://download.pytorch.org/whl/torch/
  🔑 version: manylinux2014_aarch64
    torch-1.12.1-cp310-cp310-manylinux2014_aarch64.whl

    cu- 접두사가 없는 PyTorch 빌드는 일반적으로 CUDA 런타임을 포함하지 않습니다.
    Jetson Nano에서는 JetPack SDK가 이미 시스템에 설치된 CUDA 런타임을 제공하므로, cu- 접두사가 없는 PyTorch 패키지도 GPU 가속을 사용할 수 있습니다.
ABI(이진 인터페이스)


👍 https://forums.developer.nvidia.com/t/gpu-usage-info-nvidia-smi-is-not-there/76155
  Hi Markus, nvidia-smi isn’t supported on Tegra-based platforms. Instead please try the tegrastats utility, you can launch it by running “sudo tegrastats” from the terminal.
  See here for the documentation on tegrastats tool: [url]Welcome — Jetson Linux<br/>Developer Guide 34.1 documentation 487
  jetson nano: >> %shell> tegratstats





libnvidia-ml.so.1는 NVIDIA Management Library (NVML)의 라이브러리입니다. NVML은 NVIDIA GPU의 상태를 모니터링하고 제어할 수 있는 API를 제공합니다. PyTorch와 같은 GPU 기반 라이브러리에서 이 라이브러리를 사용하여 GPU 프로세스 정보를 얻고, 메모리 사용량을 관리하는 등의 작업을 수행합니다. 오류 메시지를 보면, Docker 컨테이너 내에서 이 라이브러리를 찾을 수 없어서 발생한 문제입니다.



🚨 https://forums.developer.nvidia.com/t/how-to-enable-cuda-with-pytorch-running-on-a-jetson-nano-2gb-device/282762
head -n 1 /etc/nv_tegra_release

###### https://qengineering.eu/install-pytorch-on-jetson-nano.html?ref=xaviergeerinck.com

https://developer.nvidia.com/embedded/learn/tutorials/jetson-container

Jetson Nano에 맞는 PyTorch 버전과 휠 파일을 다운로드하여 설치해야 합니다. 현재 Jetson Nano에서 사용할 수 있는 PyTorch의 가장 최신 버전은 1.10.x입니다.

pip list --not-required


docker run --runtime nvidia -it --rm --network=host jayfalls/l4t-20.04:full-cp311
python3 -c 'import torch; print(f"PyTorch version: {torch.__version__}"); print(f"CUDA available:  {torch.cuda.is_available()}"); print(f"cuDNN version:   {torch.backends.cudnn.version()}"); print(torch.__config__.show());'
Package             Version
------------------- --------------------
certifi             2019.11.28
chardet             3.0.4
cmake               3.29.6
dbus-python         1.2.16
idna                2.8
numpy               1.23.1
pillow              10.3.0
pip                 24.1
psutil              6.0.0
PyGObject           3.36.0
python-apt          2.0.1+ubuntu0.20.4.1
requests            2.22.0
requests-unixsocket 0.2.0
setuptools          45.2.0
six                 1.14.0
tensorrt            8.2.1.8
torch               1.13.0
torchvision         0.14.0a0+5ce4506
typing_extensions   4.12.2
urllib3             1.25.8
wheel               0.34.2


TODO 남기기.
  #️⃣📰 Personalizing with dotfile repositories ; https://code.visualstudio.com/docs/devcontainers/containers#_personalizing-with-dotfile-repositories
  #️⃣📰 Known limitations ; https://code.visualstudio.com/docs/devcontainers/containers#_known-limitations
일단 컨테이너 안의 파일 복사 안됨.
TODO: for prototype.. VSCode dev containers..  ➡️ #📍 "Always installed" extensions
  TODO: vscode, ultralytics tos 크롤링


>>> Container DISPLAY...  ; https://stackoverflow.com/a/75663407

🚨 During the Docker build process, some steps may default to the root user. This means certain parts of the installation try to access directories like / (root directory) or attempt to write logs there.


🚨 왜 xhost +local:이 SSH 터미널에서 제대로 작동하지 않나?
  xhost 설정은 X 서버가 실행 중인 시스템에서 직접 실행해야 합니다. SSH를 통해 접속한 시스템은 X 서버를 직접 운영하지 않으므로, 권한을 변경하려고 할 때 문제가 발생합니다.

⚓ pytorch 정적 양자화 ; https://pytorch.org/docs/stable/quantization.html#quantization-api-summary






https://github.com/dusty-nv/jetson-containers

https://hub.docker.com/r/dustynv/pytorch/tags
https://github.com/dusty-nv/jetson-containers/blob/master/packages/pytorch/Dockerfile

https://www.elinux.org/Jetson_Zoo
⭕ Official ; https://github.com/dusty-nv/jetson-containers/tree/master
  docker run --runtime nvidia -it --rm --network=host dustynv/pytorch:1.10-r32.7.1
  %shell> python3 -c 'import torch; print(f"PyTorch version: {torch.__version__}"); print(f"CUDA available:  {torch.cuda.is_available()}"); print(f"cuDNN version:   {torch.backends.cudnn.version()}"); print(torch.__config__.show());'
    - CuDNN 8.2.1
    - Built with CuDNN 8.0

  docker run --runtime nvidia -it --rm --network=host nvcr.io/nvidia/l4t-pytorch:r32.7.1-pth1.10-py3
  docker run --runtime nvidia -it --rm --network=host jayfalls/l4t-20.04:full-cp311

in python 3.12
  (myenv) root@jsnano:~/test# python3 -c 'import torch; print(f"PyTorch version: {torch.__version__}"); print(f"CUDA available:  {torch.cuda.is_available()}"); print(f"cuDNN version:   {torch.backends.cudnn.version()}"); print(torch.__config__.show());'
  Error in cpuinfo: prctl(PR_SVE_GET_VL) failed
  /root/test/myenv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)
    cpu = _conversion_method_template(device=torch.device("cpu"))
  PyTorch version: 2.5.0
  CUDA available:  False
  cuDNN version:   None
  PyTorch built with:
    - GCC 10.2
    - C++ Version: 201703
    - Intel(R) MKL-DNN v3.5.3 (Git Hash 66f0cb9eb66affd2da3bf5f8d897376f04aae6af)
    - OpenMP 201511 (a.k.a. OpenMP 4.5)
    - LAPACK is enabled (usually provided by MKL)
    - NNPACK is enabled
    - CPU capability usage: NO AVX
    - Build settings: BLAS_INFO=open, BUILD_TYPE=Release, CXX_COMPILER=/opt/rh/devtoolset-10/root/usr/bin/c++, CXX_FLAGS=-ffunction-sections -fdata-sections -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=open, TORCH_VERSION=2.5.0, USE_CUDA=OFF, USE_CUDNN=OFF, USE_CUSPARSELT=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 
      



root@jsnano:~/test# pyenv global 3.10
root@jsnano:~/test# python3 -m venv myenv
root@jsnano:~/test# source myenv/bin/activate
(myenv) root@jsnano:~/test# pip install https://download.pytorch.org/whl/torch-1.12.1-cp310-cp310-manylinux2014_aarch64.whl#sha256=4e1b9c14cf13fd2ab8d769529050629a0e68a6fc5cb8e84b4a3cc1dd8c4fe541
Collecting torch==1.12.1
  Downloading https://download.pytorch.org/whl/torch-1.12.1-cp310-cp310-manylinux2014_aarch64.whl (55.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.7/55.7 MB 736.2 kB/s eta 0:00:00
Collecting typing-extensions
  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)
Installing collected packages: typing-extensions, torch
Successfully installed torch-1.12.1 typing-extensions-4.12.2
WARNING: There was an error checking the latest version of pip.
(myenv) root@jsnano:~/test# python3 -c 'import torch; print(f"PyTorch version: {torch.__version__}"); print(f"CUDA available:  {torch.cuda.is_available()}"); print(f"cuDNN version:   {torch.backends.cudnn.version()}"); print(torch.__config__.show());'
PyTorch version: 1.12.1
CUDA available:  False
cuDNN version:   None
PyTorch built with:
  - GCC 10.2
  - C++ Version: 201402
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: NO AVX
  - Build settings: BLAS_INFO=open, BUILD_TYPE=Release, CXX_COMPILER=/opt/rh/devtoolset-10/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=open, TORCH_VERSION=1.12.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 



(myenv) root@jsnano:~/test# deactivate
root@jsnano:~/test# python3 -c 'import torch; print(f"PyTorch version: {torch.__version__}"); print(f"CUDA available:  {torch.cuda.is_available()}"); print(f"cuDNN version:   {torch.backends.cudnn.version()}"); print(torch.__config__.show());'^C
pyroot@jsnano:~/test# pyenv global system
root@jsnano:~/test# python3 --version
Python 3.6.9
root@jsnano:~/test# python3 -c 'import torch; print(f"PyTorch version: {torch.__version__}"); print(f"CUDA available:  {torch.cuda.is_available()}"); print(f"cuDNN version:   {torch.backends.cudnn.version()}"); print(torch.__config__.show());'
PyTorch version: 1.10.0
CUDA available:  True
cuDNN version:   8201
PyTorch built with:
  - GCC 7.5
  - C++ Version: 201402
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: NO AVX
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_53,code=sm_53;-gencode;arch=compute_62,code=sm_62;-gencode;arch=compute_72,code=sm_72
  - CuDNN 8.2.1
    - Built with CuDNN 8.0
  - Build settings: BLAS_INFO=open, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=8.0.0, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -DMISSING_ARM_VST1 -DMISSING_ARM_VLD1 -Wno-stringop-overflow, FORCE_FALLBACK_CUDA_MPI=1, LAPACK_INFO=open, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=ON, USE_NCCL=0, USE_NNPACK=ON, USE_OPENMP=ON, 


curl -s https://download.pytorch.org/whl/torch/ | grep 'torch-1.10' | grep 'aarch64' | grep 'cp37' | awk -F'"' '{print "https://download.pytorch.org" $2}'
curl -s https://download.pytorch.org/whl/torch/ | grep 'aarch64' | grep 'cp36' | awk -F'"' '{print "https://download.pytorch.org" $2}'



torch (1.10.0)
torchaudio (0.10.0+d2634d8)
torchvision (0.11.0a0+fa347eb)

(myenv) root@jsnano:~/test# pip install https://download.pytorch.org/whl/cpu/torch-1.10.2-cp36-cp36m-manylinux2014_aarch64.whl#sha256=935e5ac804c5093c79f23a7e6ca5b912c166071aa9d8b4a0a3d6a85126d6a47b
torch-1.10.2-cp36-cp36m-manylinux2014_aarch64.whl is not a supported wheel on this platform.






https://github.com/jayfalls/jetson_nano_ubuntu20_docker/tree/main



🖨️ 인천관리시설관리공단 / 포스코 ICT 📅 2024-10-21 09:12:00
  내년 2월에 한 번 있음.
  >>>>>>>
  교대근무하면 4300   안하면 4000

  기술자 ?

1시 발표.



하드웨어 아키텍처
소프트웨어 아키텍처
  // 모델..
  작업 흐름도
  



젯슨 나노용 프로젝트 git 하나 만들어서 올려야겟음. 변경사항 확인이 너무 어려워서.. ppt 에 만들 README.md 이거에 대해서도 만들어야 하고.

지금 현재 가상환경 사용 안하는중인데 (어차피 버전낮아서 가상환경 만들면 torch +yolo 설치 어려움.. pip install 에 목록 없어서), 이 경우 내 패키지 모듈화하고 있는데 이게 자꾸 module not found 떠서 이거도 해결해야함.


Jetson nano yolo 양자화 (금방할듯?) 하고 테스트하고 pyserial 로 연동하기



docker run --runtime nvidia -it --rm --network=host ultralytics/ultralytics:latest-jetson-jetpack4
python3 -c 'import torch; print(f"PyTorch version: {torch.__version__}"); print(f"CUDA available:  {torch.cuda.is_available()}"); print(f"cuDNN version:   {torch.backends.cudnn.version()}"); print(torch.__config__.show());'

cuDNN version:   None

sudo apt update
sudo apt install libcudnn8 libcudnn8-dev


docker run --runtime nvidia \
  --gpus all \
  -e LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/aarch64-linux-gnu \
  -it ultralytics/ultralytics:latest-jetson-jetpack4 /bin/bash


이미 설치 잘 되어있어vscode@jsnano:/workspaces/signal-masters$ pip freeze --all | grep torch
torch @ https://github.com/ultralytics/assets/releases/download/v0.0.0/torch-1.11.0a0+gitbc2c6ed-cp38-cp38-linux_aarch64.whl#sha256=87c3b1fade566123ddb4e1aa3e7a4ad49a5f05c8324556b0d8b40896731bf4ae
torchvision @ https://github.com/ultralytics/assets/releases/download/v0.0.0/torchvision-0.12.0a0+9b5a3fe-cp38-cp38-linux_aarch64.whl#sha256=71fb38cc7c39e825119f23cd896877b883c5e9fb4a77ca3fabfb35912dd41c60

ChatGPT의 말:
ChatGPT
현재 PyTorch와 torchvision이 설치되어 있지만, 해당 버전에서는 cuDNN을 사용하지 않도록 설정된 상태입니다. 문제의 원인은 PyTorch가 cuDNN이 비활성화된 상태로 빌드된 버전을 사용하고 있기 때문입니다.

아래 해결책으로 문제를 확인하고 해결해 보세요.

1. PyTorch 빌드 문제 확인
PyTorch가 cuDNN을 인식하지 못하는 이유는 해당 버전이 cuDNN 지원 없이 빌드된 버전이기 때문입니다. 현재 설치된 버전이 다음과 같습니다:

PyTorch: 1.11.0a0+gitbc2c6ed
Torchvision: 0.12.0a0+9b5a3fe
이 버전들은 ultralytics에서 제공된 커스텀 빌드이며, 이 빌드에서 USE_CUDNN=OFF로 설정된 상태입니다. 따라서 cuDNN이 활성화되지 않는 것입니다.



FROM nvcr.io/nvidia/l4t-base:r32.7.1 이미지에, python3.12 가상환경을 설치해서 import torch 후 is_available() 하는 경우
  Error in cpuinfo: prctl(PR_SVE_GET_VL) failed
jetson




docker run --runtime nvidia -it --rm --network=host jayfalls/l4t-20.04:full-cp311

✅  model -> onnx 변환까지는 어느 호스트에서 해도 상관없는데,tensorrt 로 변환만 젯슨 나노에서 하면되는거?
  sudo apt install python3-pip -y

git clone https://github.com/jayfalls/jetson_nano_ubuntu20_docker
cd jetson_nano_ubuntu20_docker/yolo
wget https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu102 --force-reinstall
pip3 install ultralytics
yolo export model=yolo11n-pose.pt format=onnx opset=13 simplifyxz

python3 -c 'import torch; print(f"PyTorch version: {torch.__version__}"); print(f"CUDA available:  {torch.cuda.is_available()}"); print(f"cuDNN version:   {torch.backends.cudnn.version()}"); print(torch.__config__.show());'
python3 -c 'import cv2; print(f"CUDA available: {cv2.cuda.getCudaEnabledDeviceCount() > 0}")'
>>>>>>>> 👍 True True for  sudo docker run --runtime nvidia -it --rm --network=host dustynv/l4t-ml:r32.7.1
  https://github.com/dusty-nv/jetson-containers/tree/master/packages/l4t/l4t-ml

from ultralytics import YOLO
from ultralytics.engine.results import Results
model = YOLO("yolo11n-pose.pt")  # Load pre-trained YOLO model
print(model.predict("https://ultralytics.com/images/bus.jpg"))
print(model.predict("https://ultralytics.com/images/bus.jpg"))
print(model.predict("https://ultralytics.com/images/bus.jpg"))
print(model.predict("https://ultralytics.com/images/bus.jpg"))
print(model.predict("https://ultralytics.com/images/bus.jpg"))
print(model.predict("https://ultralytics.com/images/bus.jpg"))
print(model.predict("https://ultralytics.com/images/bus.jpg"))
print(model.predict("https://ultralytics.com/images/bus.jpg"))
print(model.predict("https://ultralytics.com/images/bus.jpg"))
print(model.predict("https://ultralytics.com/images/bus.jpg"))
...

>>>>>>>> 👍  for
xhost +local:docker && sudo docker run \
--runtime nvidia \
-it --rm
--network host --ipc host 
--gpus all \
-e DISPLAY=$DISPLAY \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-v ~/.Xauthority:/root/.Xauthority \
ultralytics/ultralytics:latest-jetson-jetpack4



>>>> NVIDIA Machine Learning Containers for Jetson and JetPack ; https://github.com/dusty-nv/jetson-containers
docker pull dustynv/l4t-ml:r36.2.0

docker run --runtime nvidia -it --rm --network=host dustynv/pytorch:1.10-r32.7.1
https://hub.docker.com/r/dustynv/l4t-ml/tags
sudo docker run --runtime nvidia -it --rm --network=host dustynv/l4t-ml:r32.7.1


🚨 https://forums.developer.nvidia.com/t/unable-to-install-ultralytics-in-python3-6-in-jetpack4-6/275989/3
  https://github.com/ultralytics/yolov5/issues/11341
  python < 3.7 ...
https://pypi.org/project/pip-tools/


캘리브레이션 과정
[10/21/2024-12:34:34] [TRT] [W] Skipping tactic 3 due to insuficient memory on requested size of 796 detected for tactic 4.
Try decreasing the workspace size with IBuilderConfig::setMaxWorkspaceSize().
[10/21/2024-12:34:35] [TRT] [W] Tactic Device request: 947MB Available: 472MB. Device memory is insufficient to use tactic.
[10/21/2024-12:34:35] [TRT] [W] Skipping tactic 3 due to insuficient memory on requested size of 947 detected for tactic 4.
Try decreasing the workspace size with IBuilderConfig::setMaxWorkspaceSize().
[10/21/2024-12:34:42] [TRT] [W] Tactic Device request: 531MB Available: 515MB. Device memory is insufficient to use tactic.
[10/21/2024-12:34:42] [TRT] [W] Skipping tactic 3 due to insuficient memory on requested size of 531 detected for tactic 4.
Try decreasing the workspace size with IBuilderConfig::setMaxWorkspaceSize().

jsnano@jsnano:~$ tegrastats 
RAM 1332/3956MB (lfb 24x4MB) SWAP 234/6074MB (cached 31MB) CPU [5%@710,5%@921,4%@921,2%@921] EMC_FREQ 0% GR3D_FREQ 0% PLL@27C CPU@28.5C PMIC@50C GPU@29.5C AO@33.5C thermal@29C
RAM 1332/3956MB (lfb 24x4MB) SWAP 234/6074MB (cached 31MB) CPU [4%@102,4%@102,5%@102,2%@102] EMC_FREQ 0% GR3D_FREQ 0% PLL@27C CPU@29C PMIC@50C GPU@29.5C AO@33.5C thermal@29C
RAM 1332/3956MB (lfb 24x4MB) SWAP 234/6074MB (cached 31MB) CPU [9%@204,4%@204,8%@204,4%@204] EMC_FREQ 0% GR3D_FREQ 0% PLL@27C CPU@28.5C PMIC@50C GPU@29.5C AO@33.5C thermal@29C
RAM 1333/3956MB (lfb 24x4MB) SWAP 234/6074MB (cached 31MB) CPU [13%@102,6%@102,15%@102,16%@102] EMC_FREQ 0% GR3D_FREQ 0% PLL@27C CPU@28.5C PMIC@50C GPU@29.5C AO@33.5C thermal@29C
^C
🅾️ jsnano@jsnano:~$ sudo docker run --runtime nvidia -it --rm --network=host --ipc=host --gpus all ultralytics/ultralytics:latest-jetson-jetpack4
[sudo] password for jsnano: 
root@jsnano:/ultralytics# 


Try 1
  >>>>>>> sudo docker run --runtime nvidia -it --rm --network=host --ipc=host --gpus all ultralytics/ultralytics:latest-jetson-jetpack4
  from ultralytics import YOLO
  model = YOLO("yolo11n-pose.pt")
  model.export(format="engine", int8=True)
  trt_model = YOLO("yolov11n-pose.engine")
  results = trt_model("https://ultralytics.com/images/bus.jpg")

  >> output 
  [10/21/2024-15:02:11] [TRT] [E] ModelImporter.cpp:779: ERROR: builtin_op_importers.cpp:3352 In function importRange:
  [8] Assertion failed: inputs.at(0).isInt32() && "For range operator with dynamic inputs, this version of TensorRT only supports INT32!"


👍 Try 2
  >>>>>>> sudo docker run --runtime nvidia -it --rm --network=host --ipc=host --gpus all ultralytics/ultralytics:latest-jetson-jetpack4
from ultralytics import YOLO
model = YOLO("yolo11n-pose.pt")
model.export(format="engine", half=True)
trt_model = YOLO("yolov11n-pose.engine")
results = trt_model("https://ultralytics.com/images/bus.jpg")





👍 Try 3
  >>>>>>> sudo docker run --runtime nvidia -it --rm --network=host --ipc=host --gpus all -v ~/repo/signal-masters:/workspace ultralytics/ultralytics:latest-jetson-jetpack4
  Run python3, and ctrl + c and ctrl + v

  from ultralytics import YOLO

  # Load a YOLOv8n PyTorch model
  model = YOLO("/workspace/best.pt")

  # Export the model
  model.export(format="engine", half=True)  # creates 'yolov8n.engine'

  # Load the exported TensorRT model
  trt_model = YOLO("/workspace/best.engine")

  # Run inference
  results = trt_model("https://ultralytics.com/images/bus.jpg")


Yolo NAS

class_labels = {
    0: "GO",
    1: "LEFT",
    2: "RIGHT",
    3: "STOP",
    4: "SLOW"
}

👍 Try 4 with Fine Tuning
  >>>>>>> docker run --runtime nvidia -it --rm --network=host --ipc=host --gpus all \
-v ~/repo/signal-masters:/workspace ultralytics/ultralytics:latest-jetson-jetpack4 \
python3 -c "

from ultralytics import YOLO
model = YOLO('/workspace/best.pt')
model.names = {0: 'GO', 1: 'LEFT', 2: 'RIGHT', 3: 'STOP', 4: 'SLOW'}
model.export(format='engine', half=True)

trt_model = YOLO('/workspace/best.engine', task='pose')
trt_model.names = {0: 'GO', 1: 'LEFT', 2: 'RIGHT', 3: 'STOP', 4: 'SLOW'}
results = trt_model('https://ultralytics.com/images/bus.jpg')
results_origin = model('https://ultralytics.com/images/bus.jpg')
print(results)
print(results_origin)
"




🧮 Snippet: container_host_file_transfer.sh
  #!/bin/bash

  # Define variables for container names, paths, and filenames
  CONTAINER_NAME="cranky_elgamal"  # Docker container name
  DOCKER_CONTAINER_PATH="/ultralytics"  # Path inside the Docker container
  LOCAL_REPO_PATH="$HOME/repo/signal-masters/ml/models/from_ultralytics_container"  # Local path for storing copied files
  ENGINE_FILENAME="yolo11n-pose.engine"  # Name of the engine model file
  ONNX_FILENAME="yolo11n-pose.onnx"  # Name of the ONNX model file

  # Define variables for remote server connection
  DESTINATION_USER="wbfw109v2"  # Username for the remote server
  DESTINATION_HOST="10.10.16.154"  # Remote server's IP address
  DESTINATION_PATH="/home/$DESTINATION_USER/repo/intel-edge-academy-6/prototypes/_initialization/devcontainers/jetson_nano-mount/signal-masters/ml/models/from_ultralytics_container"  # Remote destination path

  # Copy ONNX model from Docker container to the local directory with '_half' suffix
  docker cp "$CONTAINER_NAME:$DOCKER_CONTAINER_PATH/$ONNX_FILENAME" "$LOCAL_REPO_PATH/${ONNX_FILENAME%.*}_half.onnx"

  # Copy engine model from Docker container to the local directory with '_half' suffix
  docker cp "$CONTAINER_NAME:$DOCKER_CONTAINER_PATH/$ENGINE_FILENAME" "$LOCAL_REPO_PATH/${ENGINE_FILENAME%.*}_half.engine"

  # Transfer the local model files to the remote server using SCP
  scp -r $LOCAL_REPO_PATH/* "$DESTINATION_USER@$DESTINATION_HOST:$DESTINATION_PATH/"

scp -r ~/repo/signal-masters wbfw109v2@10.10.16.154:/home/wbfw109v2/repo/intel-edge-academy-6/prototypes/_initialization/devcontainers/jetson_nano-mount

🥇 scp -r ~/repo/signal-masters/* wbfw109v2@10.10.16.154:/home/wbfw109v2/repo/Signal-Project/jetson-nano-mount/
rsync --archive --verbose --update --delete --dirs --progress \
  --exclude='.git' \
  $(git -C ~/repo/signal-masters ls-files --others --ignored --exclude-standard | sed 's/^/--exclude=/') \
  ~/repo/signal-masters/ \
  wbfw109v2@10.10.14.19:/home/wbfw109v2/repo/Signal-Project/jetson-nano-mount/


젯슨 나노에 전원이 들어오면, 젯슨 나노컨테이너에서 yolo 모델을 불러온 후 /dev/ttyACM0 serial 로 통신하도록 했는데, 이를 python3 run.py 를 실행하도록 하는 서비스로 만들어서 사용중.
sen 
근데 ACM0 와 통신이 안되는것? Python Serial 



docker exec -d -w /workspaces/signal-masters f4ce64f0a3af python3 run.py


rsync -av --exclude='.venv' ../cuda_test/ ./


사용한 볼트.
3.4v*3
3.7V, 4.44Wh
18650-2200mAh
YU10801-18001
Shenzhen Huayu New Energy Techonlogy Co.,LTD

# Copy ONNX model from Docker container to the local directory with '_half' suffix
docker cp "$CONTAINER_NAME:$DOCKER_CONTAINER_PATH/$ONNX_FILENAME" "$LOCAL_REPO_PATH/${ONNX_FILENAME%.*}_half.onnx"

# Copy engine model from Docker container to the local directory with '_half' suffix
docker cp "$CONTAINER_NAME:$DOCKER_CONTAINER_PATH/$ENGINE_FILENAME" "$LOCAL_REPO_PATH/${ENGINE_FILENAME%.*}_half.engine"

# Transfer the local model files to the remote server using SCP
scp -r $LOCAL_REPO_PATH/* "$DESTINATION_USER@$DESTINATION_HOST:$DESTINATION_PATH/"


**docker exec -d**는 백그라운드에서 명령을 실행합니다. 이 경우 systemd는 서비스가 즉시 완료되었다고 인식합니다.

journalctl -u signal-masters.service -f



여러 사람이 공용으로 들어가는 ssh 로 접속하는 환경에서 프로젝트에 git 을 써야하는데 공용으로 계정을 쓰기는 어렵잔아?
일회성으로 현재 터미널에 대해서만, 현재 프로젝트에 대해서만 권한을 주거나 oauth 같은 것이 있나? 


최근 Linux 환경에서 가장 주목받는 클립보드 도구 중 하나는 **wl-clipboard**입니다. 이 도구는 Wayland 기반 시스템에서 사용되며, 명령줄에서 클립보드 복사 및 붙여넣기를 수행하는 간단한 유틸리티입니다. X11 환경에서 주로 사용된 xclip과 xsel이 Wayland에서 제대로 작동하지 않는 문제를 해결하며 인기를 끌고 있습니다.
  wl-clipboard 주요 기능:
  wl-copy: 텍스트나 파일을 클립보드에 복사합니다.
  예: echo "Hello World" | wl-copy는 텍스트를 클립보드에 복사합니다.
  wl-paste: 클립보드에 저장된 내용을 터미널에 출력하거나 파일로 저장합니다.
  예: wl-paste > clipboard.txt는 클립보드 내용을 파일에 저장합니다.
  이 도구는 Unix 파이프와 파일 간의 데이터 전송을 용이하게 하며, Wayland 시스템의 특성에 최적화되어 있습니다.
  Wayland는 X11을 대체하는 최신 디스플레이 서버로 자리 잡고 있으며, wl-clipboard는 그 환경에 맞춘 가볍고 직관적인 도구로 평가받고 있습니다. 특히 보안과 성능을 강화한 Wayland에서 wl-clipboard는 클립보드 데이터를 안전하게 처리할 수 있는 기능을 제공합니다.

  또한, wl-clipboard를 더 확장한 **wl-clipboard-manager**도 주목받고 있는데, 이는 이미지 미리보기와 SQLite 기반 데이터 저장소를 활용해 고급 기능을 제공합니다. 이와 같은 도구들은 현대적인 Linux 배포판에서 편리한 사용성을 제공합니다.

  이 도구들은 Arch Linux 등 주요 배포판에서 쉽게 설치할 수 있으며, Wayland를 기본 지원하는 시스템에서 매우 유용하게 사용할 수 있습니다. Wayland를 사용하는 경우 이와 같은 도구를 통해 X11에서 겪던 호환성 문제를 해결할 수 있습니다.


✅ 
sudo apt update
sudo apt install wl-clipboard


set -x USER_EMAIL wbfw109v2@gmail.com
ssh-keygen -t rsa -b 4096 -C $USER_EMAIL

>>
Generating public/private rsa key pair.
Enter file in which to save the key (/home/wbfw109v2/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/wbfw109v2/.ssh/id_rsa
Your public key has been saved in /home/wbfw109v2/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:kAZVp9FN7399ZIoV87ew2Nwofaa5QM8et7T9wuwOn+A wbfw109v2@gmail.com
The key's randomart image is:
+---[RSA 4096]----+
|    ....o..o.    |
|     . . +. ..   |
|      + .     +  |
|     . .     . + |
|        S  . .o =|
|          . Bo==+|
|           ++%.B=|
|           .++%.*|
|            EB*++|
+----[SHA256]-----+


cat ~/.ssh/id_rsa.pub
# and copy and paste https://github.com/settings/keys
# >> ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCpnZq5CnCdclRxcJUtkite3Av1Inpp1ibyiOK0P6ERonW2nTknD/lesW2nzSDFypxK3YDUg6YZaplgQvH/K0+DOFoh7rf8aoPJGXGqOjbp3o+bbXPqxg/MkZ1F29OA3t5Iu0vLE6l59r7OxBSnqyjnXW43oqNgau9HH0LhlM6gSCDjLLAZMPBdVXrriAzlLnwCrccy/sCAn/npeBBMHdGXOFWdVxZUhXl+rUoTwM91VyxrOx67giOMgq55nVksutUXgEdeQ2duO8EkuZAiSU0vSQT0US0NUsT6XN5D+lHJx/0EF9GCjIaHDIMehMh9tHxhlwMvJen4LB+JUEcY1Nob11h9/loHIFJvl1F9ja9I+UStQuCbfO3grHIJmDp29eW/Vd6gSW2IiC1MSCL2oU8dIUOmMjqHWdBcziYHUnKG9o5nTUvObPy7pFZOn8HXQLIdEF05RFZI3rVFgSohVKsh16BwwrmTOVWkHS9glOItDifkhx5rfb8Hkr0JVXyV4EQnhAVBfgwaaKukLYOQX9sfPLkZKA2voyHK6ve2eTTabtynkzLNcNefKaMKCbUGTc4Y8CZSUQSh8ECs7EBS6eIKvNcQCh1DO1DvcKtm+kVqLpngDYn1vzCeZkHV8B/7mbrAS3/QuJbgP5GwkuLDUi6W7bGFv0MhFDxYOzEXQpYkKQ== wbfw109v2@gmail.com


eval (ssh-agent -c)
# e.g. >> Agent pid 34890


ssh-add ~/.ssh/id_rsa
# e.g. >> Identity added: /home/wbfw109v2/.ssh/id_rsa (wbfw109v2@gmail.com)

echo "복사한 공개 키 내용" >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys

서버에 설정





git config user.name "wbfw109v2"
git config user.email "wbfw109v2@gmail.com"

>>>>>>>>>>>📰❓ 각 A, B, C (각자 다른 데스크탑 PC를 가지고 있음. ssh 클라이언트이다.) 에서 D (SSH 서버) 로 접속한다고 가정. 📅 2024-10-23 06:19:56
  - A, B, C 는 ssh 클라이언트로서 D 에 "동일한" jsnano 계정으로 접속한다.
  - A, B, C 는 D에 접속 후, D-C 에 "동일한" nano 계정으로 접속한다.
  - A, B, C 는 D-C 에 "동일한" 작업 디렉터리에 대해 각 다른 git email, name 으로 관리를 해야 한다.
  1번, 4번에 대해 쓰려고 하는데,
  - A, B, C 는 D-C 에 접속할 때 docker exec 명령어가 아니라 VSCode 의 dev container 기능인 Reopen in dev container 기능을 가지고 컨테이너 안에서 작업을 한다.
    devcontainer.json 설정을 사용.
 어떻게 해야해?

    >> ssh -t jsnano@D "export GIT_AUTHOR_NAME='User A'; export GIT_AUTHOR_EMAIL='userA@example.com'; docker exec -it D-c /bin/bash"

      #!/bin/bash 에 설정필요..

      # SSH 접속 시 환경변수로 전달된 사용자 정보 활용
      GIT_USER_NAME=${GIT_AUTHOR_NAME:-"Default User"}
      GIT_USER_EMAIL=${GIT_AUTHOR_EMAIL:-"default@example.com"}

      # Git 설정 임시 적용
      git config user.name "$GIT_USER_NAME"
      git config user.email "$GIT_USER_EMAIL"

      1-1. entrypoint.sh 작성
        컨테이너 내부 /workspace/entrypoint.sh에 다음과 같이 작성합니다:

        bash
        Copy code
        #!/bin/bash

        # 사용자의 Git 정보 설정
        git config --global user.name "${GIT_USER_NAME:-'Default User'}"
        git config --global user.email "${GIT_USER_EMAIL:-'default@example.com'}"

        # 원래의 쉘을 시작합니다 (VSCode가 필요로 하는 bash 세션 유지)
        exec "$@"


      2-1. devcontainer.json 수정
        json
        Copy code
        {
          "name": "My Dev Container",
          "image": "ubuntu:latest",
          "remoteEnv": {
            "GIT_USER_NAME": "User A",
            "GIT_USER_EMAIL": "userA@example.com"
          },
          "overrideCommand": false,
          "initializeCommand": "/workspace/entrypoint.sh"
        }

        initializeCommand: 컨테이너가 시작될 때마다 entrypoint.sh가 실행됩니다.
        overrideCommand: 기본 명령을 덮어쓰지 않고 유지합니다.

  ... 인증은 어떻게함? config 는 달리해서 로그인했는데
    그냥 push 할 떄마다 매번 oauth 요구하게는 못하나?

    git config --global --unset credential.helper
  GitHub OAuth 인증을 강제하거나 매번 로그인을 유도하려면 다음과 같은 우회 방법을..?


  아니면 commit 만 하고 확장 프로그램 사용해서 관리하거나, 

Mediapipe -> 

👍 자이언트 스텝. 좋은 중견기업 ?

한국엔지니어링협회 .. 뭐하는곳?
  gksrnrwjdqhxhdtls

Joint, Velocity, Bone의 의미:
  Joint (관절 정보):

  인체의 주요 관절(예: 어깨, 팔꿈치, 무릎 등)의 절대적인 위치 좌표를 사용합니다. 이는 특정 순간의 신체 상태를 나타냅니다.
  Velocity (속도 정보):

  관절의 시간에 따른 변화량을 측정하며, 움직임의 속도를 의미합니다.
  이는 동작의 빠르기나 가속도를 포착해 특정 활동의 다이내믹을 구별하는 데 유용합니다.
  Bone (뼈 정보):

  인접한 두 관절 사이의 벡터를 나타내며, 신체의 포즈나 관절 간 관계를 파악합니다.
  이는 움직임의 구조적 특징을 추출하는 데 사용됩니다

PoseC3D는 3D-CNN 기반으로, 기존 GCN 기반 접근과 달리 그래프 구조 대신 3D 히트맵 볼륨을 사용해 스켈레톤 데이터를 처리합니다​
  - GCN과 차별화된 점:
    - 노이즈 강인성: 포즈 추정 시 발생하는 오류에 덜 민감하며, 다중 인물 추적에서도 추가 비용 없이 동작합니다.
    - 확장성: RGB 데이터와 쉽게 통합할 수 있어 다양한 모달리티 결합에 유리합니다.
    - 스켈레톤 행동 인식의 효율성 개선: 시간적, 공간적 패턴 학습에 유리한 구조를 가집니다​
  - 한 줄 요약:
    - PoseC3D는 3D-CNN을 활용하여 GCN의 한계를 극복하고, 더 높은 정확도와 유연성을 제공하는 최신 스켈레톤 행동 인식 모델입니다.


- X3D는 경량화된 3D-CNN 아키텍처로, 동작 인식에서 높은 성능과 효율성을 제공하기 위해 설계되었습니다.
  - 채널 수와 네트워크 깊이를 최소화하여, FLOPs(연산량)과 파라미터 수를 크게 줄이는 것이 특징입니다.
  - 이러한 특성 덕분에, 임베디드 시스템과 같은 제한된 자원 환경에서도 실시간 성능을 제공합니다.
  - PoseC3D와 같은 스켈레톤 기반 모델의 백본으로 활용되어, 동작 인식의 정확도를 높이는 데 기여합니다

3D-CNN(3D Convolutional Neural Network)은 시간과 공간 축을 함께 처리하는 합성곱 계층을 통해, 동영상이나 시계열 데이터에서 시간적 및 공간적 패턴을 동시에 학습하는 네트워크입니다.


>> 전체 파이프라인 필요할듯

골격 기반(Skeleton-based) 모델: 관절 포인트(keypoints) 데이터를 활용한 액션 인식에 최적화됨.
경량화된 아키텍처: X3D-s 백본은 총 241K 파라미터와 0.6GFLOPs만 사용하여 높은 효율성 제공.
노이즈 강인성(Noise Robustness): 다양한 자세 변형과 외부 환경 노이즈에도 안정적인 성능 제공.
실시간 처리 가능성: FLOPs가 낮아 임베디드 시스템이나 제한된 자원에서의 실시간 처리에 유리.
정확도: NTU60-XSub 데이터셋에서 92.3% 정확도를 달성, 최적의 경량 모델로 인정.
확장성: 다른 X3D 변형들과 마찬가지로 더 깊고 넓은 네트워크로 확장 가능하지만, 이 경우 성능과 자원 간의 균형 고려 필요.


# PoseC3D와 GCN의 문제점 및 해결 방안 정리

## 1. GCN(Graph Convolutional Network)의 문제점
GCN 기반 접근법은 스켈레톤 데이터를 그래프로 처리하며 액션 인식을 수행하지만 여러 단점이 존재합니다:

- **포즈 추정 모델 의존성**:  
  GCN은 관절 데이터를 그래프 노드로 사용하므로, 정확한 포즈 추정이 중요합니다. 그러나 포즈 추정 과정에서 발생하는 작은 오류나 노이즈에 매우 민감해 인식 성능이 저하될 수 있습니다.

- **시공간적 정보 처리의 한계**:  
  GCN은 그래프의 공간적 구조에는 강하지만 시간적 변화(temporal dynamics)를 효과적으로 처리하는 데 제한이 있습니다. 이로 인해 복잡한 움직임 인식에 어려움이 있습니다.

- **연산 복잡도의 선형 증가**:  
  GCN에서는 스켈레톤의 각 관절이 노드로 표현되며, 관절의 수와 프레임이 증가함에 따라 연산 복잡도가 선형적으로 늘어납니다. 이는 특히 실시간 처리 시스템에서 성능 저하를 초래할 수 있습니다.

- **확장성 문제**:  
  GCN 기반 모델은 주로 스켈레톤 데이터에 최적화되어 있어, RGB와 같은 다중 모달리티 데이터를 함께 처리하기 어렵습니다.

## 2. PoseC3D와 X3D 백본의 해결 방안
PoseC3D는 위의 문제를 해결하기 위해 **3D-CNN** 기반의 접근을 도입합니다. 이 모델은 **3D 열 지도(heatmap volume)**로 스켈레톤 데이터를 처리해 보다 강인한 시공간 정보 학습을 가능하게 합니다:

- **노이즈 강인성(Noise Robustness)**:  
  3D-CNN은 포즈 추정 과정에서 발생하는 작은 오류에 더 강하며, 스켈레톤 시퀀스의 시공간적 변화를 잘 포착합니다.

- **효율적인 연산 및 경량화**:  
  X3D와 같은 백본을 사용해 경량화된 네트워크로 설계되어, 241K 파라미터와 0.6GFLOPs로 실시간 처리가 가능합니다. 이는 Jetson Nano와 같은 임베디드 시스템에서도 높은 효율을 제공합니다.

- **다양한 모달리티와의 호환성**:  
  PoseC3D는 스켈레톤뿐 아니라 RGB 데이터와의 결합도 가능하여 확장성과 유연성을 제공합니다.

## 3. 결론
PoseC3D는 GCN의 단점을 해결하면서도 효율성을 극대화한 모델로, **임베디드 시스템 및 실시간 액션 인식**에 적합한 솔루션입니다. X3D 백본을 활용한 가벼운 설계는 연산 자원을 줄이면서도 높은 성능을 제공합니다. 이 모델은 특히 **교통 신호 인식**과 같은 정확한 동작 인식이 필요한 응용에 유리합니다.

더 깊이 있는 내용은 논문의 [원문](https://arxiv.org/pdf/2104.13586)에서 확인할 수 있습니다.
"""

5개 포즈
17


>> preprocess 어떻게 개선할것이냐.
  threading - 
  FlowChart 개선



양자화 INT8


10월 23 09:39:14 jsnano docker[6285]: Traceback (most recent call last):
10월 23 09:39:14 jsnano docker[6285]:   File "run.py", line 58, in <module>
10월 23 09:39:14 jsnano docker[6285]:     results = model(source=frame, save=False)
10월 23 09:39:14 jsnano docker[6285]:   File "/ultralytics/ultralytics/engine/model.py", line 176, in __call__
10월 23 09:39:14 jsnano docker[6285]:     return self.predict(source, stream, **kwargs)
10월 23 09:39:14 jsnano docker[6285]:   File "/ultralytics/ultralytics/engine/model.py", line 554, in predict
10월 23 09:39:14 jsnano docker[6285]:     return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)
10월 23 09:39:14 jsnano docker[6285]:   File "/ultralytics/ultralytics/engine/predictor.py", line 168, in __call__
10월 23 09:39:14 jsnano docker[6285]:     return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one
10월 23 09:39:14 jsnano docker[6285]:   File "/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py", line 43, in generator_context
10월 23 09:39:14 jsnano docker[6285]:     response = gen.send(None)
10월 23 09:39:14 jsnano docker[6285]:   File "/ultralytics/ultralytics/engine/predictor.py", line 274, in stream_inference
10월 23 09:39:14 jsnano docker[6285]:     s[i] += self.write_results(i, Path(paths[i]), im, s)
10월 23 09:39:14 jsnano docker[6285]:   File "/ultralytics/ultralytics/engine/predictor.py", line 334, in write_results
10월 23 09:39:14 jsnano docker[6285]:     string += f"{result.verbose()}{result.speed['inference']:.1f}ms"
10월 23 09:39:14 jsnano docker[6285]:   File "/ultralytics/ultralytics/engine/results.py", line 663, in verbose
10월 23 09:39:14 jsnano docker[6285]:     log_string += f"{n} {self.names[int(c)]}{'s' * (n > 1)}, "
10월 23 09:39:14 jsnano docker[6285]: KeyError: 8


가상환경 
  [10/21/2024-15:02:11] [TRT] [E] ModelImporter.cpp:779: ERROR: builtin_op_importers.cpp:3352 In function importRange:
  [8] Assertion failed: inputs.at(0).isInt32() && "For range operator with dynamic inputs, this version of TensorRT only supports INT32!"
  


WIKFI MODULE: NodeMCU
  price is ow
공유기 enba

AP(Application Processor)
휴대폰 스테이션 모드?
VNC ?



sudo apt -y install flatpak 
sudo flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo




https://www.ros.org/reps/rep-2000.html#jazzy-jalisco-may-2024-may-2029
 d

🆚 
  Flatpak	Snap	APT/RPM
  배포판 호환성	모든 리눅스 배포판 지원	Ubuntu 중심, 다른 배포판도 지원

  sudo apt -y install flatpak 

  flatpak install flathub com.usebottles.bottles
  flatpak run com.usebottles.bottles

find ~/.var/app/com.usebottles.bottles/ -type f -iname "*kakaotalk*"


VMWare network bridge settings? 

CMOS 세팅?
sudo apt -y install icoutils
wrestool -x -t3 -n1 --raw KakaoTalk_Setup.exe --output=/path/to/KakaoTalk.png

내가 가상 환경 window IP:
  10.10.14.179/19번
  linux ~ 21 번~ 39번

netwmark, gateway 는 동일하게 하기.
nameserver 는 윈도우와 동일하게
10.10.10.254 gateway


sudo halt -p ??

>>>
  flatpak install flathub com.usebottles.bottles
  flatpak run com.usebottles.bottles

  - Create a new Bottle
    Bottle type: Application



# https://ubuntu.com/server/docs/openssh-server
sudo apt install openssh-server -y

# lscpu | grep Virtualization
qemu-system-x86_64 -boot d -cdrom ubuntu-20.04.6-desktop-amd64.iso -m 2048 -enable-kvm -cpu host -machine pc



TODO:
  windows (share server) to ubuntu
  ubuntu (share server) to ubuntu
  자동갱신되게 하는 방법.
#  https://docs.usebottles.com/advanced/cli


>>>  NFS (Network Attached Storage) 쓰자.


오늘은 장치공유까지 해보자. 젯슨 나노에


>>>>>>>> Helix install prototype 자동화 스크립트에 추가. (일단 여기서 해보고)




lsb_release -a 🆚  uname -a



===== Run level
  # df
  #   /boot/efi : 부트 로더
  #   /dev/sda<n>: n번쨰 파티션에 설치되어있음.

  # text mode
  sudo init 3
  runlevel 
  # runlevel 은 유지보수, 복구모드에서 사용된다. root 로 이용됨.
  ps -F
  # system call functions: folk ..

  cat /etc/init.d/smbd

  #
  sudo apt install samba
  sudo service smbd status
  # make menuconfig
  sudo apt instsal lbuild-essential libncursed-5-dev libssh-dev bision flex libelf-dev dwarves

  ### "time"
  time make bzImage -j4

  man read
  su - root 🆚 su root
  📰 lost+found 복구.. sync 명령어?
  📰 which cd 하면 안나오는 이유가 내장 명령어라서. which cp 는 나옴. 외장 명령어라서. (일반 바이너리 파일들)
  📰 lsmod. 모델에 적재되어있는 디바이스 드라이브 확인. .ko 는 커널 오브젝트 파일
  커널 다운로드 후 커널이미지.. ls -l arch/x86/boot/bzImage
  sudo make modules_install --> 램디스크.. 필요..?
  /boot/..? 부트 로더 위치?
  /llib/modules/
  find . -name *.ko | wc -l  # 개수 확인.
  # debug mode, release mod 가 있음. strip 옵션은 릴리즈 모드에서 사용하는 것.
  # sudo find . -name *.ko -exec strip --strip-unnedded {} \; 
  📰 du -
  gedit ~/linux_c/copy.d/copy.c
  sudo apt install universal-ctags vim
  sudo ctags -R
  vi .. ~/.vimrc
    set number
    set ts=4
    set cindent
    set autoindent
    set smartindent
    set tags=/usr/include/tags
  # ps -ef | grep c.txt
https://docs.ros.org/en/jazzy/Releases.html

TPM이나 Secure Boot 환경이 아닌 경우: TPM이나 Secure Boot가 지원되지 않거나 사용되지 않는 QEMU 같은 가상화 환경에서는 X.509 서명 검증이 필요하지 않습니다.
  $ make menuconfig			   
  -*- Cryptographic API  --->          // 이동 후 제일 아래(인증 문자열 제거)
  # 커널의 보안과 무결성 검증에 사용되는 X.509 인증서와 간련된 것.
  Certificates for signature checking  --->   //선택, 아래 문자열 제거
  (debian/canonical-certs.pem) Additional X.509 keys for default system key  
  ()  Additional X.509 keys for default system keyring

  (debian/canonical-revoked-certs.pem) X.509 certificates to be preloaded into the system blacklist key 
  ()    X.509 certificates to be preloaded into the system blacklist key
    수정 후 Exit 선택 후 저장/종료

📍 Interview: Sticky bit ; https://en.wikipedia.org/wiki/Sticky_bit
  for /tmp dirctory 


"workbench.externalBrowser": "firefox" 설정 문제엿음.. 자꾸 파이어폭스로 열리는거..
📰 TODO: Although QEMU has a command line interface and a monitor to interact with running guests, they are typically only used for development purposes. libvirt provides an abstraction from specific versions and hypervisors and encapsulates some workarounds and best practices.
📰 TODO: Running QEMU/KVM. Warning: This example is just for illustration purposes - it is not generally recommended without verifying the checksums; Multipass and UVTool are much better ways to get actual guests easily.
/


sudo hx /etc/netplan/00-installer-config.yaml

📍 효림엑스이: https://m.saramin.co.kr/job-search/company-info-view?csn=aEpLbHFBQjhucHdVQ1FuWkl5QmZFQT09&t_ref_content=generic
  KS ? 에 들어가는 네비게이션을 만드는 회사? 
  
  
  DMS
    시나리오 설정.: 졸음 감지,  등등
    !! 모델 평가 및 검증에!!
      정밀도, 재현율 등 다양한 지표를 통해 모델의 정확서 ㅇ평가.
      및, 실제 테스트 및 검증. 필드에서 정확성과 신뢰성 평가.
      - 벤치 평가 & 👍 실차 평가. CANalyzer와 CANoe가
    검증 툴? 👍 캔벡터 Vector.. 강 교수님께서 가지고 계신다고 한다.
      Candb?
    


  자동차 검증 엔지니아
  DMS 시나리오. 자동차 전장 시나리오에서..  자율주행 3단계 정도에 들어가는 시나리오.
  이론&법규 세미나
  Driving Montioring system (DMS). 운전자의 상태를 실시간 감지/분석 운전 중 발생할 수 있는 위험 요소르 ㄹ최소화.
    졸음 운전, 주의 산만 등..
    Types
      ADDW (Advanced Driver Distraction Warning); 고급 운전자 주의산만 경고시스템
      DDAW (Driver Drowsiness and Attention Warning); 운전자 졸음 및 주의 경고시스템
    https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A02019R2144-20240707
    👍 자동차를 판매하기 위해서 무조건 넣어야 한다.
      2024년부터 출시하는 모든 차량에 DMS 탑재 의무화.. (EU 법규). >>>>  ㅜㅅ요가확실.
      General Safety Regulation(GSR)* 규정
      https://single-market-economy.ec.europa.eu/publications/new-rules-vehicle-safety-and-automated-mobility_en
      2024년 7월 7일부터 유럽 연합(EU)에서는 모든 신규 차량에 Driver Monitoring System(DMS) 탑재를 의무화했습니다
      EU 에서 정한 법규가 잇다.
        👍 요구사항, 활성/비활성화 기준, 경고 방식, 경고 성격, 항목.
  동적 계산 그래프 vs 정적 계산 그래프
    tensorflow 는 정적 계산 그래프 기반.
    pytorch sm 동적 계산 그래프 기반.

Driving Montioring system (DMS) - Driver Drowsiness and Attention Warning
https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=PI_COM:Ares(2021)1075107&rid=11
💯 https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=PI_COM%3AAres%282021%291075107


Regulation (EU) 2019/2144



address bus, data bus... 
  데이터 access 할 때 address bus 에 주소를 지정하고 그 주소에 8bit나 그런 데이터를 데이터 버스에 실어서 내가 
  32비트 아키텍처에서 접근할 수 있는 메모리 범위
b0 ~ 32 
🪱 Segmentaton Fault 오류 ; 메모리 잘못 접근.
전역 변수는 global 변수와 같은 공간에..
32 비트 운영체제에서는 4 바이트 주소 체계를 가짐. 포인터의 크기. 
  어드레스는 단순히 주소. 포인터는?.. 크기정보가 있는 포인터. 없는것도 잇긴 함. void pointer.
data bus 가 32 bit 이기 때문에 컴퓨터가 빠르게 접근할 수 있는 크기가 4바이트. 🚣 한 클럭에.
>>> 💹 chaggpt explain detail of program mermoy layout at most

스택에다가 구조체를 할당하면 ❓ **스택에 할당하는 구초제 비율이 커지니까 비효율적이니까** 힙 영역에 할당 권장. new delete
    copy. evalulation strategy

  list.append(..) 그래서 비효율적이다. ????

  - stack 은 힙보다 한정적인 
  -- X 프로그램을 실행할 때 스택 stack size 를 정할 수가 있나?
    
👍 https://byeo.tistory.com/entry/QEMU-VM%EC%9D%98-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%EB%A5%BC-%EC%9D%B8%ED%84%B0%EB%84%B7%EA%B3%BC-%EC%97%B0%EA%B2%B0%ED%95%98%EA%B8%B0?category=1362285



>> Linux commands todo sort out
  background -> foreground 로 가져올 수 있나? (ctrl+z) fg%a ?
  ps -ef, ps -f.  port.. 1 ~ 1024 known ports, unknown ports: ~ 65535
  sudo killall <process_name>; e.g. firefox
  sudoers
  umask
  top
  print
    Linux 커널에서 로그 메시지를 출력하기 위해 사용하는 함수
  dmesg
    dmesg eth0  # display meesage. driver message
    sudo dmesg | grep eth0
    sudo dmesg | wc -l    # line numbers
    sudo dmesg | wc -c    # character numbers
    sudo dmesg | wc -w    # word numbers

  samba
    sudo apt update
    sudo apt install smbclient cifs-utils

    sudo mount -t cifs -o username=<Windows_Username>,password=<Windows_Password> //Windows_IP_Address/Shared_Folder /mnt/windows_share
    sudo mount -t cifs -o username=intel-teacher/user,password=,uid=$(id -u),gid=$(id -g) //10.10.16.180/shared /mnt/windows-share
    sudo mount -t cifs -o username=ubuntu,password=ubuntu,uid=$(id -u),gid=$(id -g) //10.10.14.40/samba /mnt/windows_share
  🚣 pstree
  ps -f .. 서버 쉘? 로그인 쉘? 에 딸느 차이?

  /etc/shadow .. md5 암호화?






CIFS는 파일 공유 프로토콜이고, Samba는 해당 프로토콜을 구현한 소프트웨어 패키지
  CIFS: 지원 SMB 버전: SMB 1.0 (구버전, 보안 문제 존재)
  Smaba: 지원 SMB 버전: 최신 SMB(2.x, 3.x) 프로토콜 지원
!gcc >> gcc 에 대한 history.





jobs
fg %1   // ..파일 복구??? 이게 무슨ㅅ ㅗ리..
ps -f 에서 하이픈이 없는 것은 로그인 쉘?
. bash -> 현재 쉘로 실행.

./ 를 안하면 path 에 잇는것들중에 우선적으로 검색한다는듯.

내장/외장 명령어 확인 방법..
file /bin/pwd ??

ELF: excutable link format.

계획:
  환경
    host
      OS: Ubuntu 24.04
      network IPv4:
        Address: 10.10.14.19
        Network: 255.255.255.0
        Gateway: 10.10.14.254
        DNS: 203.248.252.2
    qemu vm is crated by
    
bash
  qemu-img create -f qcow2 ~/qemu/ubuntu-vm.qcow2 150G
  qemu-system-x86_64 \
    -enable-kvm \
    -smp 1 \
    -m 2048 \
    -machine q35 \
    -cpu host \
    -global ICH9-LPC.disable_s3=1 \
    -net nic,model=virtio \
    -net user,hostfwd=tcp::8022-:22,hostfwd=tcp::8090-:80 \
    -drive file=$HOME/qemu/OVMF_CODE_4M.secboot.fd,if=pflash,format=raw,unit=0,readonly=on \
    -drive file=$HOME/qemu/OVMF_VARS_4M.ms.fd,if=pflash,format=raw,unit=1 \
    -drive file=$HOME/qemu/ubuntu-vm.qcow2,if=none,id=disk0,format=qcow2,cache=writeback \
    -device virtio-blk-pci,drive=disk0,bootindex=0 \
    -drive file=$HOME/qemu/ubuntu-22.04.5-desktop-amd64.iso,if=none,id=cdrom,media=cdrom \
    -device ide-cd,bus=ide.1,drive=cdrom \
    -serial mon:stdio

  qemu virtual machine ubuntu 22
  명령어로 설정해야 하는 내용 
    network IPv4:
      Address: 10.10.14.39
      Network: 255.255.255.0
      Gateway: 10.10.14.254
      DNS: 203.248.252.2
    host OS 에서 복사했던 텍스트를 이 가상 머신 안에서 그대로 이용할 수 있도록 하는 방법
  
오직 명령어 또는 스크립트만 사용해서 처리하도록 하고싶ㅇ


탭
th0
br0
아무거도 설정안하면 virtual bridge, virtual ehternet swtich 도 없는 것.
  Host (Physical) NIC ; 라우터 

  Gateway, 서브넷마스크, DNS 

  같은 네트워크 대역이라고 하면. 24 bit subnetmask 
  원기둥이 라우터 모양.
  같은 네트워크 내역
    대역1: 10.1.1.1 ~ 10.1.1.20
    대역2:  192.168.0.1 24bit 면 (C 클래스) ~ 192.168.0.255
    대역3: 172.1.1.

    강튼 네트워크가 아니면 스위치에게 보내면 -> L2 브로드캐스트
    172 PC 가 자신의 스위치에게 패킷을 보내서 다른 192를 찾을 때, 자기 스위치 안에 없으면 다른 라우터 모두에게 브로드캐스트 / ARb Request..
      해서 뿌렷음. 
    스위치 넘어서 라우터 앞에 게이트웨이. 
      라우터의 eth0 이런것을 특별해서 게이트웨이라고 부름.
    라우터에는 "라우팅 테이블"이 있음 -- 모든(?) IP 에 대한 경로를 지정하고있어서 .. 라우팅 테이블 계산식에 의해서 찾아감.
      다른 대역대 갈거면.. "가장 가까운" 라우터의 eth0 가 게이트웨이를 지난다.
      
    게이트웨이를 정상적으로 설정해야 
      게이트웨이 IP 의 맨 끝 값은 누가 정하냐? 10.10.14.254
        일반적으로 254가 게이트웨이. 네트워크 망을 관리하고 있는 네트워크 회사 KT, 회사들이 이 값을 정하는 것.
        라우터 (통신/라우터 사업자. ISP. KT, LG) ---> 방화벽 (웹 방화벽과 다름) --> 스위치 --> 서버
          - 라우팅에서 개인에게 할당을 해준다.
          - 게이트웨이 IP
          ======== 공인 아이피가 결국 게이트웨이 IP
          게이트웨이 IP 아래 방화벽: L3 스위치, 방화벽도 L3 계층. 공유기도 L3 계층
          * picture: Test Network 구성도. 소규모 스위칭 구성. IP NAT ---> 192, 172
            공인 IP 를 사설 IP 로 변경해주는 것. 172 대역, 192대역.
            -- 방화벽 입장에서 게이트웨이는 External Router IP
            -- 메인 백본 (L2 or L3) . LC 면 방화벽 바로 앞의 Gateway.
            -- L2 는 같은 네트워크끼리의 통신을 이어주는 장비의 역할이라서 게이트웨이 IP 라는 것이 필요 없음. 실제로 할당도 불가능함.
            -- L3 는 필요하다.
            -- 공유기 허브 역할이 라우팅 테이블 역할도 하고, .. 공유기의 회색선이 통신 사업자랑 연결되어있는 선
              공인 IP 를 사설 IP 로 변경해주는 작업; NAT
            -- 개인 PC 입장에서는 보통 방화벽 앞의 
        // 게이트 웨이 IP 는     
  Virtual Bridge 라는 설정이 L2 스위치 (Virtual Ethernet Switch) 와 L3 스위치와 연결해주는 선.
  -- 브릿지가 없으면 NAT 를 사용 못해서 IP 를 한개밖에 없음.. ? ㅇㅇ?

  1. L2, L3 가 뭔가?
    계층 이름.
    L3 부터 NAT 를 할 수 있다.
    공인 IP = ISP 가 할당해준 영역.
  2. 172. 192, 10. ..
    192, 172, 10.. 은 사설 IP 대역으로 통상적으로 지정해좋은 대역. 약속. ㅇㅇ
      LAN (192.168.0.1) 게이트웨이 IP 를 끝 값을 1, 2, 3 으로 할 수도 있는데 이것도 약속이 있다. 관습

  192.168.0.1 이 게이트웨이 IP이다. 
    x port
    🪱 Port full NAT?
    포트의 대역대는 1 ~ 65535 까지 있음
      0 ~ 1024번까지는 지정되어있음
      Known port, unknoown port. 1024 ~ 65535.
  내 컴퓨터에서 승환 공유기로 보낼 때 공인 IP:<random port> (내부 IP 구분하기 위함)
    ** TCP session 마다 바꿈.


  -- 모든 L3 스위치가 게이트웨이가 여러 개 있을 수 있따. 라우팅 테이블이란 것이 있어서.       
    ISP 업체의 테이블 뿐 아니라공유기도 가능하긴 하다.


  LAN ----bridge------- Host machine
    bridge 안에는 여러 개의 인터페이스를 만들 수 있다?
    어댑터에 브릿지?? 
    bridge 가 선 자체가 아니고 eth0, tap0, tap1 인터페이스.
    "eth0" 은 실제 물리적인 인터페이스.
      브리지 설정을 하기 위해서 eth0 을 포기하고, 
      br0 

  클라우드드할떄
    eth0 이 있으면 원래 eth0이 이더넷꺼인데 eth0 대신에 br0 에 할당하고 
      


  byeotap0, wlo0 (이더넷), 
    wlo0, eth0 이런거도 이름 지정할 수 있음. ㅇㅇ.. 통상적으로.
      wlo0 내가 지정해준 이름.

  enps0 os 에 대한 인터페이스 명. 

  .........https://byeo.tistory.com/entry/QEMU-VM%EC%9D%98-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%EB%A5%BC-%EC%9D%B8%ED%84%B0%EB%84%B7%EA%B3%BC-%EC%97%B0%EA%B2%B0%ED%95%98%EA%B8%B0?category=1362285
    이거 이러면 외부에서 .. 포트 방식으로.. 201에서 포트로 구분하는 것..
    >> NAT ㅂ




  >>>>>>>> Helix install prototype 자동화 스크립트에 추가. (일단 여기서 해보고)
  그리고 ip -a c 이거 gnu 에서 찾고 권장이니까 O 표시하고, 관련 해석 보기.
  QEMU 우분투 22로 하기 ROS LTS 버전과 호한 + Yocto 버전과 tier1 호환.
  samba ? linux to linux 는 어떻게?

  내 host OS 가 라우터가 되는것
  https://richong.tistory.com/351ㅇ\

  브맂 ㅣ생성하고, tab0 tab1 생성해서 

  >??? 인터페이스 하나에 IP 여러 개 할당 가능하다고 한다?


  AWS -- 선택해서 구매하면 웹방화벽이 올라감.
    웹방화벽 OS 
  오라클 클라우드는, Ubuntu OS 를 설치를 하고, 웹방화벽을 올리기 위한 패키지들을 
    펜타OS

  tap: tab 장비. 그냥 흘려주는 역할.
  eth0 와 ㄱ비슷함.
  같은 L2 단에서는 MAC 값을 찾을 수 없다?
    Virtual MAC (VMAC
    L2 장비 (같은 사설 네트워크단을 연결해주는 장비) 는 MAC table
      IP2
      ??? 찾아보기 MAC table 을 누가 가지고 있지?
    
      MAc Table을 경로를 지정하기 위한 L2단의 프로토콜
    GARP: 나 여기 잇다.. 라고 하는 다른 것.

    1. L2 S/W 스위치에 연결되면, PC 가 GARP 를 뿌려서 Mac Table 을 업데이트한다.
    2. 또는 어떤 PC가 요청을 했을 때, L2 S/W 는 모든 PC 에게 브로드캐스트해서, 응답을 반환한다.
      ARP Request, Reponse 방식; 
    

    
    G ARP 패킷?

    
    L2 S/W tap1 

  외부에서도 LAn gateway 르 ㄹ통해서 응답을 주는 것 뿐?

  L3단은 포트마다 IP가 있고 L2 단은.. 아님?..아닌가?

  Gateway 가 IP 로 이루어진 것이고, Gateway 가 그 특징을 나타낸 것 뿐.

  A-1 ---(Routing Table)---- L3 ----(Routing Table)---- L3 -------

  Vm0 -> Vm1 로 ㄱ


  같은 네트워크 대역은 
    MAC 으로 통신한다고 보자..
    ping 으로 ip 를 보내도 <Mac> <inetrface name>
    "Bridge / Tap 네트워크 구성"


    penta os
    nmcli 

    // 장비 명령어
    

  두개 이상의 VM이 사설망 공유하도록 만들기 >> NAT

  EOL, EOF, EOD


  ⭕ ⚓ Bridge / Tap 네트워크 구성 ; https://gee6809.github.io/posts/qemu-network/



  DLSS 

  https://www.nvidia.com/ko-kr/geforce/graphics-cards/40-series/rtx-4060-4060ti/
  


sudo qemu-system-x86_64 -enable-kvm -cdrom http://archive.ubuntu.com/ubuntu/dists/bionic-updates/main/installer-amd64/current/images/netboot/mini.iso
  그리고 ip -a c 이거 gnu 에서 찾고 권장이니까 O 표시하고, 관련 해석 보기.
  QEMU 우분투 22로 하기 ROS LTS 버전과 호한 + Yocto 버전과 tier1 호환.
  samba ? linux to linux 는 어떻게?


  📰 mv ~/.wine/drive_c/users/wbfw109v2/Documents/카카오톡\ 받은\ 파일/2024년_제6회_K디지털_트레이닝_해커톤_참가신청서_작성중.docx ~/Downloads/
    sudo mount -t cifs -o username=Guest,password=,uid=$(id -u),gid=$(id -g) //10.10.14.40/class_401 /mnt/windows-share
    
    ### gnome 파일 시스템 바로가기 등록하는 명령어 필요.
    ~/.wine/drive_c/users/wbfw109v2/Documents/KakaoTalk Downloads
  ⚓ syscalls ; https://man7.org/linux/man-pages/man2/syscalls.2.html
    The list of system calls that are available as at Linux 5.14 (or in a few cases only on older kernels) is as follows: ...
    passwd

    -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -device e1000,netdev=net0


  sudo systemctl status qemu-kvm --no-pager && sudo systemctl status libvirtd --no-pager
  sudo usermod -aG kvm $USER && sudo usermod -aG libvirt $USER
  
  # VM managemnet tool virsh in libvirt-daemon-system package
  virsh net-list 
  virsh net-info default
  qemu-kvm

  리눅스 가상화 소프트웨어 도구 

  (Kernel Space쪽은 KVM, User Space 쪽은 QEMU)

  libvirt-daemon-system

  리눅스에서 Hypervisor기반 가상화 관리 및 제어도구

  bridge-utils

  VM간 가상 네트워크 구성 위한  브릿지 구성 및 관리 도구 
  virtinst

  리눅스 가상화 환경에서 VM 생성 및 관리 도구



  sudo mkdir -p /etc/qemu
  echo "allow br0" | sudo tee /etc/qemu/bridge.conf
  sudo setcap cap_net_admin+ep /usr/lib/qemu/qemu-bridge-helper

  # 1. 호스트의 유효한 네트워크 인터페이스 감지 (예: eno1)
  set INTERFACE (ip -o link show | awk -F': ' '/state UP/ && $2 !~ /lo|tailscale0/ {print $2; exit}')

  # 2. 감지된 네트워크 인터페이스 출력
  echo "Detected network interface: $INTERFACE"

  # 3. 브리지 네트워크 설정
  sudo ip link add name br0 type bridge
  sudo ip link set br0 up
  sudo ip link set "$INTERFACE" master br0

  # 4. 브리지 상태 확인
  echo "Bridge br0 created and linked to $INTERFACE."
  ip addr show br0



  virtual box -> ctrl+alt+f2 tty 개수. QEMU? 에서는?

  📰 DHCP


  sudo apt install -y spice-v

  nmcli c add type bridge ifname bridge0 con-name bridge0 ip4 192.168.0.3/24 gw4 192.168.0.1 ipv4.dns 164.124.101.2
  nmcli c add type ethernet slave-type bridge con-name eth-bridge0 ifname enp1s0 master bridge0
  nmcli c up eth-bridge0
  오직 명령어 또는 스크립트만 사용해서 처리하도록 하고싶ㅇ

  Netplan 설정 파일 이름과 우선순위
    Netplan은 /etc/netplan/ 디렉터리의 모든 YAML 파일을 읽어서 네트워크 설정을 적용합니다.
      파일이 여러 개일 경우, 숫자가 작은 파일이 먼저 적용됩니다.
        예: 00-, 01-, 50- 등과 같은 이름의 파일이 있으면 00-이 먼저 적용됩니다.
    파일 이름은 임의로 지정해도 되지만, 우선순위는 숫자 접두사를 기준으로 합니다.
    >> nmcli 를 쓰면 자동으로 이 곳에 파일 생성됨.
  권장 파일 이름
    서버와 데스크탑에서 공통적으로 사용하고 싶다면 00-network-config.yaml 또는 **01-netcfg.yaml**처럼 이름을 사용하는 것이 좋습니다.
      이유: 00- 또는 01- 접두사를 사용하면 다른 설정보다 우선 적용됩니다.
      예: /etc/netplan/00-network-config.yaml

  NAT 란: https://aws-hyoh.tistory.com/145
  .... https://m.blog.naver.com/love_tolty/222650880951


  ==========
  echo "
  <network>
    <name>br0</name>
    <forward mode='bridge'/>
    <bridge name='br0'/>
  </network>
  " | virsh net-define /dev/stdin

  virsh net-start br0
  virsh net-autostart br0

  qemu info qc2img


⚓ TAP (Traffic Access Point) ; https://en.wikipedia.org/wiki/Network_tap
  A network tap is a system that monitors events on a local network. 
  
  >> 현재
    qemu-img create -f qcow2 ~/qemu/ubuntu-vm.qcow2 150G
    qemu-system-x86_64 \
      -enable-kvm \
      -smp 1 \
      -m 2048 \
      -machine q35 \
      -cpu host \
      -global ICH9-LPC.disable_s3=1 \
      -net nic,model=virtio \
      -net user,hostfwd=tcp::8022-:22,hostfwd=tcp::8090-:80 \
      -drive file=$HOME/qemu/OVMF_CODE_4M.secboot.fd,if=pflash,format=raw,unit=0,readonly=on \
      -drive file=$HOME/qemu/OVMF_VARS_4M.ms.fd,if=pflash,format=raw,unit=1 \
      -drive file=$HOME/qemu/ubuntu-vm.qcow2,if=none,id=disk0,format=qcow2,cache=writeback \
      -device virtio-blk-pci,drive=disk0,bootindex=0 \
      -serial mon:stdio


sudo nmcli c add type bridge ifname br0 con-name br0 #Bridge 생성하기
sudo nmcli c add type ethernet slave-type bridge con-name eth-br0 ifname eth0 master br0 #eth0를 bridge에 연결하기
sudo nmcli c up eth-br0 #bridge 연결 활성화하기


cat /etc/udev/rules.d/99-bridge-filter.rules


👍 required: sudo apt install cloud-init cloud-image-utils
# write ... and
cloud-localds cloud-init.iso user-data.yaml meta-data.yaml
  Infrasturcture as a Code ..
👍 Run
sudo qemu-system-x86_64 \
  -enable-kvm \
  -smp 1 \
  -m 2048 \
  -machine q35 \
  -cpu host \
  -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -device e1000,netdev=net0 \
  -global ICH9-LPC.disable_s3=1 \
  -drive file=$HOME/qemu/OVMF_CODE_4M.secboot.fd,if=pflash,format=raw,unit=0,readonly=on \
  -drive file=$HOME/qemu/OVMF_VARS_4M.ms.fd,if=pflash,format=raw,unit=1 \
  -drive file=$HOME/qemu/ubuntu-vm.qcow2,if=none,id=disk0,format=qcow2,cache=writeback \
  -device virtio-blk-pci,drive=disk0,bootindex=0 \
  -serial mon:stdio

  # -cdrom cloud-init.iso \
  -netdev tap,id=net0,ifname=tap0,script=no,downscript=no \
  -device e1000,netdev=net0,mac=52:54:00:12:34:56 \
  # -net user,hostfwd=tcp::8022-:22,hostfwd=tcp::8090-:80 \

sudo qemu-system-x86_64 \
  -enable-kvm \
  -smp 1 \
  -m 2048 \
  -machine q35 \
  -cpu host \
  -netdev tap,id=net0,ifname=tap0,script=no,downscript=no \
  -device virtio-net-pci,netdev=net0,mac=52:54:00:12:34:56 \
  -global ICH9-LPC.disable_s3=1 \
  -drive file=$HOME/qemu/OVMF_CODE_4M.secboot.fd,if=pflash,format=raw,unit=0,readonly=on \
  -drive file=$HOME/qemu/OVMF_VARS_4M.ms.fd,if=pflash,format=raw,unit=1 \
  -cdrom cloud-init.iso \
  -boot d \
  -drive file=$HOME/qemu/ubuntu-vm.qcow2,if=none,id=disk0,format=qcow2,cache=writeback \
  -device virtio-blk-pci,drive=disk0,bootindex=1 \
  -serial mon:stdio
  
>> 👍 cloud-localds cloud-init.iso user-data.yaml meta-data.yaml --network-config=network-config.yaml
>> sudo cat /var/log/cloud-init-output.log
>> 👍 Cloud-init는 일반적으로 첫 부팅 시 한 번만 실행됩니다. 만약 기존 인스턴스를 수정하고 싶다면 cloud-init clean 명령을 사용하여 초기화 상태로 복구한 후, 인스턴스를 다시 시작해야 합니다. 이렇게 하면 cloud-init가 다시 실행됩니다.
  


🚨 cloud-init은 일반적으로 우분투 서버 이미지와 공식 클라우드 이미지에 기본적으로 설치되어 제공됩니다. 그러나 모든 우분투 이미지에 포함된 것은 아닙니다. 특히, 데스크탑 버전이나 일부 커스텀 ISO에서는 기본 설치가 누락될 수 있습니다.
  sudo apt update && sudo apt install cloud-init


- virtio를 사용하려면 VM 내부의 OS(예: Ubuntu)에 virtio-net 드라이버가 설치되어 있어야 합니다. 최신 리눅스 배포판에는 기본적으로 포함되어 있습니다.

cloud-init or use dnsmasq
  https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/configuring_and_managing_cloud-init_for_rhel_8/introduction-to-cloud-init_cloud-content
    by https://stackoverflow.com/questions/19721938/setting-ip-address-of-the-guest-operating-system-while-launching-qemu

sudo cloud-init clean
sudo cloud-init init
sudo cloud-init status --wait

tap0를 이용해서 QEMU VM을 구동합니다. tap0는 br0로 eth0와 묶여있기 때문에, 호스트와 같은 대역의 IP를 사용할 수 있습니다. LAN에 물려있는 공유기가 DHCP서버로써 가상머신들에도 ip를 할당해주게 됩니다.

만약 위의 작업을 하고도 tap 네트워크가 활성화되지 않는다면, 다음과 같은 작업을 추가로 해줍니다. 1) /etc/sysctl.d/bridge-filter.conf에 다음 내용 추가

net.bridge.bridge-nf-call-ip6tables=0
net.bridge.bridge-nf-call-iptables=0
net.bridge.bridge-nf-call-arptables=0
2) /etc/udev/rules.d/99-bridge-filter.rules 에 다음 내용 추가

ACTION=="add", SUBSYSTEM=="module", KERNEL=="br_netfilter", RUN+="/sbin/sysctl -p /etc/> sysctl.d/bridge-filter.conf"
3) 재부팅


My network IPv4:
  Address: 10.10.14.19
  Network: 255.255.255.0
  Gateway: 10.10.14.254
  DNS: 203.248.252.2
My VM1 network IPv4:
  Address: 10.10.14.39
  Network: 255.255.255.0
  Gateway: 10.10.14.254
  DNS: 203.248.252.2


2. tuntap의 약자와 의미
  tuntap은 TUN/TAP의 합성어입니다.

  TUN (Network TUNnel):
    3계층 네트워크 장치로서, IP 패킷을 처리합니다.
    일반적으로 VPN과 같은 터널링에 사용됩니다.
  TAP (Network TAP):
    2계층 네트워크 장치로서, Ethernet 프레임을 처리합니다.
    가상 네트워크 환경에서 브리지 연결 등에 사용됩니다.
    즉, tuntap은 TUN 및 TAP 가상 네트워크 인터페이스를 제공하는 커널 드라이버를 의미합니다.




>> 문제
  - 네트워크 구성한 것 영구 설정해야 함
  - 네트워크 구성한 것 다시 되돌릴 수 있어야 함.
  - 클립보드 공유해야 함.
  - QEMU 에서 활성화 창에서 사용한 단축키는 QEMU 에서 작동해야 함.
  - samba 우분투-우분투 간 통ee신 해야 함.


한국정보통신공사협회 - 초급기술자 발급받기 - 기술자감리원. 기술자자격 신청


https://unix.stackexchange.com/a/677988
# lname = link nmae
  find /sys/class/net -mindepth 1 -maxdepth 1 -lname '*virtual*' -prune -o -printf '%f\n'

텔레칩스, 하만, 인텔,글루브드 swm


sudo apt install mariadb-server
https://velog.io/@xangj0ng/Linux-Ubuntu-Mariadb-%EC%84%A4%EC%B9%98



# ⚓ Kernel offical urls ; https://www.kernel.org/
mkdir -p ~/qemu
wget -P ~/qemu https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.15.169.tar.xz
xz -d ~/qemu/linux-5.15.169.tar.xz



IP_ADDRESS="10.10.14.19"
NETMASK="255.255.255.0"
GATEWAY="10.10.14.254"
DNS="203.248.252.2"


- NoCloud는 cloud-init의 데이터 소스 중 하나로, 클라우드 환경 없이도 로컬 또는 가상 머신 환경에서 cloud-init 구성을 적용할 수 있게 합니다. 즉, 인터넷을 통해 클라우드 서비스에서 데이터를 가져오는 대신, 로컬 ISO 파일이나 파일 시스템에 있는 데이터로부터 초기화 설정을 수행합니다​
Nautilus


Atemgea 128 에 flash memory 가 128KB, SRAM: 4KB (힙 메모리영역..?전역 변수, 지역 변수가 잡히는 영역)
📰 Helix TODO:..
  dw: 현재 커서에서 다음 단어까지 삭제합니다.
  dW: 공백으로 구분된 단어까지 삭제합니다 (구두점 포함).
  xd: You can use xd, which will select the current line (x) and delete it (d). Tip: pressing x repeatedly will select more lines downwards.
범용 레지스터? R0 ~ R31 ? 32 비트구조.. ARM architecture.. LU.. 
메모리 공간을 잡고 변수에 이름을 붙인 것.


로컬 머신에서
  >> ssh-keygen -t rsa -b 4096 -C "wbfw109v2@gamil.com"

~/home/wbfw109v2/.ssh/id_rsa

사용자 이름 + 시스템 이름 + 용도를

ssh-keygen -t rsa -b 4096 -C "wbfw109v2@gmail.com" -f ~/.ssh/id_rsa_vm-management
  The key fingerprint is:
  SHA256:QTPThTRgQ5vdpmHnib1CqQPgWG2jvwSDZg2IF1tf5dQ wbfw109v2@gmail.com
  The key's randomart image is:
  +---[RSA 4096]----+
  |  . .  .@==+.    |
  |.. + o + @ooE    |
  |o + o = + = +    |
  | . B + . o X .   |
  |  = * . S = +    |
  | o   + . o   .   |
  |      o o . .    |
  |     . . . .     |
  |      .          |
  +----[SHA256]-----+

??? 임베디드 프로그래밍에서는 system call 함수를 직접 사용해서 사용하나? stdlib 를 안쓰고?

>> Holy.. 복구하기
  wbfw109v2@iot4-computer ~/r/intel-edge-academy-6 (main)> nmcli connection delete eth-br0
  Connection 'eth-br0' (9a85aeb1-c9a0-4cc7-971f-f6993b216e04) successfully deleted.
  wbfw109v2@iot4-computer ~/r/intel-edge-academy-6 (main)> nmcli connection connect eno1
  Error: argument 'connect' not understood. Try passing --help instead.
  wbfw109v2@iot4-computer ~/r/intel-edge-academy-6 (main) [2]> nmcli connection connect eno1^C
  wbfw109v2@iot4-computer ~/r/intel-edge-academy-6 (main) [2]> sudo nmcli device connect eno1

>>>>>>>>>>>>> 실패
wbfw109v2@iot4-computer ~/r/intel-edge-academy-6 (main)> sudo nmcli connection add type tap ifname tap0 con-name tap0 mode tap owner (i
d -u)
Error: bad connection type: 'tap' not among [6lowpan, 802-11-olpc-mesh (olpc-mesh), 802-11-wireless (wifi), 802-3-ethernet (ethernet), adsl, bluetooth, bond, bridge, cdma, dummy, generic, gsm, hsr, infiniband, ip-tunnel, loopback, macsec, macvlan, ovs-bridge, ovs-dpdk, ovs-interface, ovs-patch, ovs-port, pppoe, team, tun, veth, vlan, vpn, vrf, vxlan, wifi-p2p, wimax, wireguard, wpan, bond-slave, bridge-slave, team-slave]

네, nmcli 명령어를 사용해 TAP 인터페이스를 생성하려고 할 때 발생한 문제를 정확히 짚어주셨습니다. 현재 nmcli는 TAP 인터페이스에 대한 직접적인 타입(tap)을 지원하지 않습니다. 대신 tun 타입으로 생성됩니다.

  TUN과 TAP은 둘 다 가상 네트워크 인터페이스이지만, 각각의 역할이 약간 다릅니다.

  1. TUN과 TAP의 차이점
  TUN (Network Tunnel Device):

  IP 레벨의 가상 네트워크 인터페이스입니다.
  OSI 네트워크 계층에서 3계층(네트워크 계층) 패킷(IP 패킷)을 처리합니다.
  VPN 같은 네트워크 터널링에서 주로 사용됩니다.
  TAP (Network Tap Device):

  이더넷 프레임을 직접 처리하는 2계층(데이터 링크 계층) 인터페이스입니다.
  가상 머신(VM)이나 컨테이너에서 브리지 인터페이스를 통해 LAN과 연결할 때 주로 사용됩니다.
  2. nmcli로 TAP을 직접 생성할 수 없는 이유
  **nmcli**는 현재 TAP 인터페이스를 직접적으로 지원하지 않으며, TUN 인터페이스만을 지원합니다.
  하지만, ip 명령어를 사용하여 TAP 인터페이스를 생성한 후 브리지에 연결할 수 있습니다.

  