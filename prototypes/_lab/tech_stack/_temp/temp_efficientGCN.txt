EfficientGCN is a model designed for Skeleton-based Action Recognition.
  This means it analyzes the movements of a person's skeleton (joint position changes) to determine what action they are performing.
  
  What is skeleton data?
  Skeleton data is a set of coordinates that represents how a personâ€™s joints move over time in 3D space.
  These coordinates are captured using 3D cameras or pose estimation algorithms.
  For example, when a person raises their hand or walks, their wrists, elbows, shoulders, and other joints move over time.
  
  EfficientGCN processes this skeleton data to recognize actions, and it uses a structure called Graph Convolutional Network (GCN).
  GCN is suitable for learning relationships between joints in the network (for example, how arms and legs interact).

  1. The overall structure of EfficientGCN.
    EfficientGCN contains several important modules.
    These modules analyze and process skeleton data.

    a) Multiple Input Branches (MIB) Architecture.
      Skeleton data can include several features.
      For example, the position of joints, velocity (how position changes over time), bone lengths, and angles are all crucial.
      MIB is designed to handle these various pieces of information simultaneously.
      Specifically, it takes joint positions, velocities, bone lengths, and angles through different paths (branches), and fuses them at an early stage.
      This reduces complexity while ensuring that important information is not lost.

    b) Spatial Temporal Joint Attention (ST-JointAtt) Module.
      When processing skeleton data, not all joints are equally important.
      For example, in a waving motion, the wrist or elbow might be more important than the leg joints.
      Also, the specific moment in time when the action happens is important.
      The ST-JointAtt module is designed to find the important spatial (which joint is important) and temporal (which moment in time is important) information.
      It selects the key joints and moments for better recognition.

    c) Compound Scaling Strategy.
      EfficientGCN is available in several model sizes.
      For example, EfficientGCN-B0, EfficientGCN-B2, and EfficientGCN-B4 are different versions.
      These models are designed to balance performance and size (number of parameters, computation).
      The compound scaling strategy expands both the width (number of channels) and depth (number of layers) of the model simultaneously to improve performance while keeping complexity manageable.
      This strategy helps EfficientGCN maintain efficient and powerful performance across different situations.

  2. EfficientGCNâ€™s Layers.
    EfficientGCN uses various types of convolution layers.
    These layers analyze skeleton data and extract important features.

    a) Graph Convolution (GCN) Layer.
      The GCN layer is specialized for processing graph-shaped data.
      Skeleton data is represented as a graph where joints are nodes and the connections between joints are edges.
      GCN is highly suitable for learning the relationships between joints.
      It combines the information from each joint (node) and its neighboring joints to extract features of each joint.
      This helps in understanding the spatial structure of the skeleton.

    b) Temporal Convolution (TC) Layer.
      The TC layer is used to analyze how skeleton data changes over time.
      That is, it analyzes how the joints move as time progresses.
      This layer processes data along the time axis, helping understand the temporal patterns of actions.

    c) Separable Convolution (SepLayer).
      General convolution layers require many computations.
      Separable convolution reduces the amount of computation by dividing convolution into two steps:
        depth-wise Convolution: It applies convolution to each channel individually.
        Point-wise Convolution: It combines the results to form the output channels.
      By separating the operations, it reduces the computational cost and makes the model faster and more efficient.

    d) Bottleneck Layer (BottleLayer).
      This layer uses a bottleneck structure to reduce the computational cost of the model.
      It temporarily reduces the dimensionality of the input data and then expands it again.
      This reduces the computation cost while preserving the important information.

    e) Sandglass Layer (SGLayer).
      The Sandglass structure combines Depth-wise and Point-wise convolution.
      This structure is designed to reduce the computational load while maintaining performance.

  3. Efficiency and Performance.
    EfficientGCN is highly efficient compared to previous models.
    EfficientGCN-B4, for example, is three times smaller and three times faster than state-of-the-art models while delivering similar or even better accuracy.
    As a result, EfficientGCN achieves high performance with fewer resources.

  4. Conclusion and Summary for Presentation.
    EfficientGCN is an efficient model designed to solve complex action recognition problems.
    It processes skeleton data efficiently while maximizing performance by using various modules and layers.
    Specifically, the MIB architecture, ST-JointAtt module, and diverse convolution layers come together to deliver high performance with fewer parameters compared to more complex models.

    For a presentation, you can explain EfficientGCN using the following structure:
      Why EfficientGCN is necessary (background).
      How the MIB architecture processes data.
      The role and importance of the ST-JointAtt module.
      How each layer processes skeleton data and improves efficiency.
      A comparison of EfficientGCNâ€™s performance with existing models.

âš“ Softmax function ; https://en.wikipedia.org/wiki/Softmax_function
(Subscript | ë°‘ìˆ˜) ì™€ (Superscript | ìœ—ìˆ˜)
âš“ Attention (machine learning) ; https://en.wikipedia.org/wiki/Attention_(machine_learning)#Scaled_dot-product_attention
  #ï¸âƒ£ Self-attention ; https://en.wikipedia.org/wiki/Attention_(machine_learning)#Self-attention
âš“ Word embedding ; https://en.wikipedia.org/wiki/Word_embedding

ë””ì½”ë”ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ RNNLM(RNN Language Model)ì…ë‹ˆë‹¤. RNNLMì˜ ê°œë…ì„ ê¸°ì–µí•˜ê³  ìˆë‹¤ë©´ ì¢€ ë” ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ë””ì½”ë”ëŠ” ì´ˆê¸° ì…ë ¥ìœ¼ë¡œ ë¬¸ì¥ì˜ ì‹œì‘ì„ ì˜ë¯¸í•˜ëŠ” ì‹¬ë³¼ <sos>ê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤. ë””ì½”ë”ëŠ” <sos>ê°€ ì…ë ¥ë˜ë©´, ë‹¤ìŒì— ë“±ì¥í•  í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. 
https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html

ğŸ‘
[íŠ¸ë ŒìŠ¤í¬ë¨¸ ëª¨ë¸ ì´í•´í•˜ê¸°] Self-Attentionì—ì„œ Q, K, V(Query, Key, Value)ì˜ ì˜ë¯¸ ; https://cn-c.tistory.com/68#%EB%AA%A8%EB%93%A0%20%EB%8B%A8%EC%96%B4%EB%8A%94%20%EC%A7%88%EB%AC%B8(Query)%EC%9D%B4%EC%9E%90%20%EB%8B%B5%EB%B3%80(Key)%EC%9D%B4%EB%8B%A4-1
https://velog.io/@sjinu/%EA%B0%9C%EB%85%90%EC%A0%95%EB%A6%AC-Attention-Mechanism


c) Separable Convolution (SepLayer)
ì¼ë°˜ì ì¸ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ëŠ” ë§ì€ ê³„ì‚°ì„ ìš”êµ¬í•©ë‹ˆë‹¤. Separable Convolutionì€ ì´ëŸ¬í•œ ê³„ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ì»¨ë³¼ë£¨ì…˜ì„ ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤:
Depth-wise Convolution: ê° ì±„ë„ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ì»¨ë³¼ë£¨ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
Point-wise Convolution: ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ì¶œë ¥ ì±„ë„ì„ ë§Œë“­ë‹ˆë‹¤.
ì´ë ‡ê²Œ ë‚˜ëˆ„ì–´ì„œ ì²˜ë¦¬í•˜ë©´ ê³„ì‚°ëŸ‰ì´ ì¤„ì–´ë“¤ê³ , ëª¨ë¸ì´ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ë™ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
d) Bottleneck Layer (BottleLayer)
  ì¼ë°˜ì ì¸ CNN ì—ì„œ torch ë¡œ layer ë¥¼ êµ¬ì„±í•  ë•Œ ì½”ë“œëŠ” ë³´í†µ ì–´ë–»ê²Œ ì§œë‚˜?

  - Multiple Input Branches (MIB) ì•„í‚¤í…ì²˜
  - ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´:
    Graph Convolution (GCN) ë ˆì´ì–´
    Temporal Convolution (TC) ë ˆì´ì–´
    Separable Convolution (SepLayer)
    Expanded Separable Layer (EpSepLayer)
    Bottleneck Layer (BottleLayer)
    Sandglass Layer (SGLayer)
    ë“±ì˜ êµ¬ì¡°ë¥¼ GCNì— ì ìš©í•´ ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
  - Compound Scaling ì „ëµ:
    EfficientGCN-B0, EfficientGCN-B2, EfficientGCN-B4 ê°™ì€ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ëª¨ë¸


EfficientGCNv1
  https://github.com/zyxjtu/EfficientGCNv1
  ACMMM 2020 ; https://dl.acm.org/doi/abs/10.1145/3394171.3413802
  Arxiv Preprint ; https://arxiv.org/pdf/2010.09978.pdf

EfficientGCN 2
  https://github.com/attention-eq-everything/effgcn_cam
  IEEE T-PAMI; https://ieeexplore.ieee.org/abstract/document/9729609
  Arxiv Preprint ; https://arxiv.org/pdf/2106.15125

MLP (Multi-layer Perceptron): ì—¬ëŸ¬ ê°œì˜ ì¸µìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê¸°ë³¸ì ì¸ ì‹ ê²½ë§ êµ¬ì¡°ì…ë‹ˆë‹¤. ì±„ë„ ë‹¨ìœ„ë‚˜ ê³µê°„ì  ì°¨ì›ì—ì„œ ê°ê° ë…ë¦½ì ìœ¼ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
SENet (Squeeze-and-Excitation Networks): ê° ì±„ë„ì˜ ì¤‘ìš”ë„ë¥¼ í•™ìŠµí•˜ì—¬ ì ì‘ì ìœ¼ë¡œ ì¡°ì ˆí•˜ëŠ” ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤. AGC-LSTMê³¼ MS-AAGCNë„ ì´ëŸ° ë°©ì‹ìœ¼ë¡œ ì±„ë„ë³„ ì¤‘ìš”ë„ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
  - AGC-LSTM: Attention-Guided Convolution Long Short-Term Memory
    LSTM ê¸°ë°˜ì˜ ê³¨ê²© í–‰ë™ ì¸ì‹ ëª¨ë¸ë¡œ, ê´€ì ˆ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ë° MLPì™€ SENet êµ¬ì¡°ë¥¼ ê²°í•©í•˜ì—¬ ì±„ë„ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
  - MS-AAGCN: Multi-Scale Adaptive Graph Convolutional Network
    Spatial Graph Convolutionì„ ì‚¬ìš©í•œ ëª¨ë¸ë¡œ, ì£¼ë¡œ ê³µê°„ì  ì°¨ì›ì—ì„œ í•™ìŠµí•˜ê³  ë‹¤ë¥¸ ì°¨ì›ì„ í‰ê·  ì²˜ë¦¬í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.
LSTM (Long Short-Term Memory):
  ìˆœí™˜ ì‹ ê²½ë§(RNN)ì˜ í•œ ì¢…ë¥˜ë¡œ, ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ ê³¼ê±°ì˜ ì •ë³´ë¥¼ ê¸°ì–µí•˜ë©´ì„œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ëª¨ë¸ì…ë‹ˆë‹¤.
  íŠ¹íˆ ê¸´ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì£¼ë¡œ ì‹œê°„ì— ë”°ë¥¸ ë°ì´í„° ì²˜ë¦¬ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
. EfficientGCN-B0, B2, B4
  ; Base ì˜ ì•½ì. 


Data input
  ì• ì´ˆì— ì—¬ëŸ¬ í”„ë ˆì„ì„ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤.
  ì‹œê°„ì  ì •ë³´ê°€ ë³µì¡í•´ì§€ë©´, ë” ë§ì€ í”„ë ˆì„ì„ í†µí•´ ëª¨ë¸ì´ ì •í™•í•˜ê²Œ í–‰ë™ì„ ì¸ì‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì„±ëŠ¥ì€ **ì •í™•ë„(Accuracy)**ë¥¼ ì˜ë¯¸í•˜ë©°, ë” ë§ì€ ì‹œê°„ì  ë°ì´í„°ë¥¼ ì‚¬ìš©í• ìˆ˜ë¡ í–‰ë™ì˜ ì„¸ë¶€ì ì¸ ë³€í™”ê¹Œì§€ë„ í•™ìŠµí•  ìˆ˜ ìˆì–´ ì •í™•ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤.
  ê·¸ëŸ¬ë‚˜ í”„ë ˆì„ ìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ì¶”ë¡  ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì¦ê°€í•©ë‹ˆë‹¤. ê° í”„ë ˆì„ì— ëŒ€í•´ ì¶”ê°€ì ì¸ ì—°ì‚°ì„ ìˆ˜í–‰í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— TC ë ˆì´ì–´ê°€ ë” ë§ì´ í•„ìš”í•´ì§€ê³ , ì´ëŠ” ê³„ì‚° ë¹„ìš©ì„ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
  Jetson Nanoì™€ ê°™ì€ ì„ë² ë””ë“œ í•˜ë“œì›¨ì–´ì—ì„œëŠ” ì²˜ë¦¬ ì†ë„ì™€ ë©”ëª¨ë¦¬ê°€ ì œí•œì ì´ê¸° ë•Œë¬¸ì—, ì ì ˆí•œ í”„ë ˆì„ ìˆ˜ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. YOLO, MediaPipe, EfficientGCN ë“±ì˜ ëª¨ë¸ì„ ë™ì‹œì— ì‚¬ìš©í•´ì•¼ í•œë‹¤ë©´, ì‹œê°„ì  í”„ë ˆì„ ìˆ˜ëŠ” ì ì ˆíˆ ì¤„ì´ë©´ì„œë„ ì¤‘ìš”í•œ í–‰ë™ì„ ì¶©ë¶„íˆ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ì¡°ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. Jetson Nanoì˜ ì„±ëŠ¥ì„ ê³ ë ¤í•˜ë©´, ì´ˆë‹¹ 15~30 í”„ë ˆì„ìœ¼ë¡œ í•™ìŠµ ë° ì¶”ë¡ ì„ ì§„í–‰í•˜ëŠ” ê²ƒì´ ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  2. EfficientGCNì˜ N ê°œ í”„ë ˆì„ ì„¤ì • ê°€ëŠ¥ ì—¬ë¶€
    EfficientGCNì—ì„œ Nê°œì˜ í”„ë ˆì„ì„ ì…ë ¥ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Nì€ ëª¨ë¸ì´ í–‰ë™ì„ ì¸ì‹í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì‹œê°„ì  ì •ë³´ì˜ ë²”ìœ„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. ì¦‰, í•œ ë²ˆì˜ í•™ìŠµ ë˜ëŠ” ì¶”ë¡ ì—ì„œ ëª‡ ê°œì˜ ì—°ì†ëœ í”„ë ˆì„ì„ ì²˜ë¦¬í• ì§€ë¥¼ Nìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    í”„ë ˆì„ ìˆ˜ë¥¼ ì¡°ì •í•˜ëŠ” ì´ìœ :
    ë” ë§ì€ í”„ë ˆì„(N)ì´ ì‚¬ìš©ë˜ë©´ ëª¨ë¸ì€ ë” ê¸´ ì‹œê°„ ë™ì•ˆì˜ í–‰ë™ì„ ì¸ì‹í•  ìˆ˜ ìˆì–´ ì •í™•ë„ê°€ ë†’ì•„ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³„ì‚° ë¹„ìš©ì´ ì¦ê°€í•˜ê²Œ ë©ë‹ˆë‹¤.
    4ê°œì˜ ëª¨ë¸ ì‚¬ìš© ì‹œ í”„ë ˆì„ ìˆ˜ ì¡°ì •:
    Jetson Nanoì—ì„œ 4ê°œì˜ ëª¨ë¸(YOLO, MediaPipe Hand Landmarks, Pose, EfficientGCN)ì„ ì‚¬ìš©í•  ë•ŒëŠ” í•˜ë“œì›¨ì–´ ì„±ëŠ¥ì— ë”°ë¼ í”„ë ˆì„ ìˆ˜ë¥¼ ì¡°ì ˆí•´ì•¼ í•©ë‹ˆë‹¤.
    ê¶Œì¥ í”„ë ˆì„ ìˆ˜ëŠ” 8~16 í”„ë ˆì„ ì‚¬ì´ê°€ ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¶©ë¶„í•œ ì‹œê°„ì  ì •ë³´ë¥¼ ì œê³µí•˜ë©´ì„œë„ Jetson Nanoì˜ ì„±ëŠ¥ í•œê³„ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì„¤ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
  EfficientGCN-B0
    x-sub120 ë°ì´í„°ì…‹ì— ëŒ€í•´ ì •í™•ë„ 86.6, x-set120 ë°ì´í„°ì…‹ì— ëŒ€í•´ ì •í™•ë„ 85.0

  90.2 2.73 1Ã— 0.29 1Ã—


Cross-subjectì™€ Cross-setupì€ í–‰ë™ ì¸ì‹ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë‘ ê°€ì§€ í‰ê°€ ì„¤ì •ì…ë‹ˆë‹¤.
  Cross-subject: í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë‹¤ë¥¸ ì£¼ì²´ë“¤ë¡œë¶€í„° ë‚˜ì˜¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. í›ˆë ¨ ì‹œ ì‚¬ìš©ëœ ì‚¬ëŒê³¼ í…ŒìŠ¤íŠ¸ ì‹œ ì‚¬ìš©ëœ ì‚¬ëŒì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì¼ë°˜í™”ë˜ì—ˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
  Cross-setup: ë‹¤ë¥¸ í™˜ê²½ ì„¤ì •ì´ë‚˜ ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì´ëŠ” ë‹¤ì–‘í•œ ë°°ê²½ì´ë‚˜ í™˜ê²½ì—ì„œì˜ í–‰ë™ ì¸ì‹ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

, Table 7ì€ X-sub ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ ì„±ëŠ¥ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í‰ê°€í•œ ê²ƒì…ë‹ˆë‹¤.
FLOPsëŠ” íŠ¹ì • ë°ì´í„°ì…‹ì—ì„œ ì¸¡ì •ëœ ì—°ì‚°ëŸ‰ì„ ë‚˜íƒ€ë‚´ë©°, ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œë„ í° ì°¨ì´ ì—†ì´ ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, FLOPsëŠ” ì£¼ì–´ì§„ ëª¨ë¸ì˜ êµ¬ì¡°ì— ë”°ë¼ ê±°ì˜ ê³ ì •ì ì…ë‹ˆë‹¤.

â“ 5. ì»¤ë„ì˜ ê¸¸ì´ì™€ ê³¨ê²© ëª¨ë¸ì—ì„œì˜ ì˜ë¯¸
  ì»¤ë„ì€ ì¼ë°˜ì ìœ¼ë¡œ í•©ì„±ê³± ì—°ì‚°ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•„í„°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì»¤ë„ì˜ í¬ê¸°ì™€ ê°€ì¤‘ì¹˜ëŠ” ë„¤íŠ¸ì›Œí¬ê°€ í•™ìŠµí•˜ëŠ” ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„° ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

  L x 1 Convì—ì„œ Lì€ ì»¤ë„ì˜ ê¸¸ì´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, L=3ì´ë©´ 3ê°œì˜ ê°’ì„ í•©ì„±ê³±ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

  ê³¨ê²© ê¸°ë°˜ í–‰ë™ ì¸ì‹ì—ì„œì˜ ì»¤ë„ì€ ê´€ì ˆ ê°„ì˜ ê³µê°„ì  ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” í•„í„°ë¡œ ì‘ë™í•©ë‹ˆë‹¤. ì»¤ë„ì€ ê° ê´€ì ˆì´ ë‹¤ë¥¸ ê´€ì ˆê³¼ ì–´ë–¤ ê´€ê³„ë¥¼ ê°€ì§€ëŠ”ì§€ í•™ìŠµí•˜ë©°, ê·¸ í¬ê¸°ì™€ ê¸¸ì´ëŠ” í•™ìŠµí•˜ëŠ” í–‰ë™ íŒ¨í„´ì˜ ë³µì¡ë„ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤.
  LÃ—1 ConvëŠ” 1D í•©ì„±ê³±ì„ ì˜ë¯¸í•˜ë©°, ì‹œê°„ì  ì°¨ì›ì—ì„œ ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

  C_in, CoutëŠ” ê°ê° ì…ë ¥ ì±„ë„ ìˆ˜ì™€ ì¶œë ¥ ì±„ë„ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

  /2ëŠ” í•©ì„±ê³± ì—°ì‚° í›„ ì¶œë ¥ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ëŠ” ì—°ì‚°ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

EfficientGCNì—ì„œëŠ” SGLayerë¥¼ ë¹„ë¡¯í•œ ë‹¤ì–‘í•œ ë ˆì´ì–´ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.
  ëª¨ë¸ì˜ êµ¬ì¡°ì— ë”°ë¼ BasicLayer, BottleLayer, SepLayer, EpSepLayer, SGLayer ì¤‘ì—ì„œ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
  SGLayerëŠ” íš¨ìœ¨ì ì¸ ê³µê°„ì , ì‹œê°„ì  ì •ë³´ë¥¼ í•™ìŠµí•˜ëŠ” ë ˆì´ì–´ë¡œ, depth-wise convolutionê³¼ point-wise convolutionì´ ê²°í•©ëœ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

1. ê³¨ê²© ê¸°ë°˜ í–‰ë™ ì¸ì‹ ëª¨ë¸ì—ì„œ Depth-wise Convolution ì‚¬ìš© ì´ìœ 
  ê³¨ê²© ê¸°ë°˜ í–‰ë™ ì¸ì‹ ëª¨ë¸ì—ì„œëŠ” ê´€ì ˆ ìœ„ì¹˜ ë°ì´í„°(Joint), ì†ë„ ë°ì´í„°(Velocity), ë¼ˆ ì •ë³´(Bone) ë“±ì´ ì±„ë„ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤. ì´ ê° ì±„ë„ì€ í–‰ë™ ì¸ì‹ì„ ìœ„í•œ ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì§•ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

  ê° ì±„ë„ ê°„ ìƒí˜¸ì‘ìš©ì´ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ì´ìœ :
    ê´€ì ˆ ìœ„ì¹˜(Joint)ëŠ” ê° ê´€ì ˆì˜ 3D ì¢Œí‘œì…ë‹ˆë‹¤.
    ì†ë„(Velocity)ëŠ” ì‹œê°„ì— ë”°ë¥¸ ê´€ì ˆì˜ ì´ë™ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    ë¼ˆ ì •ë³´(Bone)ëŠ” ê´€ì ˆ ê°„ì˜ ê±°ë¦¬ ë° ê°ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    ì´ëŸ¬í•œ ì±„ë„ë“¤ì€ ì„œë¡œ ë…ë¦½ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ë©°, ê´€ì ˆì˜ ë¬¼ë¦¬ì ì¸ ìœ„ì¹˜ë‚˜ ì´ë™ ì†ë„, ê´€ì ˆ ê°„ì˜ ê´€ê³„ë¥¼ ê°œë³„ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë” ì¤‘ìš”í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ê° ì±„ë„ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” Depth-wise Convolutionì´ ìœ ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì±„ë„ ê°„ì˜ ìƒí˜¸ì‘ìš©ì´ í•„ìˆ˜ì ì´ì§€ ì•Šê¸° ë•Œë¬¸ì—, ì—°ì‚°ëŸ‰ì„ ì¤„ì´ë©´ì„œë„ ê° ì±„ë„ì˜ ì¤‘ìš”í•œ ì •ë³´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


ê¹Šì´(Depth) í•´ì„
  **Depth(ê¹Šì´)**ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ì˜ ë ˆì´ì–´ ìˆ˜ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ, êµ¬ì²´ì ì¸ ì˜ë¯¸ëŠ” ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  ê³¨ê²© ê¸°ë°˜ í–‰ë™ ì¸ì‹ì—ì„œëŠ” ë„¤íŠ¸ì›Œí¬ê°€ ì–¼ë§ˆë‚˜ ê¹Šì´ ìˆëŠ” íŠ¹ì§•ì„ í•™ìŠµí•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë” ë§ì€ ë ˆì´ì–´ë¥¼ ìŒ“ìœ¼ë©´ ëª¨ë¸ì´ ë” ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  DepthëŠ” ë³´í†µ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì •ë˜ë©°, í•™ìŠµ ê³¼ì •ì—ì„œ ìµœì ì˜ ë ˆì´ì–´ ìˆ˜ë¥¼ ì°¾ê¸° ìœ„í•´ ì¡°ì •ë©ë‹ˆë‹¤. ë³´í†µ ë§ì€ DepthëŠ” ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì œê³µí•˜ì§€ë§Œ, ê³„ì‚°ëŸ‰ì´ ì¦ê°€í•˜ì—¬ ì—°ì‚° ì‹œê°„ì´ ê¸¸ì–´ì§‘ë‹ˆë‹¤.

ë‚´ê°€ ì´í•´í•œ ê²ƒì„ ê²€í† í•´ì¤˜.
(a) ì—ì„œëŠ” Df*Df*Cin  (ì´ˆê¸°(?) í”¼ì³ ë§µì´ ê° ì±„ë„ (R, G, B) ë§ˆë‹¤ ìˆìœ¼ë¯€ë¡œ) ì…ë ¥ 1ê³¼  Dk*Dk*Cout(í•„í„° ê°œìˆ˜) ë¥¼  ì»¨ë³¼ë£¨ì…˜ í•´ì„œ Df*Df*Cout (í”¼ì³ ë§µì´ í•„í„° ìˆ˜ë§ˆë‹¤ ìˆìœ¼ë¯€ë¡œ) ì´ ë˜ëŠ”ê±°ì•¼.
(b) ëŠ” ì• ì´ˆì— ì •ì˜ê°€ í•„í„° ìˆ˜ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³ , ê° ê¹Šì´ (ì±„ë„) ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ Df*Df ë“¤ì— ëŒ€í•´ ì»¨ë³¼ë£¨ì…˜ì„ í•˜ë¯€ë¡œ Dk*Dk*C_in (ê·¸ë˜ì„œ ì»¤ë„ ì…ë ¥ì—ì„œ Cin ì´ ë™ì¼í•˜ë‹¤!)

(c) ëŠ” ì• ì´ˆì— ì •ì˜ê°€ í•„í„° ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ í•©ì¹˜ëŠ” ì—°ì‚°ì´ë¯€ë¡œ, ê° ì±„ë„ì˜ ë™ì¼í•œ í¬ì¸íŠ¸ (1*1)ì— ëŒ€í•´ convolution ì„ ìˆ˜í–‰í•˜ê³  Cout í•„í„° ìˆ˜ë§Œí¼ ë˜ ë°˜ë³µí•œë‹¤. (ì¶”ê°€ ì°¨ì›) ê·¸ë˜ì„œ ê²°ê³¼ê°€ Df*Df*Cout ì´ ëœë‹¤.
